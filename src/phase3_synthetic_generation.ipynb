{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 3: Synthetic EEG Data Generation\n",
    "\n",
    "## Objective\n",
    "Generate synthetic EEG data using methods from recent literature:\n",
    "1. Correlation sampling method (Statistical Approach)\n",
    "2. WGAN-GP approach (simplified)\n",
    "3. Evaluation via TSTR, TRTR, clustering, and statistical tests\n",
    "\n",
    "## Literature Review Summary\n",
    "- **Correlation Sampling**: Analyze frequency band correlations, generate signals preserving structure\n",
    "- **WGAN-GP**: More reliable than vanilla GAN for EEG generation\n",
    "- **Evaluation**: Random Forest classifier, PERMANOVA, clustering overlap, TSTR/TRTR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T18:46:28.995502Z",
     "iopub.status.busy": "2025-11-02T18:46:28.995178Z",
     "iopub.status.idle": "2025-11-02T18:46:31.530824Z",
     "shell.execute_reply": "2025-11-02T18:46:31.530534Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "RANDOM SEED SET FOR REPRODUCIBILITY\n",
      "Seed value: 42\n",
      "============================================================\n",
      "\n",
      "Phase 3: Synthetic EEG Generation\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from scipy import signal, stats\n",
    "from scipy.linalg import sqrtm\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.covariance import LedoitWolf\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import pickle\n",
    "import warnings\n",
    "import random\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"RANDOM SEED SET FOR REPRODUCIBILITY\")\n",
    "print(f\"Seed value: {RANDOM_SEED}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "\n",
    "# Data path\n",
    "DATA_PATH = Path.home() / '.cache/kagglehub/datasets/nnair25/Alcoholics/versions/1'\n",
    "TRAIN_PATH = DATA_PATH / 'SMNI_CMI_TRAIN'\n",
    "TEST_PATH = DATA_PATH / 'SMNI_CMI_TEST'\n",
    "\n",
    "print(\"\\nPhase 3: Synthetic EEG Generation\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Analysis Results from Phase 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T18:46:31.551382Z",
     "iopub.status.busy": "2025-11-02T18:46:31.551079Z",
     "iopub.status.idle": "2025-11-02T18:46:31.554498Z",
     "shell.execute_reply": "2025-11-02T18:46:31.554220Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Phase 2 analysis results\n",
      "Frequency bands: {'Delta': (0.5, 4), 'Theta': (4, 8), 'Alpha': (8, 13), 'Beta': (13, 30), 'Gamma': (30, 50)}\n",
      "Sampling rate: 256 Hz\n"
     ]
    }
   ],
   "source": [
    "# Load Phase 2 results (relative path to output folder)\n",
    "with open('../output/phase2_analysis_results.pkl', 'rb') as f:\n",
    "    analysis_results = pickle.load(f)\n",
    "\n",
    "FREQUENCY_BANDS = analysis_results['frequency_bands']\n",
    "SAMPLING_RATE = analysis_results['sampling_rate']\n",
    "\n",
    "print(\"Loaded Phase 2 analysis results\")\n",
    "print(f\"Frequency bands: {FREQUENCY_BANDS}\")\n",
    "print(f\"Sampling rate: {SAMPLING_RATE} Hz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare Real Data for Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data & Feature Summary\n",
    "\n",
    "- **Corpus**: All available CSV files in `SMNI_CMI_TRAIN` (≈468 total; 235 alcoholic / 233 control). Adjust `MAX_FILES_PER_CLASS` if you need a subset.\n",
    "- **Epochs**: Dynamically determined from `sensor position × trial` combinations across the full corpus.\n",
    "- **Features**: 5-D band power vectors computed via Welch’s PSD at 256 Hz covering Δ (0.5–4 Hz), θ (4–8 Hz), α (8–13 Hz), β (13–30 Hz), γ (30–50 Hz).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T18:46:31.555803Z",
     "iopub.status.busy": "2025-11-02T18:46:31.555721Z",
     "iopub.status.idle": "2025-11-02T18:46:32.151968Z",
     "shell.execute_reply": "2025-11-02T18:46:32.151717Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identifying subject types...\n",
      "Found 235 alcoholic files\n",
      "Found 233 control files\n",
      "\n",
      "Loading 468 files for generator training...\n",
      "  Alcoholic files selected: 235\n",
      "  Control files selected  : 233\n"
     ]
    }
   ],
   "source": [
    "# Load training files\n",
    "train_files = sorted(list(TRAIN_PATH.glob('*.csv')))\n",
    "\n",
    "# Separate alcoholic and control files  \n",
    "alcoholic_files = []\n",
    "control_files = []\n",
    "\n",
    "print(\"Identifying subject types...\")\n",
    "for file in train_files:\n",
    "    df_peek = pd.read_csv(file, nrows=1)\n",
    "    subject_type = df_peek['subject identifier'].iloc[0]\n",
    "    if subject_type == 'a':\n",
    "        alcoholic_files.append(file)\n",
    "    else:\n",
    "        control_files.append(file)\n",
    "\n",
    "print(f\"Found {len(alcoholic_files)} alcoholic files\")\n",
    "print(f\"Found {len(control_files)} control files\")\n",
    "\n",
    "# Configure which files to load for generator training\n",
    "MAX_FILES_PER_CLASS = None\n",
    "\n",
    "if MAX_FILES_PER_CLASS is None:\n",
    "    selected_alcoholic = list(alcoholic_files)\n",
    "    selected_control = list(control_files)\n",
    "else:\n",
    "    selected_alcoholic = list(alcoholic_files[:MAX_FILES_PER_CLASS])\n",
    "    selected_control = list(control_files[:MAX_FILES_PER_CLASS])\n",
    "\n",
    "sample_files = selected_alcoholic + selected_control\n",
    "if not sample_files:\n",
    "    raise ValueError(\"No training files selected. Check MAX_FILES_PER_CLASS or dataset path.\")\n",
    "\n",
    "random.shuffle(sample_files)\n",
    "\n",
    "\n",
    "print(f\"\\nLoading {len(sample_files)} files for generator training...\")\n",
    "print(f\"  Alcoholic files selected: {len(selected_alcoholic)}\")\n",
    "print(f\"  Control files selected  : {len(selected_control)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Extract EEG Features from Real Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T18:46:32.153581Z",
     "iopub.status.busy": "2025-11-02T18:46:32.153474Z",
     "iopub.status.idle": "2025-11-02T18:46:32.964784Z",
     "shell.execute_reply": "2025-11-02T18:46:32.964476Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features from real EEG data...\n",
      "  Processed 46/468 files...\n",
      "  Processed 92/468 files...\n",
      "  Processed 138/468 files...\n",
      "  Processed 184/468 files...\n",
      "  Processed 230/468 files...\n",
      "  Processed 276/468 files...\n",
      "  Processed 322/468 files...\n",
      "  Processed 368/468 files...\n",
      "  Processed 414/468 files...\n",
      "  Processed 460/468 files...\n",
      "  Processed 468/468 files...\n",
      "\\nExtracted 2340 epochs from real data\n",
      "Feature shape: (2340, 5)\n",
      "Signal shape: (2340, 256)\n",
      "Class distribution: 1175 alcoholic, 1165 control\n"
     ]
    }
   ],
   "source": [
    "def extract_band_power(signal_data, fs=256, bands=None):\n",
    "    \"\"\"Extract power in each frequency band using Welch's method\"\"\"\n",
    "    if bands is None:\n",
    "        bands = FREQUENCY_BANDS\n",
    "    \n",
    "    freqs, psd = signal.welch(signal_data, fs=fs, nperseg=min(256, len(signal_data)))\n",
    "    \n",
    "    band_powers = {}\n",
    "    for band_name, (low_freq, high_freq) in bands.items():\n",
    "        idx = np.logical_and(freqs >= low_freq, freqs <= high_freq)\n",
    "        band_power = np.trapz(psd[idx], freqs[idx])\n",
    "        band_powers[band_name] = band_power\n",
    "    \n",
    "    return band_powers\n",
    "\n",
    "# Extract features from sample files\n",
    "real_features = []\n",
    "real_signals = []\n",
    "labels = []\n",
    "\n",
    "print(\"Extracting features from real EEG data...\")\n",
    "progress_interval = max(1, len(sample_files) // 10) if sample_files else 1\n",
    "for file_idx, file in enumerate(sample_files):\n",
    "    df = pd.read_csv(file)\n",
    "    subject_type = df['subject identifier'].iloc[0]\n",
    "    \n",
    "    # Get first few channels and trials\n",
    "    channels = df['sensor position'].unique()[:5]\n",
    "    trials = df['trial number'].unique()[:2]\n",
    "    \n",
    "    for channel in channels:\n",
    "        for trial in trials:\n",
    "            trial_data = df[\n",
    "                (df['sensor position'] == channel) & \n",
    "                (df['trial number'] == trial)\n",
    "            ].sort_values('sample num')\n",
    "            \n",
    "            if len(trial_data) >= 128:\n",
    "                signal_data = trial_data['sensor value'].values[:256]\n",
    "                band_powers = extract_band_power(signal_data)\n",
    "                \n",
    "                feature_vector = list(band_powers.values())\n",
    "                real_features.append(feature_vector)\n",
    "                real_signals.append(signal_data)\n",
    "                labels.append(1 if subject_type == 'a' else 0)\n",
    "    \n",
    "    if ((file_idx + 1) % progress_interval == 0) or (file_idx + 1 == len(sample_files)):\n",
    "        print(f\"  Processed {file_idx + 1}/{len(sample_files)} files...\")\n",
    "\n",
    "real_features = np.array(real_features)\n",
    "real_signals = np.array(real_signals)\n",
    "labels = np.array(labels)\n",
    "\n",
    "print(f\"\\\\nExtracted {len(real_features)} epochs from real data\")\n",
    "print(f\"Feature shape: {real_features.shape}\")\n",
    "print(f\"Signal shape: {real_signals.shape}\")\n",
    "print(f\"Class distribution: {np.sum(labels == 1)} alcoholic, {np.sum(labels == 0)} control\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Method 1: Correlation Sampling Approach\n",
    "\n",
    "Based on \"A Statistical Approach for Synthetic EEG Data Generation\"\n",
    "\n",
    "Steps:\n",
    "1. Compute correlation matrix of frequency band features\n",
    "2. Sample from multivariate normal distribution preserving correlations\n",
    "3. Generate synthetic features matching real data statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T18:46:32.966524Z",
     "iopub.status.busy": "2025-11-02T18:46:32.966415Z",
     "iopub.status.idle": "2025-11-02T18:46:32.970196Z",
     "shell.execute_reply": "2025-11-02T18:46:32.969965Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nGenerating 2340 synthetic samples...\n",
      "Correlation Matrix of Frequency Bands:\n",
      "Delta  - Delta :  1.000\n",
      "Delta  - Theta :  0.570\n",
      "Delta  - Alpha :  0.025\n",
      "Delta  - Beta  : -0.048\n",
      "Delta  - Gamma : -0.057\n",
      "Theta  - Theta :  1.000\n",
      "Theta  - Alpha :  0.352\n",
      "Theta  - Beta  : -0.019\n",
      "Theta  - Gamma : -0.010\n",
      "Alpha  - Alpha :  1.000\n",
      "Alpha  - Beta  :  0.041\n",
      "Alpha  - Gamma :  0.042\n",
      "Beta   - Beta  :  1.000\n",
      "Beta   - Gamma :  0.952\n",
      "Gamma  - Gamma :  1.000\n",
      "\\nGenerated 2340 synthetic feature vectors\n",
      "Correlation structure preserved\n",
      "Shape of synthetic features: (2340, 5)\n"
     ]
    }
   ],
   "source": [
    "def generate_correlation_based_eeg(real_features, n_synthetic=100, random_seed=42):\n",
    "    \"\"\"\n",
    "    Generate synthetic EEG using correlation sampling method\n",
    "    \n",
    "    This preserves the correlation structure between frequency bands\n",
    "    \"\"\"\n",
    "    # Set random seed for reproducibility\n",
    "    np.random.seed(random_seed)\n",
    "    \n",
    "    # Compute correlation matrix and statistics\n",
    "    correlation_matrix = np.corrcoef(real_features.T)\n",
    "    mean_features = np.mean(real_features, axis=0)\n",
    "    std_features = np.std(real_features, axis=0)\n",
    "    \n",
    "    print(\"Correlation Matrix of Frequency Bands:\")\n",
    "    band_names = list(FREQUENCY_BANDS.keys())\n",
    "    for i, band1 in enumerate(band_names):\n",
    "        for j, band2 in enumerate(band_names):\n",
    "            if j >= i:\n",
    "                print(f\"{band1:6s} - {band2:6s}: {correlation_matrix[i,j]:6.3f}\")\n",
    "    \n",
    "    # Generate synthetic features preserving correlation structure\n",
    "    covariance_matrix = np.outer(std_features, std_features) * correlation_matrix\n",
    "    \n",
    "    synthetic_features = np.random.multivariate_normal(\n",
    "        mean_features,\n",
    "        covariance_matrix,\n",
    "        size=n_synthetic\n",
    "    )\n",
    "    \n",
    "    # Ensure non-negative powers\n",
    "    synthetic_features = np.abs(synthetic_features)\n",
    "    \n",
    "    print(f\"\\\\nGenerated {n_synthetic} synthetic feature vectors\")\n",
    "    print(f\"Correlation structure preserved\")\n",
    "    \n",
    "    return synthetic_features, correlation_matrix\n",
    "\n",
    "# Generate synthetic data\n",
    "n_synthetic_samples = len(real_features)\n",
    "print(f\"\\\\nGenerating {n_synthetic_samples} synthetic samples...\")\n",
    "\n",
    "synthetic_features_corr, corr_matrix = generate_correlation_based_eeg(\n",
    "    real_features,\n",
    "    n_synthetic=n_synthetic_samples,\n",
    "    random_seed=RANDOM_SEED\n",
    ")\n",
    "\n",
    "print(f\"Shape of synthetic features: {synthetic_features_corr.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Method 2: GAN-like Generation\n",
    "\n",
    "Simplified approach inspired by WGAN-GP methodology\n",
    "(Full implementation would require deep learning framework)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T18:46:32.971372Z",
     "iopub.status.busy": "2025-11-02T18:46:32.971299Z",
     "iopub.status.idle": "2025-11-02T18:46:32.984472Z",
     "shell.execute_reply": "2025-11-02T18:46:32.984242Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating synthetic data using GAN-like approach...\n",
      "Generated 2340 synthetic samples\n",
      "Shape: (2340, 5)\n"
     ]
    }
   ],
   "source": [
    "def generate_gan_based_eeg(real_features, n_synthetic=100, random_seed=42):\n",
    "    \"\"\"\n",
    "    Simplified GAN-like generation using:\n",
    "    - Interpolation between real samples\n",
    "    - Addition of controlled gaussian noise\n",
    "    \n",
    "    Note: Full WGAN-GP would require TensorFlow/PyTorch implementation\n",
    "    This is a simplified demonstration of the concept\n",
    "    \"\"\"\n",
    "    # Set random seed for reproducibility\n",
    "    np.random.seed(random_seed)\n",
    "    \n",
    "    synthetic_features = []\n",
    "    \n",
    "    for _ in range(n_synthetic):\n",
    "        # Random interpolation between two real samples\n",
    "        idx1, idx2 = np.random.choice(len(real_features), 2, replace=False)\n",
    "        alpha = np.random.uniform(0.3, 0.7)\n",
    "        \n",
    "        interpolated = alpha * real_features[idx1] + (1 - alpha) * real_features[idx2]\n",
    "        \n",
    "        # Add controlled gaussian noise\n",
    "        noise_scale = 0.1 * np.std(real_features, axis=0)\n",
    "        noise = np.random.normal(0, noise_scale)\n",
    "        synthetic_sample = interpolated + noise\n",
    "        \n",
    "        # Ensure non-negative values (power cannot be negative)\n",
    "        synthetic_sample = np.abs(synthetic_sample)\n",
    "        synthetic_features.append(synthetic_sample)\n",
    "    \n",
    "    return np.array(synthetic_features)\n",
    "\n",
    "print(\"Generating synthetic data using GAN-like approach...\")\n",
    "synthetic_features_gan = generate_gan_based_eeg(\n",
    "    real_features,\n",
    "    n_synthetic=n_synthetic_samples,\n",
    "    random_seed=RANDOM_SEED\n",
    ")\n",
    "\n",
    "print(f\"Generated {len(synthetic_features_gan)} synthetic samples\")\n",
    "print(f\"Shape: {synthetic_features_gan.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluation: Distribution Comparison (KS Test & MMD)\n",
    "\n",
    "Statistical tests to compare real vs synthetic data distributions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T18:46:32.985832Z",
     "iopub.status.busy": "2025-11-02T18:46:32.985759Z",
     "iopub.status.idle": "2025-11-02T18:46:32.994970Z",
     "shell.execute_reply": "2025-11-02T18:46:32.994718Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\n============================================================\n",
      "Distribution Comparison: Correlation Sampling\n",
      "============================================================\n",
      "\\nKolmogorov-Smirnov Test Results:\n",
      "(p-value > 0.05 suggests distributions are similar)\n",
      "  Delta   : KS=0.3573, p=0.0000 ✗ Different\n",
      "  Theta   : KS=0.2269, p=0.0000 ✗ Different\n",
      "  Alpha   : KS=0.3338, p=0.0000 ✗ Different\n",
      "  Beta    : KS=0.5679, p=0.0000 ✗ Different\n",
      "  Gamma   : KS=0.9350, p=0.0000 ✗ Different\n",
      "\\nMMD Score: -422.8791\n",
      "(Lower MMD indicates more similar distributions)\n",
      "\\n============================================================\n",
      "Distribution Comparison: GAN-like Generation\n",
      "============================================================\n",
      "\\nKolmogorov-Smirnov Test Results:\n",
      "(p-value > 0.05 suggests distributions are similar)\n",
      "  Delta   : KS=0.1385, p=0.0000 ✗ Different\n",
      "  Theta   : KS=0.1440, p=0.0000 ✗ Different\n",
      "  Alpha   : KS=0.1321, p=0.0000 ✗ Different\n",
      "  Beta    : KS=0.1154, p=0.0000 ✗ Different\n",
      "  Gamma   : KS=0.8192, p=0.0000 ✗ Different\n",
      "\\nMMD Score: -29.6218\n",
      "(Lower MMD indicates more similar distributions)\n"
     ]
    }
   ],
   "source": [
    "def evaluate_distributions(real_features, synthetic_features, method_name=\"\"):\n",
    "    \"\"\"\n",
    "    Compare distributions using:\n",
    "    - KS test (Kolmogorov-Smirnov): tests if distributions are from same population\n",
    "    - MMD (Maximum Mean Discrepancy): measures distance between distributions\n",
    "    \"\"\"\n",
    "    print(f\"\\\\n{'='*60}\")\n",
    "    print(f\"Distribution Comparison: {method_name}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # KS test for each feature (frequency band)\n",
    "    band_names = list(FREQUENCY_BANDS.keys())\n",
    "    ks_results = []\n",
    "    \n",
    "    print(\"\\\\nKolmogorov-Smirnov Test Results:\")\n",
    "    print(\"(p-value > 0.05 suggests distributions are similar)\")\n",
    "    for i, band in enumerate(band_names):\n",
    "        ks_stat, p_value = stats.ks_2samp(real_features[:, i], synthetic_features[:, i])\n",
    "        ks_results.append({'band': band, 'ks_stat': ks_stat, 'p_value': p_value})\n",
    "        \n",
    "        significance = \"✓ Similar\" if p_value > 0.05 else \"✗ Different\"\n",
    "        print(f\"  {band:8s}: KS={ks_stat:.4f}, p={p_value:.4f} {significance}\")\n",
    "    \n",
    "    # Simplified MMD computation\n",
    "    def compute_mmd(X, Y):\n",
    "        \"\"\"Maximum Mean Discrepancy using pairwise distances\"\"\"\n",
    "        XX = cdist(X, X, metric='euclidean')\n",
    "        YY = cdist(Y, Y, metric='euclidean')\n",
    "        XY = cdist(X, Y, metric='euclidean')\n",
    "        \n",
    "        mmd = np.mean(XX) + np.mean(YY) - 2 * np.mean(XY)\n",
    "        return mmd\n",
    "    \n",
    "    mmd_score = compute_mmd(real_features, synthetic_features)\n",
    "    print(f\"\\\\nMMD Score: {mmd_score:.4f}\")\n",
    "    print(\"(Lower MMD indicates more similar distributions)\")\n",
    "    \n",
    "    return ks_results, mmd_score\n",
    "\n",
    "# Evaluate Correlation Sampling Method\n",
    "ks_corr, mmd_corr = evaluate_distributions(\n",
    "    real_features,\n",
    "    synthetic_features_corr,\n",
    "    \"Correlation Sampling\"\n",
    ")\n",
    "\n",
    "# Evaluate GAN-like Method  \n",
    "ks_gan, mmd_gan = evaluate_distributions(\n",
    "    real_features,\n",
    "    synthetic_features_gan,\n",
    "    \"GAN-like Generation\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluation: TSTR and TRTR\n",
    "\n",
    "**TRTR** = Train on Real, Test on Real  \n",
    "**TSTR** = Train on Synthetic, Test on Real\n",
    "\n",
    "If TSTR ≈ TRTR, synthetic data quality is high\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T18:46:32.996301Z",
     "iopub.status.busy": "2025-11-02T18:46:32.996222Z",
     "iopub.status.idle": "2025-11-02T18:46:33.209791Z",
     "shell.execute_reply": "2025-11-02T18:46:33.209571Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nEvaluating Correlation Sampling Method:\n",
      "\\n============================================================\n",
      "TSTR/TRTR Evaluation: Correlation\n",
      "============================================================\n",
      "\\n1. TRTR (Train on Real, Test on Real):\n",
      "   Accuracy: 0.6966\n",
      "\\n2. TSTR (Train on Synthetic, Test on Real):\n",
      "   Accuracy: 0.3561\n",
      "\\n3. Performance Comparison:\n",
      "   TRTR: 0.6966\n",
      "   TSTR: 0.3561\n",
      "   Difference: 0.3405\n",
      "   ✗ Synthetic data quality: NEEDS IMPROVEMENT\n",
      "\\n============================================================\n",
      "Evaluating GAN-like Method:\n",
      "\\n============================================================\n",
      "TSTR/TRTR Evaluation: GAN-like\n",
      "============================================================\n",
      "\\n1. TRTR (Train on Real, Test on Real):\n",
      "   Accuracy: 0.6966\n",
      "\\n2. TSTR (Train on Synthetic, Test on Real):\n",
      "   Accuracy: 0.4758\n",
      "\\n3. Performance Comparison:\n",
      "   TRTR: 0.6966\n",
      "   TSTR: 0.4758\n",
      "   Difference: 0.2208\n",
      "   ✗ Synthetic data quality: NEEDS IMPROVEMENT\n"
     ]
    }
   ],
   "source": [
    "def evaluate_tstr_trtr(real_features, real_labels, synthetic_features, method_name=\"\"):\n",
    "    \"\"\"\n",
    "    TSTR/TRTR Evaluation from literature\n",
    "    \n",
    "    Validates synthetic data by comparing model performance when:\n",
    "    - Training on real vs synthetic data\n",
    "    - Testing on real data\n",
    "    \"\"\"\n",
    "    print(f\"\\\\n{'='*60}\")\n",
    "    print(f\"TSTR/TRTR Evaluation: {method_name}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Split real data\n",
    "    X_train_real, X_test_real, y_train_real, y_test_real = train_test_split(\n",
    "        real_features, real_labels, test_size=0.3, random_state=RANDOM_SEED\n",
    "    )\n",
    "    \n",
    "    # Create synthetic labels matching real distribution\n",
    "    n_alcoholic = np.sum(y_train_real == 1)\n",
    "    n_control = np.sum(y_train_real == 0)\n",
    "    y_synthetic = np.concatenate([\n",
    "        np.ones(min(n_alcoholic, len(synthetic_features)//2)),\n",
    "        np.zeros(min(n_control, len(synthetic_features)//2))\n",
    "    ])\n",
    "    X_synthetic = synthetic_features[:len(y_synthetic)]\n",
    "    \n",
    "    # TRTR: Train on Real, Test on Real\n",
    "    print(\"\\\\n1. TRTR (Train on Real, Test on Real):\")\n",
    "    clf_trtr = RandomForestClassifier(n_estimators=100, random_state=RANDOM_SEED)\n",
    "    clf_trtr.fit(X_train_real, y_train_real)\n",
    "    y_pred_trtr = clf_trtr.predict(X_test_real)\n",
    "    acc_trtr = accuracy_score(y_test_real, y_pred_trtr)\n",
    "    print(f\"   Accuracy: {acc_trtr:.4f}\")\n",
    "    \n",
    "    # TSTR: Train on Synthetic, Test on Real\n",
    "    print(\"\\\\n2. TSTR (Train on Synthetic, Test on Real):\")\n",
    "    clf_tstr = RandomForestClassifier(n_estimators=100, random_state=RANDOM_SEED)\n",
    "    clf_tstr.fit(X_synthetic, y_synthetic)\n",
    "    y_pred_tstr = clf_tstr.predict(X_test_real)\n",
    "    acc_tstr = accuracy_score(y_test_real, y_pred_tstr)\n",
    "    print(f\"   Accuracy: {acc_tstr:.4f}\")\n",
    "    \n",
    "    # Compare\n",
    "    print(f\"\\\\n3. Performance Comparison:\")\n",
    "    print(f\"   TRTR: {acc_trtr:.4f}\")\n",
    "    print(f\"   TSTR: {acc_tstr:.4f}\")\n",
    "    print(f\"   Difference: {abs(acc_trtr - acc_tstr):.4f}\")\n",
    "    \n",
    "    if abs(acc_trtr - acc_tstr) < 0.05:\n",
    "        print(\"   ✓ Synthetic data quality: EXCELLENT\")\n",
    "    elif abs(acc_trtr - acc_tstr) < 0.10:\n",
    "        print(\"   ✓ Synthetic data quality: GOOD\")\n",
    "    else:\n",
    "        print(\"   ✗ Synthetic data quality: NEEDS IMPROVEMENT\")\n",
    "    \n",
    "    return acc_trtr, acc_tstr\n",
    "\n",
    "# Evaluate both methods\n",
    "print(\"\\\\nEvaluating Correlation Sampling Method:\")\n",
    "acc_trtr_corr, acc_tstr_corr = evaluate_tstr_trtr(\n",
    "    real_features, labels, synthetic_features_corr, \"Correlation\"\n",
    ")\n",
    "\n",
    "print(\"\\\\n\" + \"=\"*60)\n",
    "print(\"Evaluating GAN-like Method:\")\n",
    "acc_trtr_gan, acc_tstr_gan = evaluate_tstr_trtr(\n",
    "    real_features, labels, synthetic_features_gan, \"GAN-like\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Evaluation: Real vs Synthetic Classification\n",
    "\n",
    "Train classifier to distinguish real from synthetic.  \n",
    "**Goal**: Classifier should perform at ~50% (chance level) if synthetic data is indistinguishable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T18:46:33.211115Z",
     "iopub.status.busy": "2025-11-02T18:46:33.211035Z",
     "iopub.status.idle": "2025-11-02T18:46:33.354935Z",
     "shell.execute_reply": "2025-11-02T18:46:33.354667Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\n============================================================\n",
      "Real vs Synthetic Classification: Correlation Sampling\n",
      "============================================================\n",
      "\\nClassifier Accuracy: 0.9779\n",
      "✗ POOR: Classifier easily distinguishes real from synthetic\n",
      "\\nConfusion Matrix:\n",
      "                 Pred Real  Pred Synthetic\n",
      "Actual Real:         692           13\n",
      "Actual Synthetic:     18          681\n",
      "\\n============================================================\n",
      "Real vs Synthetic Classification: GAN-like\n",
      "============================================================\n",
      "\\nClassifier Accuracy: 0.9338\n",
      "✗ POOR: Classifier easily distinguishes real from synthetic\n",
      "\\nConfusion Matrix:\n",
      "                 Pred Real  Pred Synthetic\n",
      "Actual Real:         666           39\n",
      "Actual Synthetic:     54          645\n"
     ]
    }
   ],
   "source": [
    "def evaluate_real_vs_synthetic(real_features, synthetic_features, method_name=\"\"):\n",
    "    \"\"\"\n",
    "    Train classifier to distinguish real from synthetic\n",
    "    \n",
    "    From literature: If classifier performs at chance level (~50%),\n",
    "    synthetic data is indistinguishable from real\n",
    "    \"\"\"\n",
    "    print(f\"\\\\n{'='*60}\")\n",
    "    print(f\"Real vs Synthetic Classification: {method_name}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Create combined dataset: 1=real, 0=synthetic\n",
    "    X_combined = np.vstack([real_features, synthetic_features])\n",
    "    y_combined = np.concatenate([\n",
    "        np.ones(len(real_features)),\n",
    "        np.zeros(len(synthetic_features))\n",
    "    ])\n",
    "    \n",
    "    # Train classifier\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_combined, y_combined, test_size=0.3, random_state=RANDOM_SEED\n",
    "    )\n",
    "    \n",
    "    clf = RandomForestClassifier(n_estimators=100, random_state=RANDOM_SEED)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"\\\\nClassifier Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    if 0.45 <= accuracy <= 0.55:\n",
    "        print(\"✓ EXCELLENT: Classifier at chance level (50%)\")\n",
    "        print(\"  → Synthetic data indistinguishable from real\")\n",
    "    elif 0.40 <= accuracy <= 0.60:\n",
    "        print(\"✓ GOOD: Classifier struggles to distinguish\")\n",
    "    else:\n",
    "        print(\"✗ POOR: Classifier easily distinguishes real from synthetic\")\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(\"\\\\nConfusion Matrix:\")\n",
    "    print(\"                 Pred Real  Pred Synthetic\")\n",
    "    print(f\"Actual Real:        {cm[1,1]:4d}         {cm[1,0]:4d}\")\n",
    "    print(f\"Actual Synthetic:   {cm[0,1]:4d}         {cm[0,0]:4d}\")\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "# Evaluate Correlation Sampling\n",
    "acc_corr = evaluate_real_vs_synthetic(\n",
    "    real_features, synthetic_features_corr, \"Correlation Sampling\"\n",
    ")\n",
    "\n",
    "# Evaluate GAN-like\n",
    "acc_gan = evaluate_real_vs_synthetic(\n",
    "    real_features, synthetic_features_gan, \"GAN-like\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Improved Synthetic Generation Strategies\n",
    "\n",
    "To reduce real-vs-synthetic separability while preserving frequency-band correlations, we explore two additional generators:\n",
    "\n",
    "1. **Gaussian Copula Sampling**: preserves empirical marginals per class and matches correlation structure in a latent Gaussian space.\n",
    "2. **Class-Conditional Interpolation**: SMOTE-like synthesis operating in log-power space with adaptive neighborhood mixing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _allocate_samples_by_class(labels, n_total):\n",
    "    \"\"\"Allocate synthetic samples per class, preserving empirical ratios.\"\"\"\n",
    "    classes, counts = np.unique(labels, return_counts=True)\n",
    "    ratios = counts / counts.sum()\n",
    "    expected = ratios * n_total\n",
    "    allocated = np.floor(expected).astype(int)\n",
    "    remainder = n_total - allocated.sum()\n",
    "    if remainder > 0:\n",
    "        remainders = expected - allocated\n",
    "        order = np.argsort(remainders)[::-1]\n",
    "        for idx in order[:remainder]:\n",
    "            allocated[idx] += 1\n",
    "    return dict(zip(classes, allocated))\n",
    "\n",
    "def generate_gaussian_copula_eeg(real_features, labels, n_synthetic=100, random_seed=42):\n",
    "    \"\"\"\n",
    "    Gaussian copula sampling:\n",
    "    1. Fit class-conditional quantile transformers to map marginals to Gaussian space\n",
    "    2. Estimate regularised covariance (Ledoit-Wolf) in latent space\n",
    "    3. Sample multivariate normal per class and invert the transform\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(random_seed)\n",
    "    allocation = _allocate_samples_by_class(labels, n_synthetic)\n",
    "    synthetic_blocks = []\n",
    "\n",
    "    print(\"Generating Gaussian copula samples per class...\")\n",
    "    for cls, n_cls_samples in allocation.items():\n",
    "        class_features = real_features[labels == cls]\n",
    "        if len(class_features) == 0 or n_cls_samples == 0:\n",
    "            continue\n",
    "\n",
    "        n_quantiles = min(len(class_features), 1000)\n",
    "        transformer = QuantileTransformer(\n",
    "            n_quantiles=n_quantiles,\n",
    "            output_distribution='normal',\n",
    "            random_state=random_seed\n",
    "        )\n",
    "        latent = transformer.fit_transform(class_features)\n",
    "\n",
    "        cov_estimator = LedoitWolf().fit(latent)\n",
    "        latent_mean = cov_estimator.location_\n",
    "        latent_cov = cov_estimator.covariance_\n",
    "\n",
    "        latent_samples = rng.multivariate_normal(\n",
    "            latent_mean,\n",
    "            latent_cov,\n",
    "            size=n_cls_samples\n",
    "        )\n",
    "\n",
    "        samples = transformer.inverse_transform(latent_samples)\n",
    "        samples = np.clip(samples, a_min=0, a_max=None)\n",
    "        synthetic_blocks.append(samples)\n",
    "\n",
    "        print(f\"  Class {cls}: real={len(class_features)}, synthetic={n_cls_samples}\")\n",
    "\n",
    "    if not synthetic_blocks:\n",
    "        raise ValueError(\"No synthetic samples were generated. Check class labels.\")\n",
    "\n",
    "    synthetic_features = np.vstack(synthetic_blocks)\n",
    "    return synthetic_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Gaussian copula samples per class...\n",
      "  Class 0: real=1165, synthetic=1165\n",
      "  Class 1: real=1175, synthetic=1175\n",
      "\n",
      "Generated 2340 Gaussian copula samples\n",
      "Shape: (2340, 5)\n"
     ]
    }
   ],
   "source": [
    "synthetic_features_copula = generate_gaussian_copula_eeg(\n",
    "    real_features,\n",
    "    labels,\n",
    "    n_synthetic=n_synthetic_samples,\n",
    "    random_seed=RANDOM_SEED\n",
    ")\n",
    "\n",
    "print(f\"\\nGenerated {len(synthetic_features_copula)} Gaussian copula samples\")\n",
    "print(f\"Shape: {synthetic_features_copula.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_classwise_interpolation_eeg(\n",
    "    real_features,\n",
    "    labels,\n",
    "    n_synthetic=100,\n",
    "    random_seed=42,\n",
    "    k_neighbors=8,\n",
    "    noise_scale=0.02\n",
    "):\n",
    "    \"\"\"\n",
    "    Class-conditional interpolation inspired by SMOTE.\n",
    "    Operates in log-power space to better capture multiplicative structure.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(random_seed)\n",
    "    allocation = _allocate_samples_by_class(labels, n_synthetic)\n",
    "    synthetic_samples = []\n",
    "\n",
    "    log_features = np.log1p(real_features)\n",
    "\n",
    "    for cls, n_cls_samples in allocation.items():\n",
    "        class_mask = labels == cls\n",
    "        class_features_log = log_features[class_mask]\n",
    "        if len(class_features_log) == 0 or n_cls_samples == 0:\n",
    "            continue\n",
    "\n",
    "        n_neighbors_eff = min(k_neighbors, len(class_features_log) - 1)\n",
    "        if n_neighbors_eff <= 0:\n",
    "            # Not enough samples to interpolate, fallback to jittering existing ones\n",
    "            base_samples = np.repeat(class_features_log, repeats=max(1, n_cls_samples // max(1, len(class_features_log))), axis=0)\n",
    "            base_samples = base_samples[:n_cls_samples]\n",
    "            jitter = rng.normal(0, noise_scale, size=base_samples.shape)\n",
    "            augmented = base_samples + jitter\n",
    "            synthetic_samples.append(np.expm1(augmented))\n",
    "            continue\n",
    "\n",
    "        nbrs = NearestNeighbors(n_neighbors=n_neighbors_eff + 1)\n",
    "        nbrs.fit(class_features_log)\n",
    "        class_std = np.std(class_features_log, axis=0, ddof=1)\n",
    "        class_std[class_std == 0] = 1e-6\n",
    "\n",
    "        for _ in range(n_cls_samples):\n",
    "            idx = rng.integers(len(class_features_log))\n",
    "            neighbors = nbrs.kneighbors(class_features_log[idx].reshape(1, -1), return_distance=False)[0]\n",
    "            neighbors = neighbors[neighbors != idx]\n",
    "            if len(neighbors) == 0:\n",
    "                neighbor_idx = idx\n",
    "            else:\n",
    "                neighbor_idx = rng.choice(neighbors)\n",
    "\n",
    "            alpha = rng.uniform(0.2, 0.8)\n",
    "            interpolated = (\n",
    "                alpha * class_features_log[idx] +\n",
    "                (1 - alpha) * class_features_log[neighbor_idx]\n",
    "            )\n",
    "\n",
    "            noise = rng.normal(0, noise_scale, size=class_features_log.shape[1]) * class_std\n",
    "            synthetic_log = interpolated + noise\n",
    "            synthetic_samples.append(np.expm1(synthetic_log))\n",
    "\n",
    "    if not synthetic_samples:\n",
    "        raise ValueError(\"Interpolation generator did not create any samples.\")\n",
    "\n",
    "    synthetic_features = np.vstack(synthetic_samples)\n",
    "    synthetic_features = np.clip(synthetic_features, a_min=0, a_max=None)\n",
    "    return synthetic_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated 2340 interpolation-based samples\n",
      "Shape: (2340, 5)\n"
     ]
    }
   ],
   "source": [
    "synthetic_features_interp = generate_classwise_interpolation_eeg(\n",
    "    real_features,\n",
    "    labels,\n",
    "    n_synthetic=n_synthetic_samples,\n",
    "    random_seed=RANDOM_SEED,\n",
    "    k_neighbors=10, \n",
    "    noise_scale=0.015\n",
    ")\n",
    "\n",
    "print(f\"\\nGenerated {len(synthetic_features_interp)} interpolation-based samples\")\n",
    "print(f\"Shape: {synthetic_features_interp.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1 Distribution and Quality Checks\n",
    "\n",
    "Re-run the statistical and downstream evaluations for the new generators alongside previous baselines.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\n============================================================\n",
      "Distribution Comparison: Gaussian Copula\n",
      "============================================================\n",
      "\\nKolmogorov-Smirnov Test Results:\n",
      "(p-value > 0.05 suggests distributions are similar)\n",
      "  Delta   : KS=0.0192, p=0.7800 ✓ Similar\n",
      "  Theta   : KS=0.0184, p=0.8245 ✓ Similar\n",
      "  Alpha   : KS=0.0184, p=0.8245 ✓ Similar\n",
      "  Beta    : KS=0.0192, p=0.7800 ✓ Similar\n",
      "  Gamma   : KS=0.0167, p=0.9013 ✓ Similar\n",
      "\\nMMD Score: -0.1115\n",
      "(Lower MMD indicates more similar distributions)\n",
      "\\n============================================================\n",
      "Distribution Comparison: Classwise Interpolation\n",
      "============================================================\n",
      "\\nKolmogorov-Smirnov Test Results:\n",
      "(p-value > 0.05 suggests distributions are similar)\n",
      "  Delta   : KS=0.0201, p=0.7328 ✓ Similar\n",
      "  Theta   : KS=0.0295, p=0.2609 ✓ Similar\n",
      "  Alpha   : KS=0.0278, p=0.3274 ✓ Similar\n",
      "  Beta    : KS=0.0235, p=0.5378 ✓ Similar\n",
      "  Gamma   : KS=0.0333, p=0.1485 ✓ Similar\n",
      "\\nMMD Score: -0.1035\n",
      "(Lower MMD indicates more similar distributions)\n"
     ]
    }
   ],
   "source": [
    "ks_copula, mmd_copula = evaluate_distributions(\n",
    "    real_features,\n",
    "    synthetic_features_copula,\n",
    "    \"Gaussian Copula\"\n",
    ")\n",
    "\n",
    "ks_interp, mmd_interp = evaluate_distributions(\n",
    "    real_features,\n",
    "    synthetic_features_interp,\n",
    "    \"Classwise Interpolation\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Gaussian Copula Method:\n",
      "\\n============================================================\n",
      "TSTR/TRTR Evaluation: Gaussian Copula\n",
      "============================================================\n",
      "\\n1. TRTR (Train on Real, Test on Real):\n",
      "   Accuracy: 0.6966\n",
      "\\n2. TSTR (Train on Synthetic, Test on Real):\n",
      "   Accuracy: 0.3490\n",
      "\\n3. Performance Comparison:\n",
      "   TRTR: 0.6966\n",
      "   TSTR: 0.3490\n",
      "   Difference: 0.3476\n",
      "   ✗ Synthetic data quality: NEEDS IMPROVEMENT\n",
      "\n",
      "============================================================\n",
      "Evaluating Classwise Interpolation Method:\n",
      "\\n============================================================\n",
      "TSTR/TRTR Evaluation: Classwise Interpolation\n",
      "============================================================\n",
      "\\n1. TRTR (Train on Real, Test on Real):\n",
      "   Accuracy: 0.6966\n",
      "\\n2. TSTR (Train on Synthetic, Test on Real):\n",
      "   Accuracy: 0.3134\n",
      "\\n3. Performance Comparison:\n",
      "   TRTR: 0.6966\n",
      "   TSTR: 0.3134\n",
      "   Difference: 0.3832\n",
      "   ✗ Synthetic data quality: NEEDS IMPROVEMENT\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nEvaluating Gaussian Copula Method:\")\n",
    "acc_trtr_copula, acc_tstr_copula = evaluate_tstr_trtr(\n",
    "    real_features,\n",
    "    labels,\n",
    "    synthetic_features_copula,\n",
    "    \"Gaussian Copula\"\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Evaluating Classwise Interpolation Method:\")\n",
    "acc_trtr_interp, acc_tstr_interp = evaluate_tstr_trtr(\n",
    "    real_features,\n",
    "    labels,\n",
    "    synthetic_features_interp,\n",
    "    \"Classwise Interpolation\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\n============================================================\n",
      "Real vs Synthetic Classification: Gaussian Copula\n",
      "============================================================\n",
      "\\nClassifier Accuracy: 0.5349\n",
      "✓ EXCELLENT: Classifier at chance level (50%)\n",
      "  → Synthetic data indistinguishable from real\n",
      "\\nConfusion Matrix:\n",
      "                 Pred Real  Pred Synthetic\n",
      "Actual Real:         370          335\n",
      "Actual Synthetic:    318          381\n",
      "\\n============================================================\n",
      "Real vs Synthetic Classification: Classwise Interpolation\n",
      "============================================================\n",
      "\\nClassifier Accuracy: 0.4338\n",
      "✓ GOOD: Classifier struggles to distinguish\n",
      "\\nConfusion Matrix:\n",
      "                 Pred Real  Pred Synthetic\n",
      "Actual Real:         288          417\n",
      "Actual Synthetic:    378          321\n"
     ]
    }
   ],
   "source": [
    "acc_copula_sep = evaluate_real_vs_synthetic(\n",
    "    real_features,\n",
    "    synthetic_features_copula,\n",
    "    \"Gaussian Copula\"\n",
    ")\n",
    "\n",
    "acc_interp_sep = evaluate_real_vs_synthetic(\n",
    "    real_features,\n",
    "    synthetic_features_interp,\n",
    "    \"Classwise Interpolation\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Early-stop: targets met\n",
      "  Params: {0: 5, 1: 10} {0: 0.0, 1: 0.0}\n",
      "  Metrics: {'rvs_acc': 0.36186770428015563, 'tstr': 0.8408796895213454, 'trtr': 0.6998706338939198, 'gap': 0.1410090556274256, 'ks_min_p': 0.3261561157352818, 'mmd': 0.0004134147321114279, 'corr_sim': 0.995270237210689, 'rho_min': -0.03868685042240337, 'rho_mean': -0.002450164703319186}\n",
      "\n",
      "=== BEST COMBINATION ===\n",
      "k_params: {0: 5, 1: 5} noise_params: {0: 0.0, 1: 0.0}\n",
      "metrics : {'rvs_acc': 0.33787289234760054, 'tstr': 0.8745148771021992, 'trtr': 0.6998706338939198, 'gap': 0.17464424320827943, 'ks_min_p': 0.4868306191329178, 'mmd': 0.000296001036291349, 'corr_sim': 0.9921030346539852, 'rho_min': -0.03970450882614366, 'rho_mean': -0.002404962460893624}\n"
     ]
    }
   ],
   "source": [
    "# === Synthetic EEG Tuner: per-class (k, noise) grid + early-stopping ===\n",
    "# Paste this in one cell. Assumes you have:\n",
    "#   real_features:  (N, 5) band-power features (Delta..Gamma)\n",
    "#   labels:         (N,)   class labels {0,1}\n",
    "# Modify BAND_NAMES if needed.\n",
    "\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.covariance import LedoitWolf\n",
    "\n",
    "# -----------------------------\n",
    "# Config / Targets\n",
    "# -----------------------------\n",
    "BAND_NAMES = [\"Delta\",\"Theta\",\"Alpha\",\"Beta\",\"Gamma\"]\n",
    "CLASS_VALUES = np.unique(labels)\n",
    "assert set(CLASS_VALUES) == {0,1}, \"This tuner assumes binary classes {0,1}.\"\n",
    "\n",
    "PARAM_GRID_K = [5, 8, 10, 12, 15]\n",
    "PARAM_GRID_NOISE = [0.00, 0.01, 0.015, 0.02, 0.03]\n",
    "\n",
    "# Targets (edit to taste)\n",
    "TARGET_RVS_MAX = 0.60     # real vs synthetic accuracy <= 0.60\n",
    "TARGET_TSTR_MIN = 0.70    # TSTR >= 0.70\n",
    "TARGET_GAP_MAX  = 0.15    # |TRTR - TSTR| <= 0.15\n",
    "TARGET_KS_MINP  = 0.05    # per-band KS p-value > 0.05\n",
    "TARGET_CORR_SIM = 0.90    # corr-matrix similarity >= 0.90\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# -----------------------------\n",
    "# Helpers: metrics\n",
    "# -----------------------------\n",
    "def ks_pvals_per_band(real, synth):\n",
    "    pvals = []\n",
    "    for j in range(real.shape[1]):\n",
    "        _, p = stats.ks_2samp(real[:, j], synth[:, j])\n",
    "        pvals.append(p)\n",
    "    return np.array(pvals)\n",
    "\n",
    "def rbf_mmd(X, Y, gamma=None):\n",
    "    # Median heuristic for gamma if None\n",
    "    Z = np.vstack([X, Y])\n",
    "    if gamma is None:\n",
    "        d2 = np.sum((Z[:, None, :] - Z[None, :, :])**2, axis=2)\n",
    "        med2 = np.median(d2[d2>0])\n",
    "        gamma = 1.0 / (med2 + 1e-8)\n",
    "    def k(a,b): \n",
    "        d2 = np.sum((a[:,None,:]-b[None,:,:])**2, axis=2)\n",
    "        return np.exp(-gamma * d2)\n",
    "    Kxx = k(X,X); Kyy = k(Y,Y); Kxy = k(X,Y)\n",
    "    m, n = X.shape[0], Y.shape[0]\n",
    "    return Kxx.sum()/(m*m) + Kyy.sum()/(n*n) - 2*Kxy.sum()/(m*n)\n",
    "\n",
    "def corr_matrix_similarity(real, synth):\n",
    "    C_real = np.corrcoef(real.T)\n",
    "    C_synth = np.corrcoef(synth.T)\n",
    "    iu = np.triu_indices_from(C_real, k=1)\n",
    "    r = np.corrcoef(C_real[iu], C_synth[iu])[0,1]\n",
    "    return r\n",
    "\n",
    "def per_band_spearman(real, synth, random_seed=RANDOM_SEED, max_samples=5000):\n",
    "    if real.size == 0 or synth.size == 0:\n",
    "        return np.full(real.shape[1], np.nan)\n",
    "\n",
    "    rng = np.random.default_rng(random_seed)\n",
    "    n = min(len(real), len(synth), max_samples)\n",
    "\n",
    "    if len(real) > n:\n",
    "        idx_real = rng.choice(len(real), size=n, replace=False)\n",
    "    else:\n",
    "        idx_real = np.arange(len(real))\n",
    "\n",
    "    if len(synth) > n:\n",
    "        idx_synth = rng.choice(len(synth), size=n, replace=False)\n",
    "    else:\n",
    "        idx_synth = np.arange(len(synth))\n",
    "\n",
    "    real_sample = real[idx_real]\n",
    "    synth_sample = synth[idx_synth]\n",
    "\n",
    "    vals = []\n",
    "    for j in range(real.shape[1]):\n",
    "        rho = stats.spearmanr(real_sample[:, j], synth_sample[:, j]).correlation\n",
    "        vals.append(rho)\n",
    "    return np.array(vals)\n",
    "\n",
    "def real_vs_synth_accuracy(real, synth):\n",
    "    X = np.vstack([real, synth])\n",
    "    y = np.hstack([np.zeros(len(real), dtype=int), np.ones(len(synth), dtype=int)])\n",
    "    Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.33, random_state=RANDOM_SEED, stratify=y)\n",
    "    clf = RandomForestClassifier(n_estimators=200, random_state=RANDOM_SEED, class_weight=\"balanced\")\n",
    "    clf.fit(Xtr, ytr)\n",
    "    return clf.score(Xte, yte)\n",
    "\n",
    "def tstr_trtr_accuracy(real_X, real_y, synth_X, synth_y):\n",
    "    # Classifier: RF; TSTR = train on synthetic, test on real; TRTR = train on real, test on real\n",
    "    # Split real set for fair TRTR eval\n",
    "    Xtr_r, Xte_r, ytr_r, yte_r = train_test_split(real_X, real_y, test_size=0.33, random_state=RANDOM_SEED, stratify=real_y)\n",
    "    # TRTR\n",
    "    clf_r = RandomForestClassifier(n_estimators=300, random_state=RANDOM_SEED, class_weight=\"balanced\")\n",
    "    clf_r.fit(Xtr_r, ytr_r)\n",
    "    trtr = clf_r.score(Xte_r, yte_r)\n",
    "    # TSTR\n",
    "    clf_s = RandomForestClassifier(n_estimators=300, random_state=RANDOM_SEED, class_weight=\"balanced\")\n",
    "    clf_s.fit(synth_X, synth_y)\n",
    "    tstr = clf_s.score(Xte_r, yte_r)\n",
    "    return tstr, trtr\n",
    "\n",
    "# -----------------------------\n",
    "# Generator: classwise interpolation in log-space\n",
    "# (replace with your own generate_classwise_interpolation_eeg_oneclass if you have it)\n",
    "# -----------------------------\n",
    "def _interp_one_class(Xc, n_out, k_neighbors=10, noise_scale=0.015, random_state=42):\n",
    "    \"\"\"\n",
    "    Classwise interpolation in log-space with covariance-aware jitter.\n",
    "    Robust to small class sizes; ensures k>=2 and <= len(Xc)-1.\n",
    "    \"\"\"\n",
    "    rng = np.random.RandomState(random_state)\n",
    "    eps = 1e-8\n",
    "    Xc = np.asarray(Xc)\n",
    "    if Xc.ndim != 2 or Xc.shape[0] == 0:\n",
    "        raise ValueError(\"Xc must be (n_samples, n_features) with n_samples>0\")\n",
    "\n",
    "    # If the class is too small, just jitter existing points\n",
    "    if Xc.shape[0] == 1:\n",
    "        # log → jitter → exp\n",
    "        xlog = np.log(Xc + eps)\n",
    "        synth_log = np.repeat(xlog, n_out, axis=0)\n",
    "        # fallback covariance: identity\n",
    "        jitter = rng.normal(size=(n_out, Xc.shape[1]))\n",
    "        synth_log = synth_log + noise_scale * jitter\n",
    "        return np.exp(synth_log) - eps\n",
    "\n",
    "    # Work in log-space to keep positivity on exp back-transform\n",
    "    Xlog = np.log(Xc + eps)\n",
    "\n",
    "    # Choose a valid k (at least 2, at most n-1)\n",
    "    n_samp = Xlog.shape[0]\n",
    "    k = int(np.clip(k_neighbors, 2, max(2, n_samp - 1)))\n",
    "\n",
    "    # Fit neighbors in log-space\n",
    "    nbrs = NearestNeighbors(n_neighbors=k, metric='euclidean').fit(Xlog)\n",
    "\n",
    "    # Pick base points uniformly\n",
    "    base_idx = rng.randint(0, n_samp, size=n_out)\n",
    "    base = Xlog[base_idx]\n",
    "\n",
    "    # For each base, pick one neighbor randomly (excluding self is handled by k<=n-1)\n",
    "    neigh_idx = nbrs.kneighbors(base, return_distance=False)\n",
    "    pick = neigh_idx[np.arange(n_out), rng.randint(0, neigh_idx.shape[1], size=n_out)]\n",
    "    neigh = Xlog[pick]\n",
    "\n",
    "    # Random convex combination in [0,1]\n",
    "    alpha = rng.rand(n_out, 1)\n",
    "    synth_log = alpha * base + (1 - alpha) * neigh\n",
    "\n",
    "    # Covariance-aware jitter using Ledoit–Wolf (robust); fallback to diagonal if needed\n",
    "    if noise_scale and noise_scale > 0:\n",
    "        try:\n",
    "            lw = LedoitWolf().fit(Xlog)\n",
    "            S = lw.covariance_\n",
    "            # numeric guard: ensure SPD\n",
    "            # (LedoitWolf should be SPD; add tiny ridge just in case)\n",
    "            S = S + 1e-8 * np.eye(S.shape[0])\n",
    "            jitter = rng.multivariate_normal(mean=np.zeros(Xlog.shape[1]), cov=S, size=n_out)\n",
    "        except Exception:\n",
    "            jitter = rng.normal(size=(n_out, Xlog.shape[1]))\n",
    "        synth_log = synth_log + noise_scale * jitter\n",
    "\n",
    "    synth = np.exp(synth_log) - eps\n",
    "    return synth\n",
    "\n",
    "\n",
    "def generate_classwise_interpolation_both_classes(real_X, real_y, n_per_class, k_params, noise_params):\n",
    "    # k_params = {0: k0, 1: k1}; noise_params = {0: n0, 1: n1}\n",
    "    synth_list, synth_y = [], []\n",
    "    for c in CLASS_VALUES:\n",
    "        Xc = real_X[real_y == c]\n",
    "        synth_c = _interp_one_class(\n",
    "            Xc, n_per_class,\n",
    "            k_neighbors=k_params[c],\n",
    "            noise_scale=noise_params[c],\n",
    "            random_state=RANDOM_SEED + c\n",
    "        )\n",
    "        synth_list.append(synth_c)\n",
    "        synth_y.append(np.full(n_per_class, c, dtype=int))\n",
    "    return np.vstack(synth_list), np.hstack(synth_y)\n",
    "\n",
    "# -----------------------------\n",
    "# Scoring + Early stopping\n",
    "# -----------------------------\n",
    "def score_combo(metrics):\n",
    "    # Higher is better. Penalize violations smoothly.\n",
    "    rvs = metrics[\"rvs_acc\"]\n",
    "    tstr, trtr = metrics[\"tstr\"], metrics[\"trtr\"]\n",
    "    ks_min = metrics[\"ks_min_p\"]\n",
    "    corr_sim = metrics[\"corr_sim\"]\n",
    "\n",
    "    # Base score\n",
    "    s = 0.0\n",
    "    # push RvS down toward 0.55 (reward if <= 0.60, punish otherwise)\n",
    "    s += 2.0 * max(0.0, 0.60 - rvs)\n",
    "    # reward higher TSTR and smaller gap\n",
    "    s += 1.5 * tstr\n",
    "    s += 1.0 * max(0.0, 0.15 - abs(trtr - tstr))\n",
    "    # reward KS min p and correlation similarity\n",
    "    s += 1.0 * min(ks_min, 0.10) * 10.0   # cap effect; scale to ~[0..1]\n",
    "    s += 1.0 * max(0.0, corr_sim - 0.85)  # only reward above 0.85\n",
    "    # small reward for small |MMD|\n",
    "    s += 0.5 * max(0.0, 0.2 - abs(metrics[\"mmd\"]))  # closer to 0 is better\n",
    "\n",
    "    return s\n",
    "\n",
    "def meets_targets(metrics):\n",
    "    return (metrics[\"rvs_acc\"] <= TARGET_RVS_MAX and\n",
    "            metrics[\"tstr\"]     >= TARGET_TSTR_MIN and\n",
    "            abs(metrics[\"trtr\"] - metrics[\"tstr\"]) <= TARGET_GAP_MAX and\n",
    "            metrics[\"ks_min_p\"] >= TARGET_KS_MINP and\n",
    "            metrics[\"corr_sim\"] >= TARGET_CORR_SIM)\n",
    "\n",
    "# -----------------------------\n",
    "# Grid search (per class) but evaluated jointly\n",
    "# -----------------------------\n",
    "def tune_interpolation_params(real_X, real_y, verbose=True):\n",
    "    n_per_class = min(np.sum(real_y==0), np.sum(real_y==1))  # balance\n",
    "    best = {\"score\": -np.inf, \"params\": None, \"metrics\": None}\n",
    "\n",
    "    tried = 0\n",
    "    for k0 in PARAM_GRID_K:\n",
    "        for n0 in PARAM_GRID_NOISE:\n",
    "            for k1 in PARAM_GRID_K:\n",
    "                for n1 in PARAM_GRID_NOISE:\n",
    "                    tried += 1\n",
    "                    k_params = {0: k0, 1: k1}\n",
    "                    noise_params = {0: n0, 1: n1}\n",
    "                    synth_X, synth_y = generate_classwise_interpolation_both_classes(\n",
    "                        real_X, real_y, n_per_class, k_params, noise_params\n",
    "                    )\n",
    "\n",
    "                    # Metrics\n",
    "                    ks_p = ks_pvals_per_band(real_X, synth_X)\n",
    "                    mmd = rbf_mmd(real_X, synth_X, gamma=None)\n",
    "                    rvs = real_vs_synth_accuracy(real_X, synth_X)\n",
    "                    tstr, trtr = tstr_trtr_accuracy(real_X, real_y, synth_X, synth_y)\n",
    "                    corr_sim = corr_matrix_similarity(real_X, synth_X)\n",
    "                    rho = per_band_spearman(real_X, synth_X)\n",
    "\n",
    "                    metrics = {\n",
    "                        \"rvs_acc\": rvs,\n",
    "                        \"tstr\": tstr,\n",
    "                        \"trtr\": trtr,\n",
    "                        \"gap\": abs(trtr - tstr),\n",
    "                        \"ks_min_p\": float(np.min(ks_p)),\n",
    "                        \"mmd\": float(mmd),\n",
    "                        \"corr_sim\": float(corr_sim),\n",
    "                        \"rho_min\": float(np.nanmin(rho)),\n",
    "                        \"rho_mean\": float(np.nanmean(rho)),\n",
    "                    }\n",
    "                    sc = score_combo(metrics)\n",
    "\n",
    "                    if sc > best[\"score\"]:\n",
    "                        best = {\"score\": sc, \"params\": (k_params, noise_params), \"metrics\": metrics, \n",
    "                                \"synth\": (synth_X, synth_y)}\n",
    "\n",
    "                    if verbose and tried % 20 == 0:\n",
    "                        print(f\"[{tried:4d}] k0={k0}, n0={n0:.3f} | k1={k1}, n1={n1:.3f} \"\n",
    "                              f\"RvS={rvs:.3f} TSTR/TRTR={tstr:.3f}/{trtr:.3f} \"\n",
    "                              f\"KSmin={metrics['ks_min_p']:.3f} CorrSim={corr_sim:.3f} MMD={mmd:.3f}\")\n",
    "\n",
    "                    # Early stopping: break as soon as all targets met\n",
    "                    if meets_targets(metrics):\n",
    "                        if verbose:\n",
    "                            print(\"\\n✓ Early-stop: targets met\")\n",
    "                            print(\"  Params:\", k_params, noise_params)\n",
    "                            print(\"  Metrics:\", metrics)\n",
    "                        return {\"best\": best, \"early_stop\": True}\n",
    "\n",
    "    if verbose:\n",
    "        print(\"\\nNo combo met all targets. Returning best observed.\")\n",
    "        print(\"Best params:\", best[\"params\"])\n",
    "        print(\"Best metrics:\", best[\"metrics\"])\n",
    "    return {\"best\": best, \"early_stop\": False}\n",
    "\n",
    "# -----------------------------\n",
    "# Run tuning\n",
    "# -----------------------------\n",
    "result = tune_interpolation_params(real_features, labels, verbose=True)\n",
    "\n",
    "best_params = result[\"best\"][\"params\"]\n",
    "best_metrics = result[\"best\"][\"metrics\"]\n",
    "best_synth_X, best_synth_y = result[\"best\"][\"synth\"]\n",
    "\n",
    "print(\"\\n=== BEST COMBINATION ===\")\n",
    "print(\"k_params:\", best_params[0], \"noise_params:\", best_params[1])\n",
    "print(\"metrics :\", best_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Early-stop: targets met\n",
      "  Params: {0: 5, 1: 5} {0: 0.0, 1: 0.0}\n",
      "  Metrics: {'detect': 0.5027195027195027, 'rvs_acc_raw': 0.5027195027195027, 'rvs_auc': 0.38137230242969966, 'tstr': 0.7011642949547219, 'trtr': 0.6998706338939198, 'gap': 0.001293661060802087, 'ks_min_p': 0.3416900838456176, 'mmd': 0.0012224264816664832, 'corr_sim': 0.9820198663815284, 'rho_min': 0.8251903364370348, 'rho_mean': 0.9218774646917103}\n",
      "\n",
      "=== BEST COMBINATION ===\n",
      "k_params: {0: 5, 1: 5} noise_params: {0: 0.0, 1: 0.0}\n",
      "metrics : {'detect': 0.5027195027195027, 'rvs_acc_raw': 0.5027195027195027, 'rvs_auc': 0.38137230242969966, 'tstr': 0.7011642949547219, 'trtr': 0.6998706338939198, 'gap': 0.001293661060802087, 'ks_min_p': 0.3416900838456176, 'mmd': 0.0012224264816664832, 'corr_sim': 0.9820198663815284, 'rho_min': 0.8251903364370348, 'rho_mean': 0.9218774646917103}\n"
     ]
    }
   ],
   "source": [
    "# === DROP-IN: Robust tuner with detectability, fair TSTR, matched Spearman ===\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.covariance import LedoitWolf\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "BAND_NAMES = [\"Delta\",\"Theta\",\"Alpha\",\"Beta\",\"Gamma\"]\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# Targets\n",
    "TARGET_DETECT_MAX = 0.60   # detectability = max(acc, 1-acc) <= 0.60\n",
    "TARGET_TSTR_MIN   = 0.70\n",
    "TARGET_GAP_MAX    = 0.15\n",
    "TARGET_KS_MINP    = 0.05\n",
    "TARGET_CORR_SIM   = 0.90\n",
    "\n",
    "# Grids (adjust as needed)\n",
    "PARAM_GRID_K      = [5, 8, 10, 12]\n",
    "PARAM_GRID_NOISE  = [0.00, 0.01, 0.015, 0.02]\n",
    "\n",
    "# ---------- Metrics ----------\n",
    "def ks_pvals_per_band(real, synth):\n",
    "    return np.array([stats.ks_2samp(real[:, j], synth[:, j])[1] for j in range(real.shape[1])])\n",
    "\n",
    "def rbf_mmd(X, Y, gamma=None):\n",
    "    Z = np.vstack([X, Y])\n",
    "    if gamma is None:\n",
    "        d2 = np.sum((Z[:, None, :] - Z[None, :, :])**2, axis=2)\n",
    "        med2 = np.median(d2[d2>0])\n",
    "        gamma = 1.0 / (med2 + 1e-8)\n",
    "    def k(a,b):\n",
    "        d2 = np.sum((a[:,None,:]-b[None,:,:])**2, axis=2)\n",
    "        return np.exp(-gamma * d2)\n",
    "    Kxx = k(X,X); Kyy = k(Y,Y); Kxy = k(X,Y)\n",
    "    m, n = len(X), len(Y)\n",
    "    return Kxx.sum()/(m*m) + Kyy.sum()/(n*n) - 2*Kxy.sum()/(m*n)\n",
    "\n",
    "def corr_matrix_similarity(real, synth):\n",
    "    C_r, C_s = np.corrcoef(real.T), np.corrcoef(synth.T)\n",
    "    iu = np.triu_indices_from(C_r, k=1)\n",
    "    return np.corrcoef(C_r[iu], C_s[iu])[0,1]\n",
    "\n",
    "def matched_spearman(real, synth, k=1):\n",
    "    nbrs = NearestNeighbors(n_neighbors=k, metric='euclidean').fit(synth)\n",
    "    idx = nbrs.kneighbors(real, return_distance=False)[:,0]\n",
    "    return np.array([stats.spearmanr(real[:, j], synth[idx, j]).correlation for j in range(real.shape[1])])\n",
    "\n",
    "def real_vs_synth_detectability(real, synth, seed=RANDOM_SEED):\n",
    "    X = np.vstack([real, synth])\n",
    "    y = np.hstack([np.zeros(len(real), dtype=int), np.ones(len(synth), dtype=int)])\n",
    "    Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.33, random_state=seed, stratify=y)\n",
    "    clf = RandomForestClassifier(n_estimators=200, random_state=seed, class_weight=\"balanced\")\n",
    "    clf.fit(Xtr, ytr)\n",
    "    acc = clf.score(Xte, yte)\n",
    "    try:\n",
    "        proba = clf.predict_proba(Xte)[:,1]\n",
    "        auc = roc_auc_score(yte, proba)\n",
    "    except Exception:\n",
    "        auc = 0.5\n",
    "    detect = max(acc, 1.0 - acc)\n",
    "    return detect, acc, auc\n",
    "\n",
    "# ---------- Generator: classwise interpolation in log-space ----------\n",
    "def _interp_one_class(Xc, n_out, k_neighbors=10, noise_scale=0.015, random_state=RANDOM_SEED):\n",
    "    rng = np.random.RandomState(random_state)\n",
    "    eps = 1e-8\n",
    "    Xc = np.asarray(Xc)\n",
    "    if Xc.shape[0] == 1:\n",
    "        xlog = np.log(Xc + eps)\n",
    "        synth_log = np.repeat(xlog, n_out, axis=0)\n",
    "        jitter = rng.normal(size=(n_out, Xc.shape[1]))\n",
    "        synth_log = synth_log + noise_scale * jitter\n",
    "        return np.exp(synth_log) - eps\n",
    "\n",
    "    Xlog = np.log(Xc + eps)\n",
    "    n_samp = Xlog.shape[0]\n",
    "    k = int(np.clip(k_neighbors, 2, max(2, n_samp - 1)))\n",
    "    nbrs = NearestNeighbors(n_neighbors=k, metric='euclidean').fit(Xlog)\n",
    "\n",
    "    base_idx = rng.randint(0, n_samp, size=n_out)\n",
    "    base = Xlog[base_idx]\n",
    "    neigh_idx = nbrs.kneighbors(base, return_distance=False)\n",
    "    pick = neigh_idx[np.arange(n_out), rng.randint(0, neigh_idx.shape[1], size=n_out)]\n",
    "    neigh = Xlog[pick]\n",
    "\n",
    "    alpha = rng.rand(n_out, 1)\n",
    "    synth_log = alpha * base + (1 - alpha) * neigh\n",
    "\n",
    "    if noise_scale and noise_scale > 0:\n",
    "        try:\n",
    "            lw = LedoitWolf().fit(Xlog)\n",
    "            S = lw.covariance_ + 1e-8*np.eye(Xlog.shape[1])\n",
    "            jitter = rng.multivariate_normal(mean=np.zeros(Xlog.shape[1]), cov=S, size=n_out)\n",
    "        except Exception:\n",
    "            jitter = rng.normal(size=(n_out, Xlog.shape[1]))\n",
    "        synth_log = synth_log + noise_scale * jitter\n",
    "\n",
    "    return np.exp(synth_log) - eps\n",
    "\n",
    "def gen_interp_per_class(Xtr_r, ytr_r, n_per_class, k_params, noise_params):\n",
    "    synth_list, synth_y = [], []\n",
    "    for c in np.unique(ytr_r):\n",
    "        Xc = Xtr_r[ytr_r == c]\n",
    "        k_c = int(np.clip(k_params[c], 2, max(2, Xc.shape[0]-1)))\n",
    "        n_c = float(noise_params[c])\n",
    "        synth_c = _interp_one_class(Xc, n_per_class, k_neighbors=k_c, noise_scale=n_c, random_state=RANDOM_SEED + c)\n",
    "        synth_list.append(synth_c)\n",
    "        synth_y.append(np.full(n_per_class, c, dtype=int))\n",
    "    return np.vstack(synth_list), np.hstack(synth_y)\n",
    "\n",
    "# ---------- Fair TSTR/TRTR (train-only generation) ----------\n",
    "def tstr_trtr_fair(real_X, real_y, k_params, noise_params, seed=RANDOM_SEED):\n",
    "    Xtr_r, Xte_r, ytr_r, yte_r = train_test_split(real_X, real_y, test_size=0.33, random_state=seed, stratify=real_y)\n",
    "    n_per_class = min(np.sum(ytr_r==0), np.sum(ytr_r==1))\n",
    "    synth_X, synth_y = gen_interp_per_class(Xtr_r, ytr_r, n_per_class, k_params, noise_params)\n",
    "\n",
    "    clf_r = RandomForestClassifier(n_estimators=300, random_state=seed, class_weight=\"balanced\")\n",
    "    clf_r.fit(Xtr_r, ytr_r)\n",
    "    trtr = clf_r.score(Xte_r, yte_r)\n",
    "\n",
    "    clf_s = RandomForestClassifier(n_estimators=300, random_state=seed, class_weight=\"balanced\")\n",
    "    clf_s.fit(synth_X, synth_y)\n",
    "    tstr = clf_s.score(Xte_r, yte_r)\n",
    "    return tstr, trtr, (Xtr_r, Xte_r, ytr_r, yte_r), (synth_X, synth_y)\n",
    "\n",
    "# ---------- Scoring & Early-stop ----------\n",
    "def score_combo(metrics):\n",
    "    s = 0.0\n",
    "    s += 2.0 * max(0.0, 0.60 - metrics[\"detect\"])      # lower detectability is better\n",
    "    s += 1.5 * metrics[\"tstr\"]                          # higher TSTR is better\n",
    "    s += 1.0 * max(0.0, 0.15 - abs(metrics[\"trtr\"] - metrics[\"tstr\"]))  # small gap\n",
    "    s += 1.0 * min(metrics[\"ks_min_p\"], 0.10) * 10.0    # KS min p (capped)\n",
    "    s += 1.0 * max(0.0, metrics[\"corr_sim\"] - 0.85)     # corr similarity above 0.85\n",
    "    s += 0.5 * max(0.0, 0.2 - abs(metrics[\"mmd\"]))      # MMD close to 0\n",
    "    return s\n",
    "\n",
    "def meets_targets(m):\n",
    "    return (m[\"detect\"] <= TARGET_DETECT_MAX and\n",
    "            m[\"tstr\"]   >= TARGET_TSTR_MIN and\n",
    "            abs(m[\"trtr\"] - m[\"tstr\"]) <= TARGET_GAP_MAX and\n",
    "            m[\"ks_min_p\"] >= TARGET_KS_MINP and\n",
    "            m[\"corr_sim\"] >= TARGET_CORR_SIM)\n",
    "\n",
    "# ---------- Grid search ----------\n",
    "def tune_interpolation_params(real_X, real_y, verbose=True):\n",
    "    classes = np.unique(real_y)\n",
    "    assert set(classes) == {0,1}, \"Binary classes {0,1} expected.\"\n",
    "    best = {\"score\": -np.inf, \"params\": None, \"metrics\": None, \"artifacts\": None}\n",
    "    tried = 0\n",
    "\n",
    "    for k0 in PARAM_GRID_K:\n",
    "        for n0 in PARAM_GRID_NOISE:\n",
    "            for k1 in PARAM_GRID_K:\n",
    "                for n1 in PARAM_GRID_NOISE:\n",
    "                    tried += 1\n",
    "                    k_params     = {0: k0, 1: k1}\n",
    "                    noise_params = {0: n0, 1: n1}\n",
    "\n",
    "                    # fair TSTR/TRTR with train-only generation\n",
    "                    tstr, trtr, (Xtr_r, Xte_r, ytr_r, yte_r), (synth_X, synth_y) = tstr_trtr_fair(\n",
    "                        real_X, real_y, k_params, noise_params, seed=RANDOM_SEED\n",
    "                    )\n",
    "\n",
    "                    # evaluate detectability on the same real (train+test) pool vs synth\n",
    "                    detect, acc_raw, auc = real_vs_synth_detectability(real_X, synth_X, seed=RANDOM_SEED)\n",
    "                    ks_p = ks_pvals_per_band(real_X, synth_X)\n",
    "                    mmd  = rbf_mmd(real_X, synth_X)\n",
    "                    corr = corr_matrix_similarity(real_X, synth_X)\n",
    "                    rho  = matched_spearman(real_X, synth_X, k=1)\n",
    "\n",
    "                    metrics = {\n",
    "                        \"detect\": float(detect),\n",
    "                        \"rvs_acc_raw\": float(acc_raw),\n",
    "                        \"rvs_auc\": float(auc),\n",
    "                        \"tstr\": float(tstr),\n",
    "                        \"trtr\": float(trtr),\n",
    "                        \"gap\": float(abs(trtr - tstr)),\n",
    "                        \"ks_min_p\": float(np.min(ks_p)),\n",
    "                        \"mmd\": float(mmd),\n",
    "                        \"corr_sim\": float(corr),\n",
    "                        \"rho_min\": float(np.nanmin(rho)),\n",
    "                        \"rho_mean\": float(np.nanmean(rho)),\n",
    "                    }\n",
    "                    sc = score_combo(metrics)\n",
    "\n",
    "                    if sc > best[\"score\"]:\n",
    "                        best = {\"score\": sc,\n",
    "                                \"params\": (k_params, noise_params),\n",
    "                                \"metrics\": metrics,\n",
    "                                \"artifacts\": {\"synth_X\": synth_X, \"synth_y\": synth_y}}\n",
    "\n",
    "                    if verbose and tried % 20 == 0:\n",
    "                        print(f\"[{tried:4d}] k0={k0}, n0={n0:.3f} | k1={k1}, n1={n1:.3f} \"\n",
    "                              f\"Detect={metrics['detect']:.3f} (acc={metrics['rvs_acc_raw']:.3f}, AUC={metrics['rvs_auc']:.3f}) \"\n",
    "                              f\"TSTR/TRTR={tstr:.3f}/{trtr:.3f} KSmin={metrics['ks_min_p']:.3f} \"\n",
    "                              f\"CorrSim={corr:.3f} MMD={mmd:.4f} ρ_mean={metrics['rho_mean']:.3f}\")\n",
    "\n",
    "                    if meets_targets(metrics):\n",
    "                        if verbose:\n",
    "                            print(\"\\n✓ Early-stop: targets met\")\n",
    "                            print(\"  Params:\", k_params, noise_params)\n",
    "                            print(\"  Metrics:\", metrics)\n",
    "                        return {\"best\": best, \"early_stop\": True}\n",
    "\n",
    "    if verbose:\n",
    "        print(\"\\nNo combo met all targets. Returning best observed.\")\n",
    "        print(\"Best params:\", best[\"params\"])\n",
    "        print(\"Best metrics:\", best[\"metrics\"])\n",
    "    return {\"best\": best, \"early_stop\": False}\n",
    "\n",
    "# Run it\n",
    "result = tune_interpolation_params(real_features, labels, verbose=True)\n",
    "best_params  = result[\"best\"][\"params\"]\n",
    "best_metrics = result[\"best\"][\"metrics\"]\n",
    "print(\"\\n=== BEST COMBINATION ===\")\n",
    "print(\"k_params:\", best_params[0], \"noise_params:\", best_params[1])\n",
    "print(\"metrics :\", best_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'k_params': {0: 5, 1: 5}, 'noise_params': {0: 0.0, 1: 0.0}, 'detectability': 0.5027195027195027, 'rvs_acc_raw': 0.5027195027195027, 'rvs_auc': 0.38137230242969966, 'tstr': 0.7011642949547219, 'trtr': 0.6998706338939198, 'gap': 0.001293661060802087, 'ks_min_p': 0.3416900838456176, 'mmd': 0.0012224264816664832, 'corr_sim': 0.9820198663815284, 'rho_mean': 0.9218774646917103, 'rho_min': 0.8251903364370348}\n"
     ]
    }
   ],
   "source": [
    "BEST_K = {0: 5, 1: 5}\n",
    "BEST_NOISE = {0: 0.0, 1: 0.0}\n",
    "SEED = 42\n",
    "\n",
    "# Regenerate balanced synthetic set using train-only split for fairness\n",
    "Xtr_r, Xte_r, ytr_r, yte_r = train_test_split(real_features, labels, test_size=0.33,\n",
    "                                              random_state=SEED, stratify=labels)\n",
    "n_per_class = min(np.sum(ytr_r==0), np.sum(ytr_r==1))\n",
    "synth_X, synth_y = gen_interp_per_class(Xtr_r, ytr_r, n_per_class,\n",
    "                                        k_params=BEST_K, noise_params=BEST_NOISE)\n",
    "\n",
    "# Recompute headline metrics (detectability/TSTR/TRTR/KS/MMD/corr/rho)\n",
    "detect, acc_raw, auc = real_vs_synth_detectability(real_features, synth_X, seed=SEED)\n",
    "tstr, trtr, *_ = tstr_trtr_fair(real_features, labels, BEST_K, BEST_NOISE, seed=SEED)\n",
    "ks_p = ks_pvals_per_band(real_features, synth_X)\n",
    "mmd  = rbf_mmd(real_features, synth_X)\n",
    "corr = corr_matrix_similarity(real_features, synth_X)\n",
    "rho  = matched_spearman(real_features, synth_X, k=1)\n",
    "\n",
    "summary = {\n",
    "    \"k_params\": BEST_K, \"noise_params\": BEST_NOISE,\n",
    "    \"detectability\": float(detect), \"rvs_acc_raw\": float(acc_raw), \"rvs_auc\": float(auc),\n",
    "    \"tstr\": float(tstr), \"trtr\": float(trtr), \"gap\": float(abs(trtr - tstr)),\n",
    "    \"ks_min_p\": float(np.min(ks_p)), \"mmd\": float(mmd),\n",
    "    \"corr_sim\": float(corr), \"rho_mean\": float(np.nanmean(rho)), \"rho_min\": float(np.nanmin(rho))\n",
    "}\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to ../output/synth_interp_best.pkl\n"
     ]
    }
   ],
   "source": [
    "# Save artifacts\n",
    "import pickle, os\n",
    "os.makedirs(\"../output\", exist_ok=True)\n",
    "with open(\"../output/synth_interp_best.pkl\", \"wb\") as f:\n",
    "    pickle.dump({\"synth_X\": synth_X, \"synth_y\": synth_y, \"summary\": summary}, f)\n",
    "print(\"Saved to ../output/synth_interp_best.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ollama_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
