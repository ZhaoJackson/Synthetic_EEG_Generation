{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 3: Synthetic EEG Data Generation\n",
    "\n",
    "## Objective\n",
    "\n",
    "Build a robust foundation for synthetic EEG generation that accurately reflects the structure of the original Alcoholic vs Control EEG dataset, while respecting:\n",
    "\n",
    "1. subject independence to prevent information leakage\n",
    "2. condition specificity across S1, S2-match, S2-nomatch\n",
    "3. class structure (control vs alcoholic)\n",
    "4. realistic band-power relationships across Delta, Theta, Alpha, Beta, Gamma, and Total Power"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0. Load package & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "from collections import defaultdict, Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.covariance import LedoitWolf\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "plt.rcParams[\"figure.figsize\"] = (6, 4)\n",
    "\n",
    "DATA_DIR = Path(\"../output/band_extraction\")\n",
    "OUT_DIR = Path(\"../output/synthetic_generation\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Processed data features check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1. Dataset overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data structure\n",
    "LABEL_COL = \"subject_type\"         \n",
    "CONDITION_COL = \"matching_condition\"   \n",
    "SPLIT_COL = \"dataset_split\"\n",
    "META_COLS = [\"dataset_split\", \"file_name\", \"subject_type\", \"subject_id\", \"channel\", \"trial\", \"matching_condition\", \"Delta\", \"Theta\", \"Alpha\", \"Beta\", \"Gamma\", \"total_power\"]\n",
    "BAND_COLS = [\"Delta\", \"Theta\", \"Alpha\", \"Beta\", \"Gamma\", \"total_power\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full data shape: (60672, 13)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset_split</th>\n",
       "      <th>file_name</th>\n",
       "      <th>subject_type</th>\n",
       "      <th>subject_id</th>\n",
       "      <th>channel</th>\n",
       "      <th>trial</th>\n",
       "      <th>matching_condition</th>\n",
       "      <th>Delta</th>\n",
       "      <th>Theta</th>\n",
       "      <th>Alpha</th>\n",
       "      <th>Beta</th>\n",
       "      <th>Gamma</th>\n",
       "      <th>total_power</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train</td>\n",
       "      <td>Data1.csv</td>\n",
       "      <td>a</td>\n",
       "      <td>co2a0000364</td>\n",
       "      <td>FP1</td>\n",
       "      <td>0</td>\n",
       "      <td>S1 obj</td>\n",
       "      <td>20.048105</td>\n",
       "      <td>5.830134</td>\n",
       "      <td>0.854299</td>\n",
       "      <td>6.705598</td>\n",
       "      <td>6.848762</td>\n",
       "      <td>40.286898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train</td>\n",
       "      <td>Data1.csv</td>\n",
       "      <td>a</td>\n",
       "      <td>co2a0000364</td>\n",
       "      <td>FP2</td>\n",
       "      <td>0</td>\n",
       "      <td>S1 obj</td>\n",
       "      <td>21.769006</td>\n",
       "      <td>6.052321</td>\n",
       "      <td>1.013807</td>\n",
       "      <td>16.487621</td>\n",
       "      <td>15.773774</td>\n",
       "      <td>61.096530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train</td>\n",
       "      <td>Data1.csv</td>\n",
       "      <td>a</td>\n",
       "      <td>co2a0000364</td>\n",
       "      <td>F7</td>\n",
       "      <td>0</td>\n",
       "      <td>S1 obj</td>\n",
       "      <td>7.742259</td>\n",
       "      <td>6.272004</td>\n",
       "      <td>1.893497</td>\n",
       "      <td>39.119253</td>\n",
       "      <td>49.533282</td>\n",
       "      <td>104.560295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train</td>\n",
       "      <td>Data1.csv</td>\n",
       "      <td>a</td>\n",
       "      <td>co2a0000364</td>\n",
       "      <td>F8</td>\n",
       "      <td>0</td>\n",
       "      <td>S1 obj</td>\n",
       "      <td>11.400244</td>\n",
       "      <td>4.816262</td>\n",
       "      <td>2.360998</td>\n",
       "      <td>53.646940</td>\n",
       "      <td>44.502180</td>\n",
       "      <td>116.726624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train</td>\n",
       "      <td>Data1.csv</td>\n",
       "      <td>a</td>\n",
       "      <td>co2a0000364</td>\n",
       "      <td>AF1</td>\n",
       "      <td>0</td>\n",
       "      <td>S1 obj</td>\n",
       "      <td>13.188257</td>\n",
       "      <td>2.347635</td>\n",
       "      <td>0.542750</td>\n",
       "      <td>4.036543</td>\n",
       "      <td>2.914738</td>\n",
       "      <td>23.029923</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  dataset_split  file_name subject_type   subject_id channel  trial  \\\n",
       "0         train  Data1.csv            a  co2a0000364     FP1      0   \n",
       "1         train  Data1.csv            a  co2a0000364     FP2      0   \n",
       "2         train  Data1.csv            a  co2a0000364      F7      0   \n",
       "3         train  Data1.csv            a  co2a0000364      F8      0   \n",
       "4         train  Data1.csv            a  co2a0000364     AF1      0   \n",
       "\n",
       "  matching_condition      Delta     Theta     Alpha       Beta      Gamma  \\\n",
       "0             S1 obj  20.048105  5.830134  0.854299   6.705598   6.848762   \n",
       "1             S1 obj  21.769006  6.052321  1.013807  16.487621  15.773774   \n",
       "2             S1 obj   7.742259  6.272004  1.893497  39.119253  49.533282   \n",
       "3             S1 obj  11.400244  4.816262  2.360998  53.646940  44.502180   \n",
       "4             S1 obj  13.188257  2.347635  0.542750   4.036543   2.914738   \n",
       "\n",
       "   total_power  \n",
       "0    40.286898  \n",
       "1    61.096530  \n",
       "2   104.560295  \n",
       "3   116.726624  \n",
       "4    23.029923  "
      ]
     },
     "execution_count": 461,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load current feature dataset\n",
    "FEATURE_FP = DATA_DIR / \"band_features_segments.csv\"\n",
    "df_all = pd.read_csv(FEATURE_FP)\n",
    "print(\"Full data shape:\", df_all.shape)\n",
    "df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split counts:\n",
      "dataset_split\n",
      "test     30720\n",
      "train    29952\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Basic sanity checks\n",
    "print(\"Split counts:\")\n",
    "print(df_all[SPLIT_COL].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject_type counts:\n",
      "subject_type\n",
      "a    30400\n",
      "c    30272\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Subject_type counts:\")\n",
    "print(df_all[LABEL_COL].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching_condition counts:\n",
      "matching_condition\n",
      "S1 obj         20480\n",
      "S2 match       20416\n",
      "S2 nomatch,    19776\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Matching_condition counts:\")\n",
    "print(df_all[CONDITION_COL].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (29952, 13)\n",
      "Test shape : (30720, 13)\n"
     ]
    }
   ],
   "source": [
    "# Train / Test based on dataset_split\n",
    "df_train = df_all[df_all[SPLIT_COL] == \"train\"].reset_index(drop=True)\n",
    "df_test = df_all[df_all[SPLIT_COL] == \"test\"].reset_index(drop=True)\n",
    "print(\"Train shape:\", df_train.shape)\n",
    "print(\"Test shape :\", df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train label x condition:\n",
      "matching_condition  S1 obj  S2 match  S2 nomatch,\n",
      "subject_type                                     \n",
      "a                     5120      5120         4800\n",
      "c                     5120      5056         4736\n"
     ]
    }
   ],
   "source": [
    "print(\"Train label x condition:\")\n",
    "print(pd.crosstab(df_train[LABEL_COL], df_train[CONDITION_COL]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test label x condition:\n",
      "matching_condition  S1 obj  S2 match  S2 nomatch,\n",
      "subject_type                                     \n",
      "a                     5120      5120         5120\n",
      "c                     5120      5120         5120\n"
     ]
    }
   ],
   "source": [
    "print(\"Test label x condition:\")\n",
    "print(pd.crosstab(df_test[LABEL_COL], df_test[CONDITION_COL]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2. Basic check for missing values and infinite features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check missing and extra columns\n",
    "missing = [c for c in META_COLS if c not in df_all.columns]\n",
    "extra = [c for c in df_all.columns if c not in META_COLS]\n",
    "\n",
    "if missing:\n",
    "    print(\"MISSING columns:\", missing)\n",
    "if extra:\n",
    "    print(\"EXTRA columns:\", extra)\n",
    "if not missing:\n",
    "    df_all = df_all[META_COLS].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data types:\n",
      "dataset_split          object\n",
      "file_name              object\n",
      "subject_type           object\n",
      "subject_id             object\n",
      "channel                object\n",
      "trial                   int64\n",
      "matching_condition     object\n",
      "Delta                 float64\n",
      "Theta                 float64\n",
      "Alpha                 float64\n",
      "Beta                  float64\n",
      "Gamma                 float64\n",
      "total_power           float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Force to numeric and report any conversion issues\n",
    "for col in BAND_COLS:\n",
    "    df_all[col] = pd.to_numeric(df_all[col], errors=\"coerce\")\n",
    "\n",
    "print(\"Data types:\")\n",
    "print(df_all.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values per column:\n",
      "dataset_split         0\n",
      "file_name             0\n",
      "subject_type          0\n",
      "subject_id            0\n",
      "channel               0\n",
      "trial                 0\n",
      "matching_condition    0\n",
      "Delta                 0\n",
      "Theta                 0\n",
      "Alpha                 0\n",
      "Beta                  0\n",
      "Gamma                 0\n",
      "total_power           0\n",
      "dtype: int64\n",
      "Rows with NaN in any feature: 0\n"
     ]
    }
   ],
   "source": [
    "# Check missing values\n",
    "print(\"Missing values per column:\")\n",
    "print(df_all.isna().sum())\n",
    "mask_nan_features = df_all[BAND_COLS].isna().any(axis=1)\n",
    "n_nan_rows = mask_nan_features.sum()\n",
    "print(f\"Rows with NaN in any feature: {n_nan_rows}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with non-finite feature values: 0\n"
     ]
    }
   ],
   "source": [
    "# Check for non-finite values\n",
    "mask_nonfinite = ~np.isfinite(df_all[BAND_COLS].to_numpy()).all(axis=1)\n",
    "n_nonfinite = mask_nonfinite.sum()\n",
    "print(f\"Rows with non-finite feature values: {n_nonfinite}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.3. Clip outlier check and process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st percentile for features:\n",
      " Delta          0.254321\n",
      "Theta          0.126250\n",
      "Alpha          0.108306\n",
      "Beta           0.191547\n",
      "Gamma          0.057350\n",
      "total_power    1.368929\n",
      "Name: 0.01, dtype: float64\n",
      "99th percentile for features:\n",
      " Delta          125.841570\n",
      "Theta           37.735722\n",
      "Alpha           42.706482\n",
      "Beta            23.624554\n",
      "Gamma           18.142717\n",
      "total_power    187.203228\n",
      "Name: 0.99, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# clip extreme outliers to stabilize covariance\n",
    "q_low = df_all[BAND_COLS].quantile(0.01)\n",
    "q_high = df_all[BAND_COLS].quantile(0.99)\n",
    "\n",
    "print(\"1st percentile for features:\\n\", q_low)\n",
    "print(\"99th percentile for features:\\n\", q_high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After clipping, feature summary (all):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Delta</th>\n",
       "      <th>Theta</th>\n",
       "      <th>Alpha</th>\n",
       "      <th>Beta</th>\n",
       "      <th>Gamma</th>\n",
       "      <th>total_power</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>60672.000000</td>\n",
       "      <td>60672.000000</td>\n",
       "      <td>60672.000000</td>\n",
       "      <td>60672.000000</td>\n",
       "      <td>60672.000000</td>\n",
       "      <td>60672.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>14.194337</td>\n",
       "      <td>5.210760</td>\n",
       "      <td>5.212400</td>\n",
       "      <td>4.203085</td>\n",
       "      <td>1.733450</td>\n",
       "      <td>31.266163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>19.773330</td>\n",
       "      <td>6.283680</td>\n",
       "      <td>7.203401</td>\n",
       "      <td>4.136839</td>\n",
       "      <td>2.747217</td>\n",
       "      <td>31.317794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.254321</td>\n",
       "      <td>0.126250</td>\n",
       "      <td>0.108306</td>\n",
       "      <td>0.191547</td>\n",
       "      <td>0.057350</td>\n",
       "      <td>1.368929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1%</th>\n",
       "      <td>0.254393</td>\n",
       "      <td>0.126368</td>\n",
       "      <td>0.108400</td>\n",
       "      <td>0.191558</td>\n",
       "      <td>0.057352</td>\n",
       "      <td>1.368965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>7.512234</td>\n",
       "      <td>3.138961</td>\n",
       "      <td>2.681088</td>\n",
       "      <td>2.934805</td>\n",
       "      <td>0.805226</td>\n",
       "      <td>22.106393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99%</th>\n",
       "      <td>125.826750</td>\n",
       "      <td>37.729023</td>\n",
       "      <td>42.705766</td>\n",
       "      <td>23.622675</td>\n",
       "      <td>18.129812</td>\n",
       "      <td>187.112226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>125.841570</td>\n",
       "      <td>37.735722</td>\n",
       "      <td>42.706482</td>\n",
       "      <td>23.624554</td>\n",
       "      <td>18.142717</td>\n",
       "      <td>187.203228</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Delta         Theta         Alpha          Beta         Gamma  \\\n",
       "count  60672.000000  60672.000000  60672.000000  60672.000000  60672.000000   \n",
       "mean      14.194337      5.210760      5.212400      4.203085      1.733450   \n",
       "std       19.773330      6.283680      7.203401      4.136839      2.747217   \n",
       "min        0.254321      0.126250      0.108306      0.191547      0.057350   \n",
       "1%         0.254393      0.126368      0.108400      0.191558      0.057352   \n",
       "50%        7.512234      3.138961      2.681088      2.934805      0.805226   \n",
       "99%      125.826750     37.729023     42.705766     23.622675     18.129812   \n",
       "max      125.841570     37.735722     42.706482     23.624554     18.142717   \n",
       "\n",
       "        total_power  \n",
       "count  60672.000000  \n",
       "mean      31.266163  \n",
       "std       31.317794  \n",
       "min        1.368929  \n",
       "1%         1.368965  \n",
       "50%       22.106393  \n",
       "99%      187.112226  \n",
       "max      187.203228  "
      ]
     },
     "execution_count": 473,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clip outliers FIRST on full dataset\n",
    "df_all[BAND_COLS] = df_all[BAND_COLS].clip(lower=q_low, upper=q_high, axis=1)\n",
    "print(\"After clipping, feature summary (all):\")\n",
    "df_all[BAND_COLS].describe(percentiles=[0.01, 0.5, 0.99])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.4. Subject-wise train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# unique subjects in train: 16\n",
      "# unique subjects in test: 16\n",
      "# overlapping subjects: 16\n",
      "WARNING: Some subject_ids appear in BOTH train and test!\n"
     ]
    }
   ],
   "source": [
    "# make sure subjects don't appear in both splits\n",
    "train_subj = set(df_train[\"subject_id\"])\n",
    "test_subj = set(df_test[\"subject_id\"])\n",
    "overlap = train_subj & test_subj\n",
    "\n",
    "print(f\"# unique subjects in train: {len(train_subj)}\")\n",
    "print(f\"# unique subjects in test: {len(test_subj)}\")\n",
    "print(f\"# overlapping subjects: {len(overlap)}\")\n",
    "\n",
    "if overlap:\n",
    "    print(\"WARNING: Some subject_ids appear in BOTH train and test!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subject-wise train/test split\n",
    "subjects = df_all[\"subject_id\"].unique()\n",
    "rng = np.random.default_rng(42)\n",
    "rng.shuffle(subjects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train subjects: 8\n",
      "Test subjects : 8\n",
      "Overlap       : 0\n"
     ]
    }
   ],
   "source": [
    "# 50/50 split\n",
    "n_train = len(subjects) // 2\n",
    "train_subjects = set(subjects[:n_train])\n",
    "test_subjects = set(subjects[n_train:])\n",
    "\n",
    "df_train = df_all[df_all[\"subject_id\"].isin(train_subjects)].reset_index(drop=True)\n",
    "df_test = df_all[df_all[\"subject_id\"].isin(test_subjects)].reset_index(drop=True)\n",
    "\n",
    "print(\"Train subjects:\", len(train_subjects))\n",
    "print(\"Test subjects :\", len(test_subjects))\n",
    "print(\"Overlap       :\", len(train_subjects & test_subjects))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.5. Check of cleaned data for synthetic generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split counts with new split:\n",
      "dataset_split\n",
      "train    30336\n",
      "test     30336\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Overwrite dataset_split using the NEW split\n",
    "df_all[SPLIT_COL] = np.where(df_all[\"subject_id\"].isin(train_subjects), \"train\", \"test\")\n",
    "df_train = df_all[df_all[\"subject_id\"].isin(train_subjects)].reset_index(drop=True)\n",
    "df_test = df_all[df_all[\"subject_id\"].isin(test_subjects)].reset_index(drop=True)\n",
    "print(\"Split counts with new split:\")\n",
    "print(df_all[SPLIT_COL].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape with new split: (30336, 13)\n",
      "Test shape with new split: (30336, 13)\n"
     ]
    }
   ],
   "source": [
    "print(\"Train shape with new split:\", df_train.shape)\n",
    "print(\"Test shape with new split:\", df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train label x condition with new split:\n",
      "matching_condition  S1 obj  S2 match  S2 nomatch,\n",
      "subject_type                                     \n",
      "a                     5120      5120         4992\n",
      "c                     5120      5056         4928\n",
      "Test label x condition with new split:\n",
      "matching_condition  S1 obj  S2 match  S2 nomatch,\n",
      "subject_type                                     \n",
      "a                     5120      5120         4928\n",
      "c                     5120      5120         4928\n"
     ]
    }
   ],
   "source": [
    "print(\"Train label x condition with new split:\")\n",
    "print(pd.crosstab(df_train[LABEL_COL], df_train[\"matching_condition\"]))\n",
    "\n",
    "print(\"Test label x condition with new split:\")\n",
    "print(pd.crosstab(df_test[LABEL_COL], df_test[\"matching_condition\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.6. Encode labels & standardize features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final matrices ready for synthetic generation:\n",
      "X_train shape: (30336, 6)\n",
      "X_test shape: (30336, 6)\n",
      "y_train distribution: Counter({1: 15232, 0: 15104})\n",
      "y_test distribution: Counter({1: 15168, 0: 15168})\n",
      "Train condition counts: Counter({'S1 obj': 10240, 'S2 match': 10176, 'S2 nomatch,': 9920})\n",
      "Test condition counts: Counter({'S1 obj': 10240, 'S2 match': 10240, 'S2 nomatch,': 9856})\n"
     ]
    }
   ],
   "source": [
    "label_map = {\"c\": 0, \"a\": 1}\n",
    "\n",
    "y_train = df_train[LABEL_COL].map(label_map).values\n",
    "y_test = df_test[LABEL_COL].map(label_map).values\n",
    "\n",
    "X_train_raw = df_train[BAND_COLS].values\n",
    "X_test_raw = df_test[BAND_COLS].values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train_raw)\n",
    "X_test = scaler.transform(X_test_raw)\n",
    "\n",
    "conds_train = df_train[CONDITION_COL].values\n",
    "conds_test = df_test[CONDITION_COL].values\n",
    "\n",
    "print(\"Final matrices ready for synthetic generation:\")\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_train distribution:\", Counter(y_train))\n",
    "print(\"y_test distribution:\", Counter(y_test))\n",
    "print(\"Train condition counts:\", Counter(conds_train))\n",
    "print(\"Test condition counts:\", Counter(conds_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.7: Condition-conditional setup\n",
    "- All the synthetic generation is done within each task condition separately (S1 obj / S2 match / S2 nomatch), instead of pooling them together.\n",
    "1. All synthetic generation is restricted to one condition at a time:\n",
    "- S1: use only S1 trials\n",
    "- S2 match: use only S2 match trials\n",
    "- S2 nomatch: use only S2 nomatch trials\n",
    "\n",
    "2. The synthetic data you produce for S1 is not influenced by S2 distributions, and vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conditions in train: ['S1 obj', 'S2 match', 'S2 nomatch,']\n"
     ]
    }
   ],
   "source": [
    "# Unique conditions in train\n",
    "label_map = {\"c\": 0, \"a\": 1}\n",
    "\n",
    "unique_conditions = sorted(df_train[CONDITION_COL].unique())\n",
    "print(\"Conditions in train:\", unique_conditions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- P(X | condition = S1) is modelled separately from P(X | condition = S2 match) and P(X | condition = S2 nomatch).\n",
    "- every synthetic sample is generated from a model that was trained only on data from the same task condition (S1 / S2 match / S2 nomatch), so each condition gets its own generative model and distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_condition_slice(df, condition, band_cols, label_col):\n",
    "    mask = df[CONDITION_COL] == condition\n",
    "    df_cond = df.loc[mask]\n",
    "\n",
    "    features_cond = df_cond[band_cols].to_numpy()\n",
    "    labels_cond = df_cond[label_col].map(label_map).to_numpy()\n",
    "\n",
    "    return features_cond, labels_cond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit scaler on REAL training band features\n",
    "real_features_train = df_train[BAND_COLS].to_numpy()\n",
    "scaler = StandardScaler().fit(real_features_train)\n",
    "\n",
    "# keep scaled real train for later comparison / saving\n",
    "X_train = scaler.transform(real_features_train)\n",
    "y_train = df_train[LABEL_COL].map(label_map).to_numpy()\n",
    "conds_train = df_train[CONDITION_COL].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Synthetic Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1. Method 0: Mixup baseline Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mixup_baseline(real_features, n_synthetic=None, random_seed=RANDOM_SEED):\n",
    "    \"\"\"\n",
    "    Mixup-style baseline for synthetic EEG feature generation.\n",
    "    \n",
    "    - Interpolates between two real samples\n",
    "    - Adds small Gaussian noise proportional to feature std\n",
    "    - Ensures same dimensionality as the input (6 bands)\n",
    "    \"\"\"\n",
    "    np.random.seed(random_seed)\n",
    "    \n",
    "    if n_synthetic is None:\n",
    "        n_synthetic = len(real_features)\n",
    "\n",
    "    n_samples, n_features = real_features.shape\n",
    "    synthetic_features = np.zeros((n_synthetic, n_features))\n",
    "\n",
    "    # Noise scale\n",
    "    noise_scale = 0.1 * np.std(real_features, axis=0)\n",
    "\n",
    "    for i in range(n_synthetic):\n",
    "        idx1, idx2 = np.random.choice(n_samples, 2, replace=False)\n",
    "\n",
    "        alpha = np.random.uniform(0.3, 0.7)\n",
    "        interpolated = alpha * real_features[idx1] + (1 - alpha) * real_features[idx2]\n",
    "\n",
    "        # Add Gaussian noise\n",
    "        noise = np.random.normal(0, noise_scale)\n",
    "        synthetic = interpolated + noise\n",
    "\n",
    "        # Ensure non-negative powers\n",
    "        synthetic = np.clip(synthetic, 0, None)\n",
    "\n",
    "        synthetic_features[i] = synthetic\n",
    "\n",
    "    return synthetic_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Mixup: only mixes examples from the same condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mixup_by_condition(df_train, band_cols, label_col, scaler, random_seed=RANDOM_SEED):\n",
    "    \"\"\"\n",
    "    Generate mixup-based synthetic samples *per condition* (S1 obj / S2 match / S2 nomatch,).\n",
    "\n",
    "    For each condition in df_train:\n",
    "      - take real band features only from that condition\n",
    "      - generate the same number of synthetic samples via mixup\n",
    "      - standardize using the global scaler\n",
    "      - keep labels (a/c -> 1/0) and condition tokens\n",
    "\n",
    "    Returns:\n",
    "      X_syn_mixup : (N_train, n_bands) standardized synthetic features\n",
    "      y_syn_mixup : (N_train,) labels (0/1)\n",
    "      cond_syn    : (N_train,) condition tokens\n",
    "    \"\"\"\n",
    "    unique_conditions = sorted(df_train[CONDITION_COL].unique())\n",
    "    print(\"Conditions in train:\", unique_conditions)\n",
    "\n",
    "    X_syn_list, y_syn_list, cond_syn_list = [], [], []\n",
    "\n",
    "    for i, cond in enumerate(unique_conditions):\n",
    "        # slice by condition\n",
    "        real_cond, labels_cond = get_condition_slice(\n",
    "            df=df_train,\n",
    "            condition=cond,\n",
    "            band_cols=band_cols,\n",
    "            label_col=label_col,\n",
    "        )\n",
    "\n",
    "        n_synth_cond = real_cond.shape[0]\n",
    "        print(f\"[Mixup] Condition={cond}, real={n_synth_cond}, synth={n_synth_cond}\")\n",
    "\n",
    "        # generate synthetic features for this condition only\n",
    "        syn_raw = generate_mixup_baseline(\n",
    "            real_features=real_cond,\n",
    "            n_synthetic=n_synth_cond,\n",
    "            random_seed=random_seed + i,\n",
    "        )\n",
    "\n",
    "        # standardize using global scaler fitted on REAL train\n",
    "        X_syn_cond = scaler.transform(syn_raw)\n",
    "\n",
    "        # reuse labels and attach condition token\n",
    "        y_syn_cond = labels_cond.copy()\n",
    "        cond_tokens = np.full(n_synth_cond, cond, dtype=object)\n",
    "\n",
    "        X_syn_list.append(X_syn_cond)\n",
    "        y_syn_list.append(y_syn_cond)\n",
    "        cond_syn_list.append(cond_tokens)\n",
    "\n",
    "    # concatenate all conditions\n",
    "    X_syn_mixup = np.vstack(X_syn_list)\n",
    "    y_syn_mixup = np.concatenate(y_syn_list)\n",
    "    cond_syn_mixup = np.concatenate(cond_syn_list)\n",
    "\n",
    "    # shuffle to remove any block structure\n",
    "    perm = np.random.permutation(len(y_syn_mixup))\n",
    "    X_syn_mixup = X_syn_mixup[perm]\n",
    "    y_syn_mixup = y_syn_mixup[perm]\n",
    "    cond_syn_mixup = cond_syn_mixup[perm]\n",
    "\n",
    "    print(\"Final condition-conditional mixup shape:\", X_syn_mixup.shape)\n",
    "    print(\"Label dist:\", Counter(y_syn_mixup))\n",
    "    print(\"Condition dist:\", Counter(cond_syn_mixup))\n",
    "\n",
    "    return X_syn_mixup, y_syn_mixup, cond_syn_mixup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Mixup is strictly condition-conditional; label and condition distributions match the real train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conditions in train: ['S1 obj', 'S2 match', 'S2 nomatch,']\n",
      "[Mixup] Condition=S1 obj, real=10240, synth=10240\n",
      "[Mixup] Condition=S2 match, real=10176, synth=10176\n",
      "[Mixup] Condition=S2 nomatch,, real=9920, synth=9920\n",
      "Final condition-conditional mixup shape: (30336, 6)\n",
      "Label dist: Counter({1: 15232, 0: 15104})\n",
      "Condition dist: Counter({'S1 obj': 10240, 'S2 match': 10176, 'S2 nomatch,': 9920})\n"
     ]
    }
   ],
   "source": [
    "X_syn_mixup, y_syn_mixup, cond_syn_mixup = generate_mixup_by_condition(\n",
    "    df_train=df_train,\n",
    "    band_cols=BAND_COLS,\n",
    "    label_col=LABEL_COL,\n",
    "    scaler=scaler,\n",
    "    random_seed=RANDOM_SEED,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 Method 1: Correlation Sampling Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_correlation_based_eeg(real_features, band_names, n_synthetic=None, random_seed=RANDOM_SEED):\n",
    "    \"\"\"\n",
    "    Generate synthetic EEG band features using a correlation sampling method.\n",
    "\n",
    "    real_features: np.ndarray of shape (n_samples, n_bands)\n",
    "    band_names    : list of band names (for logging, same order as columns)\n",
    "    n_synthetic   : number of synthetic samples to generate. If None,\n",
    "                    uses n_synthetic = real_features.shape[0]\n",
    "    \"\"\"\n",
    "    np.random.seed(random_seed)\n",
    "    \n",
    "    if n_synthetic is None:\n",
    "        n_synthetic = real_features.shape[0]\n",
    "\n",
    "    # Correlation and summary stats\n",
    "    correlation_matrix = np.corrcoef(real_features.T)\n",
    "    mean_features = np.mean(real_features, axis=0)\n",
    "    std_features = np.std(real_features, axis=0)\n",
    "\n",
    "    print(\"Correlation Matrix of Frequency Bands:\")\n",
    "    for i, band1 in enumerate(band_names):\n",
    "        for j, band2 in enumerate(band_names):\n",
    "            if j >= i:\n",
    "                print(f\"{band1:11s} - {band2:11s}: {correlation_matrix[i, j]:7.3f}\")\n",
    "    \n",
    "    # Covariance = D * Corr * D\n",
    "    covariance_matrix = np.outer(std_features, std_features) * correlation_matrix\n",
    "    \n",
    "    # Multivariate normal sampling\n",
    "    synthetic_features = np.random.multivariate_normal(\n",
    "        mean_features,\n",
    "        covariance_matrix,\n",
    "        size=n_synthetic\n",
    "    )\n",
    "\n",
    "    # Ensure non-negative band powers\n",
    "    synthetic_features = np.clip(synthetic_features, a_min=0.0, a_max=None)\n",
    "\n",
    "    print(f\"Generated {n_synthetic} synthetic feature vectors\")\n",
    "    print(\"Correlation structure preserved (in expectation)\")\n",
    "\n",
    "    return synthetic_features, correlation_matrix, covariance_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- computes the correlation matrix only on that condition’s subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_corr_by_condition(df_train, band_cols, label_col, scaler, random_seed=RANDOM_SEED):\n",
    "    \"\"\"\n",
    "    Condition-conditional correlation sampling.\n",
    "\n",
    "    For each condition (S1 obj / S2 match / S2 nomatch,):\n",
    "      - slice df_train to that condition\n",
    "      - compute correlation & stats on that subset\n",
    "      - generate the same number of synthetic samples for that condition\n",
    "      - standardize with global scaler (fit on real train)\n",
    "      - keep labels (a/c -> 1/0) and condition tokens\n",
    "\n",
    "    Returns:\n",
    "      X_syn_corr    : (N_train, n_bands) standardized synthetic features\n",
    "      y_syn_corr    : (N_train,) labels 0/1\n",
    "      cond_syn_corr : (N_train,) condition tokens\n",
    "    \"\"\"\n",
    "    unique_conditions = sorted(df_train[CONDITION_COL].unique())\n",
    "    print(\"Conditions in train:\", unique_conditions)\n",
    "\n",
    "    X_syn_list, y_syn_list, cond_syn_list = [], [], []\n",
    "\n",
    "    for i, cond in enumerate(unique_conditions):\n",
    "        # Slice by condition using the shared helper\n",
    "        real_cond, labels_cond = get_condition_slice(\n",
    "            df=df_train,\n",
    "            condition=cond,\n",
    "            band_cols=band_cols,\n",
    "            label_col=label_col,\n",
    "        )\n",
    "\n",
    "        n_synth_cond = real_cond.shape[0]\n",
    "        print(f\"[Corr] Condition={cond}, real={n_synth_cond}, synth={n_synth_cond}\")\n",
    "\n",
    "        if n_synth_cond < band_cols.__len__():\n",
    "            # Edge case: very few samples for this condition (should not happen here, but safe)\n",
    "            print(f\"  WARNING: very few samples for {cond}; skipping correlation sampling.\")\n",
    "            continue\n",
    "\n",
    "        # Per-condition correlation-based generation\n",
    "        syn_raw, corr_mat, cov_mat = generate_correlation_based_eeg(\n",
    "            real_features=real_cond,\n",
    "            band_names=band_cols,\n",
    "            n_synthetic=n_synth_cond,\n",
    "            random_seed=random_seed + i,  # small offset per condition\n",
    "        )\n",
    "\n",
    "        # Standardize using the global scaler (fit on REAL train)\n",
    "        X_syn_cond = scaler.transform(syn_raw)\n",
    "\n",
    "        # Attach labels and condition tokens\n",
    "        y_syn_cond = labels_cond.copy()\n",
    "        cond_tokens = np.full(n_synth_cond, cond, dtype=object)\n",
    "\n",
    "        X_syn_list.append(X_syn_cond)\n",
    "        y_syn_list.append(y_syn_cond)\n",
    "        cond_syn_list.append(cond_tokens)\n",
    "\n",
    "    # Concatenate across conditions\n",
    "    X_syn_corr = np.vstack(X_syn_list)\n",
    "    y_syn_corr = np.concatenate(y_syn_list)\n",
    "    cond_syn_corr = np.concatenate(cond_syn_list)\n",
    "\n",
    "    # Shuffle to break condition-wise block ordering\n",
    "    perm = np.random.permutation(len(y_syn_corr))\n",
    "    X_syn_corr = X_syn_corr[perm]\n",
    "    y_syn_corr = y_syn_corr[perm]\n",
    "    cond_syn_corr = cond_syn_corr[perm]\n",
    "\n",
    "    print(\"Final condition-conditional CORR shape:\", X_syn_corr.shape)\n",
    "    print(\"Label dist:\", Counter(y_syn_corr))\n",
    "    print(\"Condition dist:\", Counter(cond_syn_corr))\n",
    "\n",
    "    return X_syn_corr, y_syn_corr, cond_syn_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conditions in train: ['S1 obj', 'S2 match', 'S2 nomatch,']\n",
      "[Corr] Condition=S1 obj, real=10240, synth=10240\n",
      "Correlation Matrix of Frequency Bands:\n",
      "Delta       - Delta      :   1.000\n",
      "Delta       - Theta      :   0.590\n",
      "Delta       - Alpha      :   0.230\n",
      "Delta       - Beta       :   0.274\n",
      "Delta       - Gamma      :   0.116\n",
      "Delta       - total_power:   0.804\n",
      "Theta       - Theta      :   1.000\n",
      "Theta       - Alpha      :   0.332\n",
      "Theta       - Beta       :   0.328\n",
      "Theta       - Gamma      :   0.127\n",
      "Theta       - total_power:   0.680\n",
      "Alpha       - Alpha      :   1.000\n",
      "Alpha       - Beta       :   0.268\n",
      "Alpha       - Gamma      :   0.066\n",
      "Alpha       - total_power:   0.509\n",
      "Beta        - Beta       :   1.000\n",
      "Beta        - Gamma      :   0.717\n",
      "Beta        - total_power:   0.652\n",
      "Gamma       - Gamma      :   1.000\n",
      "Gamma       - total_power:   0.480\n",
      "total_power - total_power:   1.000\n",
      "Generated 10240 synthetic feature vectors\n",
      "Correlation structure preserved (in expectation)\n",
      "[Corr] Condition=S2 match, real=10176, synth=10176\n",
      "Correlation Matrix of Frequency Bands:\n",
      "Delta       - Delta      :   1.000\n",
      "Delta       - Theta      :   0.658\n",
      "Delta       - Alpha      :   0.374\n",
      "Delta       - Beta       :   0.304\n",
      "Delta       - Gamma      :   0.104\n",
      "Delta       - total_power:   0.912\n",
      "Theta       - Theta      :   1.000\n",
      "Theta       - Alpha      :   0.419\n",
      "Theta       - Beta       :   0.297\n",
      "Theta       - Gamma      :   0.128\n",
      "Theta       - total_power:   0.735\n",
      "Alpha       - Alpha      :   1.000\n",
      "Alpha       - Beta       :   0.303\n",
      "Alpha       - Gamma      :   0.043\n",
      "Alpha       - total_power:   0.544\n",
      "Beta        - Beta       :   1.000\n",
      "Beta        - Gamma      :   0.720\n",
      "Beta        - total_power:   0.562\n",
      "Gamma       - Gamma      :   1.000\n",
      "Gamma       - total_power:   0.353\n",
      "total_power - total_power:   1.000\n",
      "Generated 10176 synthetic feature vectors\n",
      "Correlation structure preserved (in expectation)\n",
      "[Corr] Condition=S2 nomatch,, real=9920, synth=9920\n",
      "Correlation Matrix of Frequency Bands:\n",
      "Delta       - Delta      :   1.000\n",
      "Delta       - Theta      :   0.641\n",
      "Delta       - Alpha      :   0.252\n",
      "Delta       - Beta       :   0.260\n",
      "Delta       - Gamma      :   0.117\n",
      "Delta       - total_power:   0.894\n",
      "Theta       - Theta      :   1.000\n",
      "Theta       - Alpha      :   0.360\n",
      "Theta       - Beta       :   0.287\n",
      "Theta       - Gamma      :   0.086\n",
      "Theta       - total_power:   0.732\n",
      "Alpha       - Alpha      :   1.000\n",
      "Alpha       - Beta       :   0.345\n",
      "Alpha       - Gamma      :   0.054\n",
      "Alpha       - total_power:   0.479\n",
      "Beta        - Beta       :   1.000\n",
      "Beta        - Gamma      :   0.731\n",
      "Beta        - total_power:   0.556\n",
      "Gamma       - Gamma      :   1.000\n",
      "Gamma       - total_power:   0.367\n",
      "total_power - total_power:   1.000\n",
      "Generated 9920 synthetic feature vectors\n",
      "Correlation structure preserved (in expectation)\n",
      "Final condition-conditional CORR shape: (30336, 6)\n",
      "Label dist: Counter({1: 15232, 0: 15104})\n",
      "Condition dist: Counter({'S1 obj': 10240, 'S2 match': 10176, 'S2 nomatch,': 9920})\n"
     ]
    }
   ],
   "source": [
    "X_syn_corr, y_syn_corr, cond_syn_corr = generate_corr_by_condition(\n",
    "    df_train=df_train,\n",
    "    band_cols=BAND_COLS,\n",
    "    label_col=LABEL_COL,\n",
    "    scaler=scaler,\n",
    "    random_seed=RANDOM_SEED,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.3. Method 2: WGAN-GP Synthetic Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_wgangp_eeg(real_features, n_synthetic=100, noise_dim=16, hidden_dim=64, n_critic=5, gp_lambda=10.0, lr=1e-4, batch_size=128, epochs=300, random_seed=RANDOM_SEED):\n",
    "    \"\"\"\n",
    "    Train a compact WGAN-GP on band-power features and return synthetic samples.\n",
    "    real_features: np.ndarray of shape (n_samples, n_features) – here (N_cond, 6)\n",
    "    \"\"\"\n",
    "    torch.manual_seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # real_features already clipped and cleaned\n",
    "    data = torch.from_numpy(real_features.astype(np.float32))\n",
    "    dataset = TensorDataset(data)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "    feature_dim = real_features.shape[1]\n",
    "\n",
    "    class Generator(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.net = nn.Sequential(\n",
    "                nn.Linear(noise_dim, hidden_dim),\n",
    "                nn.LayerNorm(hidden_dim),\n",
    "                nn.GELU(),\n",
    "                nn.Linear(hidden_dim, hidden_dim),\n",
    "                nn.GELU(),\n",
    "                nn.Linear(hidden_dim, feature_dim),\n",
    "            )\n",
    "\n",
    "        def forward(self, z):\n",
    "            x = self.net(z)\n",
    "            # softplus keeps outputs positive but smooth\n",
    "            return torch.nn.functional.softplus(x)\n",
    "\n",
    "    class Critic(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.net = nn.Sequential(\n",
    "                nn.Linear(feature_dim, hidden_dim),\n",
    "                nn.LayerNorm(hidden_dim),\n",
    "                nn.GELU(),\n",
    "                nn.Linear(hidden_dim, hidden_dim),\n",
    "                nn.GELU(),\n",
    "                nn.Linear(hidden_dim, 1),\n",
    "            )\n",
    "        def forward(self, x):\n",
    "            return self.net(x)\n",
    "\n",
    "    def gradient_penalty(critic, real, fake):\n",
    "        batch_size = real.size(0)\n",
    "        epsilon = torch.rand(batch_size, 1, device=real.device)\n",
    "        epsilon = epsilon.expand_as(real)\n",
    "        interpolated = epsilon * real + (1 - epsilon) * fake\n",
    "        interpolated.requires_grad_(True)\n",
    "        mixed_scores = critic(interpolated)\n",
    "        grad = torch.autograd.grad(\n",
    "            outputs=mixed_scores,\n",
    "            inputs=interpolated,\n",
    "            grad_outputs=torch.ones_like(mixed_scores),\n",
    "            create_graph=True,\n",
    "            retain_graph=True,\n",
    "        )[0]\n",
    "        grad = grad.view(batch_size, -1)\n",
    "        gp = ((grad.norm(2, dim=1) - 1) ** 2).mean()\n",
    "        return gp\n",
    "\n",
    "    G = Generator().to(device)\n",
    "    D = Critic().to(device)\n",
    "\n",
    "    opt_G = optim.Adam(G.parameters(), lr=lr, betas=(0.5, 0.9))\n",
    "    opt_D = optim.Adam(D.parameters(), lr=lr, betas=(0.5, 0.9))\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for i, (real_batch,) in enumerate(loader):\n",
    "            real_batch = real_batch.to(device)\n",
    "\n",
    "            # Critic updates\n",
    "            for _ in range(n_critic):\n",
    "                z = torch.randn(real_batch.size(0), noise_dim, device=device)\n",
    "                fake_batch = G(z).detach()\n",
    "\n",
    "                opt_D.zero_grad()\n",
    "                critic_real = D(real_batch).mean()\n",
    "                critic_fake = D(fake_batch).mean()\n",
    "                gp = gradient_penalty(D, real_batch, fake_batch)\n",
    "                loss_D = -(critic_real - critic_fake) + gp_lambda * gp\n",
    "                loss_D.backward()\n",
    "                opt_D.step()\n",
    "\n",
    "            # Generator update\n",
    "            z = torch.randn(real_batch.size(0), noise_dim, device=device)\n",
    "            opt_G.zero_grad()\n",
    "            fake_batch = G(z)\n",
    "            loss_G = -D(fake_batch).mean()\n",
    "            loss_G.backward()\n",
    "            opt_G.step()\n",
    "\n",
    "        # simple monitoring every 50 epochs\n",
    "        if (epoch + 1) % 50 == 0:\n",
    "            with torch.no_grad():\n",
    "                z = torch.randn(batch_size, noise_dim, device=device)\n",
    "                preview = G(z).cpu().numpy()\n",
    "            preview_mean = preview.mean(axis=0)\n",
    "            print(\n",
    "                f\"Epoch {epoch + 1:03d}/{epochs} | \"\n",
    "                f\"D: {loss_D.item():.4f} | G: {loss_G.item():.4f} | \"\n",
    "                f\"preview mean={np.round(preview_mean, 3)}\"\n",
    "            )\n",
    "\n",
    "    # Generate n_synthetic samples\n",
    "    G.eval()\n",
    "    with torch.no_grad():\n",
    "        synth_chunks = []\n",
    "        remaining = n_synthetic\n",
    "        while remaining > 0:\n",
    "            current = min(batch_size, remaining)\n",
    "            z = torch.randn(current, noise_dim, device=device)\n",
    "            synth = G(z).cpu().numpy()\n",
    "            synth_chunks.append(synth)\n",
    "            remaining -= current\n",
    "\n",
    "    synthetic = np.vstack(synth_chunks)\n",
    "    synthetic = np.clip(synthetic, a_min=0.0, a_max=None)\n",
    "    return synthetic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- trains a separate GAN per condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_wgangp_by_condition(df_train, band_cols, label_col, scaler, epochs=300, random_seed=RANDOM_SEED):\n",
    "    \n",
    "    unique_conditions = sorted(df_train[CONDITION_COL].unique())\n",
    "    print(\"Conditions in train:\", unique_conditions)\n",
    "\n",
    "    X_syn_list, y_syn_list, cond_syn_list = [], [], []\n",
    "\n",
    "    for i, cond in enumerate(unique_conditions):\n",
    "        # Slice real data by condition\n",
    "        real_cond, labels_cond = get_condition_slice(\n",
    "            df=df_train,\n",
    "            condition=cond,\n",
    "            band_cols=band_cols,\n",
    "            label_col=label_col,\n",
    "        )\n",
    "\n",
    "        n_synth_cond = real_cond.shape[0]\n",
    "        print(f\"[WGAN-GP] Condition={cond}, real={n_synth_cond}, synth={n_synth_cond}\")\n",
    "\n",
    "        if n_synth_cond < 2 * band_cols.__len__():\n",
    "            # If you ever had too few samples for a condition, skip or fallback\n",
    "            print(f\"WARNING: very few samples for {cond}; skipping WGAN-GP.\")\n",
    "            continue\n",
    "\n",
    "        # Train WGAN-GP on this condition only\n",
    "        synth_raw = generate_wgangp_eeg(\n",
    "            real_features=real_cond.astype(np.float32),\n",
    "            n_synthetic=n_synth_cond,\n",
    "            epochs=epochs,\n",
    "            random_seed=random_seed + i\n",
    "        )\n",
    "\n",
    "        # Standardize with global scaler (already fit on REAL X_train)\n",
    "        X_syn_cond = scaler.transform(synth_raw)\n",
    "\n",
    "        # Attach labels + condition tokens\n",
    "        y_syn_cond = labels_cond.copy()\n",
    "        cond_tokens = np.full(n_synth_cond, cond, dtype=object)\n",
    "\n",
    "        X_syn_list.append(X_syn_cond)\n",
    "        y_syn_list.append(y_syn_cond)\n",
    "        cond_syn_list.append(cond_tokens)\n",
    "\n",
    "    # Concatenate all conditions\n",
    "    X_syn_wgangp = np.vstack(X_syn_list)\n",
    "    y_syn_wgangp = np.concatenate(y_syn_list)\n",
    "    cond_syn_wgangp = np.concatenate(cond_syn_list)\n",
    "\n",
    "    # Shuffle to break block structure\n",
    "    perm = np.random.permutation(len(y_syn_wgangp))\n",
    "    X_syn_wgangp = X_syn_wgangp[perm]\n",
    "    y_syn_wgangp = y_syn_wgangp[perm]\n",
    "    cond_syn_wgangp = cond_syn_wgangp[perm]\n",
    "\n",
    "    print(\"Final condition-conditional WGAN-GP shape:\", X_syn_wgangp.shape)\n",
    "    print(\"Label dist:\", Counter(y_syn_wgangp))\n",
    "    print(\"Condition dist:\", Counter(cond_syn_wgangp))\n",
    "\n",
    "    return X_syn_wgangp, y_syn_wgangp, cond_syn_wgangp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training WGAN-GP generators per condition on band-power features\n",
      "Conditions in train: ['S1 obj', 'S2 match', 'S2 nomatch,']\n",
      "[WGAN-GP] Condition=S1 obj, real=10240, synth=10240\n",
      "Epoch 050/300 | D: -1.8628 | G: 88.3494 | preview mean=[10.048  3.788  5.271  5.396  2.427 26.904]\n",
      "Epoch 100/300 | D: -0.7408 | G: 66.3920 | preview mean=[14.188  4.425  4.945  5.697  2.56  31.712]\n",
      "Epoch 150/300 | D: -1.5953 | G: 73.5776 | preview mean=[14.251  4.537  5.312  6.156  2.869 34.027]\n",
      "Epoch 200/300 | D: -1.8032 | G: 79.3281 | preview mean=[12.784  4.14   6.374  5.542  2.875 32.975]\n",
      "Epoch 250/300 | D: -1.4544 | G: 85.8254 | preview mean=[14.405  4.745  5.834  5.783  2.324 33.927]\n",
      "Epoch 300/300 | D: 0.0453 | G: 89.6252 | preview mean=[12.375  3.811  5.596  5.387  2.517 30.318]\n",
      "[WGAN-GP] Condition=S2 match, real=10176, synth=10176\n",
      "Epoch 050/300 | D: -18.2941 | G: 123.7942 | preview mean=[16.172  4.67   4.781  5.155  1.678 31.74 ]\n",
      "Epoch 100/300 | D: -6.5992 | G: 73.8731 | preview mean=[18.242  5.103  4.663  4.998  2.758 35.994]\n",
      "Epoch 150/300 | D: -0.0651 | G: 73.1502 | preview mean=[12.504  4.732  4.166  4.368  2.259 28.708]\n",
      "Epoch 200/300 | D: -2.7813 | G: 70.6727 | preview mean=[14.093  5.015  4.382  4.672  2.279 30.98 ]\n",
      "Epoch 250/300 | D: -0.9726 | G: 79.9970 | preview mean=[12.604  3.722  3.485  3.901  1.469 24.978]\n",
      "Epoch 300/300 | D: 0.2362 | G: 59.8007 | preview mean=[13.689  4.876  3.904  4.159  2.654 30.804]\n",
      "[WGAN-GP] Condition=S2 nomatch,, real=9920, synth=9920\n",
      "Epoch 050/300 | D: -9.4747 | G: 175.1105 | preview mean=[14.217  4.147  4.112  4.442  1.159 29.389]\n",
      "Epoch 100/300 | D: -4.2793 | G: 141.0442 | preview mean=[15.339  4.713  4.874  4.466  2.012 31.006]\n",
      "Epoch 150/300 | D: -2.3682 | G: 143.4997 | preview mean=[16.15   4.858  4.178  4.128  1.845 32.068]\n",
      "Epoch 200/300 | D: -0.1599 | G: 104.9590 | preview mean=[18.83   6.463  4.897  4.169  2.287 36.585]\n",
      "Epoch 250/300 | D: 0.1755 | G: 88.6000 | preview mean=[13.394  5.201  4.742  4.148  2.193 29.985]\n",
      "Epoch 300/300 | D: 0.6866 | G: 82.3281 | preview mean=[12.598  4.603  3.799  3.696  1.862 26.64 ]\n",
      "Final condition-conditional WGAN-GP shape: (30336, 6)\n",
      "Label dist: Counter({1: 15232, 0: 15104})\n",
      "Condition dist: Counter({'S1 obj': 10240, 'S2 match': 10176, 'S2 nomatch,': 9920})\n"
     ]
    }
   ],
   "source": [
    "print(\"Training WGAN-GP generators per condition on band-power features\")\n",
    "\n",
    "real_features = df_train[BAND_COLS].to_numpy()\n",
    "X_syn_wgangp, y_syn_wgangp, cond_syn_wgangp = generate_wgangp_by_condition(\n",
    "    df_train=df_train,\n",
    "    band_cols=BAND_COLS,\n",
    "    label_col=LABEL_COL,\n",
    "    scaler=scaler,\n",
    "    epochs=300,\n",
    "    random_seed=RANDOM_SEED,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.4. Method 3: Gaussian Copula Sampling Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _allocate_samples_by_class(labels, n_total):\n",
    "    \"\"\"\n",
    "    Allocate synthetic samples per class, preserving empirical ratios.\n",
    "    Returns a dict: {class_label: n_synth_for_that_class}\n",
    "    \"\"\"\n",
    "    classes, counts = np.unique(labels, return_counts=True)\n",
    "    ratios = counts / counts.sum()\n",
    "    expected = ratios * n_total\n",
    "\n",
    "    allocated = np.floor(expected).astype(int)\n",
    "    remainder = n_total - allocated.sum()\n",
    "\n",
    "    if remainder > 0:\n",
    "        remainders = expected - allocated\n",
    "        order = np.argsort(remainders)[::-1]\n",
    "        for idx in order[:remainder]:\n",
    "            allocated[idx] += 1\n",
    "\n",
    "    return dict(zip(classes, allocated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_gaussian_copula_eeg(real_features, labels, n_synthetic=100, random_seed=RANDOM_SEED):\n",
    "    \"\"\"\n",
    "    Gaussian copula sampling (class-conditional):\n",
    "\n",
    "    1. For each class (0/1), fit a quantile transformer to map marginals -> N(0,1)\n",
    "    2. Estimate regularised covariance (Ledoit-Wolf) in that latent space\n",
    "    3. Sample multivariate normal per class and invert the transform\n",
    "    4. Clip to non-negative band powers\n",
    "    5. Return synthetic features + synthetic labels\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(random_seed)\n",
    "    allocation = _allocate_samples_by_class(labels, n_synthetic)\n",
    "    synthetic_blocks = []\n",
    "    label_blocks = []\n",
    "\n",
    "    print(\"Generating Gaussian copula samples per class...\")\n",
    "    for cls, n_cls_samples in allocation.items():\n",
    "        class_features = real_features[labels == cls]\n",
    "        if len(class_features) == 0 or n_cls_samples == 0:\n",
    "            continue\n",
    "\n",
    "        # quantile transformer to approximate Gaussian marginals\n",
    "        n_quantiles = min(len(class_features), 1000)\n",
    "        transformer = QuantileTransformer(\n",
    "            n_quantiles=n_quantiles,\n",
    "            output_distribution=\"normal\",\n",
    "            random_state=random_seed,\n",
    "        )\n",
    "\n",
    "        latent = transformer.fit_transform(class_features)\n",
    "\n",
    "        # Ledoit-Wolf for stable covariance\n",
    "        cov_estimator = LedoitWolf().fit(latent)\n",
    "        latent_mean = cov_estimator.location_\n",
    "        latent_cov = cov_estimator.covariance_\n",
    "\n",
    "        # sample in latent Gaussian space\n",
    "        latent_samples = rng.multivariate_normal(\n",
    "            latent_mean,\n",
    "            latent_cov,\n",
    "            size=n_cls_samples,\n",
    "        )\n",
    "\n",
    "        # invert back to band-power space\n",
    "        samples = transformer.inverse_transform(latent_samples)\n",
    "\n",
    "        # enforce non-negativity for power features\n",
    "        samples = np.clip(samples, a_min=0, a_max=None)\n",
    "        synthetic_blocks.append(samples)\n",
    "\n",
    "        # create corresponding label block\n",
    "        label_blocks.append(np.full(n_cls_samples, cls, dtype=labels.dtype))\n",
    "\n",
    "        print(f\"  Class {cls}: real={len(class_features)}, synthetic={n_cls_samples}\")\n",
    "\n",
    "    if not synthetic_blocks:\n",
    "        raise ValueError(\"No synthetic samples were generated. Check class labels.\")\n",
    "\n",
    "    synthetic_features = np.vstack(synthetic_blocks)\n",
    "    synthetic_labels = np.concatenate(label_blocks)\n",
    "\n",
    "    return synthetic_features, synthetic_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- fits marginals + covariance on a given condition’s data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_gaussian_copula_by_condition(df_train, band_cols, label_col, scaler, random_seed=RANDOM_SEED):\n",
    "    unique_conditions = sorted(df_train[CONDITION_COL].unique())\n",
    "    print(\"Conditions in train:\", unique_conditions)\n",
    "\n",
    "    X_syn_list, y_syn_list, cond_syn_list = [], [], []\n",
    "\n",
    "    for i, cond in enumerate(unique_conditions):\n",
    "        # slice real data by condition\n",
    "        real_cond, labels_cond = get_condition_slice(\n",
    "            df=df_train,\n",
    "            condition=cond,\n",
    "            band_cols=band_cols,\n",
    "            label_col=label_col,\n",
    "        )\n",
    "        n_cond = real_cond.shape[0]\n",
    "        print(f\"[Copula] Condition={cond}, real={n_cond}, synth={n_cond}\")\n",
    "\n",
    "        if n_cond == 0:\n",
    "            print(f\"WARNING: no samples for condition={cond}, skipping.\")\n",
    "            continue\n",
    "\n",
    "        # run Gaussian Copula on this condition slice (class-conditional inside)\n",
    "        synth_raw_cond, labels_syn_cond = generate_gaussian_copula_eeg(\n",
    "            real_features=real_cond,\n",
    "            labels=labels_cond,\n",
    "            n_synthetic=n_cond,\n",
    "            random_seed=random_seed + i,   # offset per condition\n",
    "        )\n",
    "\n",
    "        # Standardize using global scaler (fit on REAL df_train band features)\n",
    "        X_syn_cond = scaler.transform(synth_raw_cond)\n",
    "\n",
    "        # condition tokens\n",
    "        cond_tokens = np.full(n_cond, cond, dtype=object)\n",
    "\n",
    "        X_syn_list.append(X_syn_cond)\n",
    "        y_syn_list.append(labels_syn_cond)\n",
    "        cond_syn_list.append(cond_tokens)\n",
    "\n",
    "    # concatenate over conditions\n",
    "    X_syn_copula = np.vstack(X_syn_list)\n",
    "    y_syn_copula = np.concatenate(y_syn_list)\n",
    "    cond_syn_copula = np.concatenate(cond_syn_list)\n",
    "\n",
    "    # shuffle\n",
    "    perm = np.random.permutation(len(y_syn_copula))\n",
    "    X_syn_copula = X_syn_copula[perm]\n",
    "    y_syn_copula = y_syn_copula[perm]\n",
    "    cond_syn_copula = cond_syn_copula[perm]\n",
    "\n",
    "    print(\"Final condition-conditional Copula shape:\", X_syn_copula.shape)\n",
    "    print(\"Label dist:\", Counter(y_syn_copula))\n",
    "    print(\"Condition dist:\", Counter(cond_syn_copula))\n",
    "\n",
    "    return X_syn_copula, y_syn_copula, cond_syn_copula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Gaussian Copula generators per condition\n",
      "Conditions in train: ['S1 obj', 'S2 match', 'S2 nomatch,']\n",
      "[Copula] Condition=S1 obj, real=10240, synth=10240\n",
      "Generating Gaussian copula samples per class...\n",
      "  Class 0: real=5120, synthetic=5120\n",
      "  Class 1: real=5120, synthetic=5120\n",
      "[Copula] Condition=S2 match, real=10176, synth=10176\n",
      "Generating Gaussian copula samples per class...\n",
      "  Class 0: real=5056, synthetic=5056\n",
      "  Class 1: real=5120, synthetic=5120\n",
      "[Copula] Condition=S2 nomatch,, real=9920, synth=9920\n",
      "Generating Gaussian copula samples per class...\n",
      "  Class 0: real=4928, synthetic=4928\n",
      "  Class 1: real=4992, synthetic=4992\n",
      "Final condition-conditional Copula shape: (30336, 6)\n",
      "Label dist: Counter({1: 15232, 0: 15104})\n",
      "Condition dist: Counter({'S1 obj': 10240, 'S2 match': 10176, 'S2 nomatch,': 9920})\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Gaussian Copula generators per condition\")\n",
    "\n",
    "X_syn_copula, y_syn_copula, cond_syn_copula = generate_gaussian_copula_by_condition(\n",
    "    df_train=df_train,\n",
    "    band_cols=BAND_COLS,\n",
    "    label_col=LABEL_COL,\n",
    "    scaler=scaler,\n",
    "    random_seed=RANDOM_SEED,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.5. Method 4: Classwise Interpolation Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_classwise_interpolation_eeg(real_features, labels, n_synthetic=100, random_seed=RANDOM_SEED, k_neighbors=8, noise_scale=0.02):\n",
    "    \"\"\"\n",
    "    Class-conditional interpolation inspired by SMOTE.\n",
    "    Operates in log-power space to better capture multiplicative structure.\n",
    "\n",
    "    real_features: np.ndarray, shape (N, n_bands)\n",
    "    labels       : np.ndarray, shape (N,) with class labels (0/1)\n",
    "\n",
    "    Returns:\n",
    "        synthetic_features: (n_synthetic, n_bands)\n",
    "        synthetic_labels  : (n_synthetic,)\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(random_seed)\n",
    "    allocation = _allocate_samples_by_class(labels, n_synthetic)\n",
    "\n",
    "    synthetic_blocks = []\n",
    "    label_blocks = []\n",
    "\n",
    "    # work in log-power space\n",
    "    log_features = np.log1p(real_features)\n",
    "\n",
    "    for cls, n_cls_samples in allocation.items():\n",
    "        class_mask = labels == cls\n",
    "        class_features_log = log_features[class_mask]\n",
    "        if len(class_features_log) == 0 or n_cls_samples == 0:\n",
    "            continue\n",
    "\n",
    "        # effective neighbors (avoid k > n-1)\n",
    "        n_neighbors_eff = min(k_neighbors, len(class_features_log) - 1)\n",
    "\n",
    "        if n_neighbors_eff <= 0:\n",
    "            # fallback: jitter existing samples\n",
    "            base_samples = np.repeat(\n",
    "                class_features_log,\n",
    "                repeats=max(1, n_cls_samples // max(1, len(class_features_log))),\n",
    "                axis=0,\n",
    "            )\n",
    "            base_samples = base_samples[:n_cls_samples]\n",
    "            jitter = rng.normal(0, noise_scale, size=base_samples.shape)\n",
    "            augmented = base_samples + jitter\n",
    "            syn_block = np.expm1(augmented)\n",
    "        else:\n",
    "            from sklearn.neighbors import NearestNeighbors  # ensure imported\n",
    "\n",
    "            nbrs = NearestNeighbors(n_neighbors=n_neighbors_eff + 1)\n",
    "            nbrs.fit(class_features_log)\n",
    "\n",
    "            class_std = np.std(class_features_log, axis=0, ddof=1)\n",
    "            class_std[class_std == 0] = 1e-6\n",
    "\n",
    "            syn_list = []\n",
    "            for _ in range(n_cls_samples):\n",
    "                # pick anchor\n",
    "                idx = rng.integers(len(class_features_log))\n",
    "                # get neighbors (excluding itself)\n",
    "                neighbors = nbrs.kneighbors(\n",
    "                    class_features_log[idx].reshape(1, -1),\n",
    "                    return_distance=False,\n",
    "                )[0]\n",
    "                neighbors = neighbors[neighbors != idx]\n",
    "\n",
    "                if len(neighbors) == 0:\n",
    "                    neighbor_idx = idx\n",
    "                else:\n",
    "                    neighbor_idx = rng.choice(neighbors)\n",
    "\n",
    "                # interpolate in log-space\n",
    "                alpha = rng.uniform(0.2, 0.8)\n",
    "                interpolated = (\n",
    "                    alpha * class_features_log[idx]\n",
    "                    + (1 - alpha) * class_features_log[neighbor_idx]\n",
    "                )\n",
    "\n",
    "                # add small Gaussian noise in log-space\n",
    "                noise = rng.normal(\n",
    "                    0,\n",
    "                    noise_scale,\n",
    "                    size=class_features_log.shape[1],\n",
    "                ) * class_std\n",
    "                synthetic_log = interpolated + noise\n",
    "\n",
    "                syn_list.append(np.expm1(synthetic_log))\n",
    "\n",
    "            syn_block = np.vstack(syn_list)\n",
    "\n",
    "        syn_block = np.clip(syn_block, a_min=0, a_max=None)\n",
    "        synthetic_blocks.append(syn_block)\n",
    "        label_blocks.append(np.full(n_cls_samples, cls, dtype=labels.dtype))\n",
    "\n",
    "    if not synthetic_blocks:\n",
    "        raise ValueError(\"Interpolation generator did not create any samples.\")\n",
    "\n",
    "    synthetic_features = np.vstack(synthetic_blocks)\n",
    "    synthetic_labels = np.concatenate(label_blocks)\n",
    "    return synthetic_features, synthetic_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- interpolates points within the same condition and class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_classwise_interpolation_by_condition(df_train, band_cols, label_col, scaler, random_seed=RANDOM_SEED, k_neighbors=10, noise_scale=0.015):\n",
    "    \"\"\"\n",
    "    Condition-conditional classwise interpolation:\n",
    "\n",
    "      - For each condition (S1 obj / S2 match / S2 nomatch,)\n",
    "        * slice df_train to that condition\n",
    "        * run classwise interpolation (class-conditional) on that slice\n",
    "        * generate the SAME number of synthetic samples as real\n",
    "      - Standardize using global scaler fit on REAL df_train\n",
    "\n",
    "    Returns:\n",
    "      X_syn_interp    : (N_train, n_bands)\n",
    "      y_syn_interp    : (N_train,)\n",
    "      cond_syn_interp : (N_train,)\n",
    "    \"\"\"\n",
    "    unique_conditions = sorted(df_train[CONDITION_COL].unique())\n",
    "    print(\"Conditions in train:\", unique_conditions)\n",
    "\n",
    "    X_syn_list, y_syn_list, cond_syn_list = [], [], []\n",
    "\n",
    "    for i, cond in enumerate(unique_conditions):\n",
    "        # 1) slice by condition\n",
    "        real_cond, labels_cond = get_condition_slice(\n",
    "            df=df_train,\n",
    "            condition=cond,\n",
    "            band_cols=band_cols,\n",
    "            label_col=label_col,\n",
    "        )\n",
    "\n",
    "        n_cond = real_cond.shape[0]\n",
    "        print(f\"[Interp] Condition={cond}, real={n_cond}, synth={n_cond}\")\n",
    "\n",
    "        if n_cond == 0:\n",
    "            print(f\"  WARNING: no samples for condition={cond}, skipping.\")\n",
    "            continue\n",
    "\n",
    "        # 2) run classwise interpolation on this condition slice\n",
    "        synth_raw_cond, labels_syn_cond = generate_classwise_interpolation_eeg(\n",
    "            real_features=real_cond,\n",
    "            labels=labels_cond,\n",
    "            n_synthetic=n_cond,\n",
    "            random_seed=random_seed + i,\n",
    "            k_neighbors=k_neighbors,\n",
    "            noise_scale=noise_scale,\n",
    "        )\n",
    "\n",
    "        # 3) Standardize using global scaler fit on REAL train\n",
    "        X_syn_cond = scaler.transform(synth_raw_cond)\n",
    "\n",
    "        cond_tokens = np.full(n_cond, cond, dtype=object)\n",
    "\n",
    "        X_syn_list.append(X_syn_cond)\n",
    "        y_syn_list.append(labels_syn_cond)\n",
    "        cond_syn_list.append(cond_tokens)\n",
    "\n",
    "    X_syn_interp = np.vstack(X_syn_list)\n",
    "    y_syn_interp = np.concatenate(y_syn_list)\n",
    "    cond_syn_interp = np.concatenate(cond_syn_list)\n",
    "\n",
    "    # Shuffle\n",
    "    perm = np.random.permutation(len(y_syn_interp))\n",
    "    X_syn_interp = X_syn_interp[perm]\n",
    "    y_syn_interp = y_syn_interp[perm]\n",
    "    cond_syn_interp = cond_syn_interp[perm]\n",
    "\n",
    "    print(\"Final condition-conditional Interp shape:\", X_syn_interp.shape)\n",
    "    print(\"Label dist:\", Counter(y_syn_interp))\n",
    "    print(\"Condition dist:\", Counter(cond_syn_interp))\n",
    "\n",
    "    return X_syn_interp, y_syn_interp, cond_syn_interp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training classwise interpolation generators per condition\n",
      "Conditions in train: ['S1 obj', 'S2 match', 'S2 nomatch,']\n",
      "[Interp] Condition=S1 obj, real=10240, synth=10240\n",
      "[Interp] Condition=S2 match, real=10176, synth=10176\n",
      "[Interp] Condition=S2 nomatch,, real=9920, synth=9920\n",
      "Final condition-conditional Interp shape: (30336, 6)\n",
      "Label dist: Counter({1: 15232, 0: 15104})\n",
      "Condition dist: Counter({'S1 obj': 10240, 'S2 match': 10176, 'S2 nomatch,': 9920})\n"
     ]
    }
   ],
   "source": [
    "print(\"Training classwise interpolation generators per condition\")\n",
    "\n",
    "X_syn_interp, y_syn_interp, cond_syn_interp = generate_classwise_interpolation_by_condition(\n",
    "    df_train=df_train,\n",
    "    band_cols=BAND_COLS,\n",
    "    label_col=LABEL_COL,\n",
    "    scaler=scaler,\n",
    "    random_seed=RANDOM_SEED,\n",
    "    k_neighbors=10,\n",
    "    noise_scale=0.015,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Save model output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_dataset(model_name, X_real, y_real, cond_real, X_syn,  y_syn,  cond_syn, out_dir=OUT_DIR):\n",
    "\n",
    "    model_dir = out_dir / model_name\n",
    "    model_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    real_df = pd.DataFrame(X_real, columns=BAND_COLS)\n",
    "    real_df[\"label\"] = y_real\n",
    "    real_df[\"condition\"] = cond_real\n",
    "    real_df[\"source\"] = \"real\"\n",
    "\n",
    "    syn_df = pd.DataFrame(X_syn, columns=BAND_COLS)\n",
    "    syn_df[\"label\"] = y_syn\n",
    "    syn_df[\"condition\"] = cond_syn\n",
    "    syn_df[\"source\"] = \"synthetic\"\n",
    "\n",
    "    pool_df = pd.concat([real_df, syn_df], axis=0).reset_index(drop=True)\n",
    "\n",
    "    real_path = model_dir / f\"{model_name}_real.csv\"\n",
    "    syn_path  = model_dir / f\"{model_name}_syn.csv\"\n",
    "    pool_path = model_dir / f\"{model_name}_pool.csv\"\n",
    "\n",
    "    real_df.to_csv(real_path, index=False)\n",
    "    syn_df.to_csv(syn_path, index=False)\n",
    "    pool_df.to_csv(pool_path, index=False)\n",
    "\n",
    "    print(f\"[Saved] {model_name}\")\n",
    "    print(\" \", real_path)\n",
    "    print(\" \", syn_path)\n",
    "    print(\" \", pool_path)\n",
    "\n",
    "    return real_df, syn_df, pool_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Saved] mixup\n",
      "  ../output/synthetic_generation/mixup/mixup_real.csv\n",
      "  ../output/synthetic_generation/mixup/mixup_syn.csv\n",
      "  ../output/synthetic_generation/mixup/mixup_pool.csv\n",
      "[Saved] corr\n",
      "  ../output/synthetic_generation/corr/corr_real.csv\n",
      "  ../output/synthetic_generation/corr/corr_syn.csv\n",
      "  ../output/synthetic_generation/corr/corr_pool.csv\n",
      "[Saved] wgangp\n",
      "  ../output/synthetic_generation/wgangp/wgangp_real.csv\n",
      "  ../output/synthetic_generation/wgangp/wgangp_syn.csv\n",
      "  ../output/synthetic_generation/wgangp/wgangp_pool.csv\n",
      "[Saved] copula\n",
      "  ../output/synthetic_generation/copula/copula_real.csv\n",
      "  ../output/synthetic_generation/copula/copula_syn.csv\n",
      "  ../output/synthetic_generation/copula/copula_pool.csv\n",
      "[Saved] interp\n",
      "  ../output/synthetic_generation/interp/interp_real.csv\n",
      "  ../output/synthetic_generation/interp/interp_syn.csv\n",
      "  ../output/synthetic_generation/interp/interp_pool.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(          Delta     Theta     Alpha      Beta     Gamma  total_power  label  \\\n",
       " 0      0.292732  0.249902 -0.610662  0.411409  1.345733     0.284397      1   \n",
       " 1      0.375690  0.290176 -0.585701  2.490218  3.991401     0.927912      1   \n",
       " 2     -0.300488  0.329996 -0.448042  4.006910  4.693634     2.271983      1   \n",
       " 3     -0.124150  0.066128 -0.374884  4.006910  4.693634     2.648213      1   \n",
       " 4     -0.037956 -0.381335 -0.659416 -0.155800  0.179559    -0.249257      1   \n",
       " ...         ...       ...       ...       ...       ...          ...    ...   \n",
       " 30331 -0.661455 -0.783984 -0.727400 -0.972912 -0.667465    -0.919100      1   \n",
       " 30332 -0.661455 -0.783984 -0.727400 -0.972912 -0.667465    -0.919100      1   \n",
       " 30333 -0.661455 -0.783984 -0.727400 -0.972912 -0.667465    -0.919100      1   \n",
       " 30334 -0.661455 -0.783984 -0.727400 -0.972912 -0.667465    -0.919100      1   \n",
       " 30335 -0.661455 -0.783984 -0.727400 -0.972912 -0.667465    -0.919100      1   \n",
       " \n",
       "       condition source  \n",
       " 0        S1 obj   real  \n",
       " 1        S1 obj   real  \n",
       " 2        S1 obj   real  \n",
       " 3        S1 obj   real  \n",
       " 4        S1 obj   real  \n",
       " ...         ...    ...  \n",
       " 30331  S2 match   real  \n",
       " 30332  S2 match   real  \n",
       " 30333  S2 match   real  \n",
       " 30334  S2 match   real  \n",
       " 30335  S2 match   real  \n",
       " \n",
       " [30336 rows x 9 columns],\n",
       "           Delta     Theta     Alpha      Beta     Gamma  total_power  label  \\\n",
       " 0     -0.350379 -0.085281  0.427328 -0.759346 -0.610462    -0.351327      0   \n",
       " 1     -0.588359 -0.144221 -0.700369 -0.781926 -0.549787    -0.731038      1   \n",
       " 2      5.215630  3.877514  1.045012 -0.324019 -0.240247     4.820642      0   \n",
       " 3     -0.612762 -0.781888 -0.686589 -0.918365 -0.555892    -0.878805      0   \n",
       " 4      0.198970  0.583152 -0.480207 -0.564385 -0.178585    -0.020233      1   \n",
       " ...         ...       ...       ...       ...       ...          ...    ...   \n",
       " 30331 -0.267148  0.277506  0.246531  0.269972 -0.420357    -0.119963      0   \n",
       " 30332  4.617263 -0.325678 -0.373086 -0.555334 -0.515239     3.072180      1   \n",
       " 30333 -0.571009 -0.580642  1.052570 -0.690968 -0.548581    -0.437643      0   \n",
       " 30334 -0.548491 -0.700263 -0.637438 -0.862038 -0.570183    -0.807654      0   \n",
       " 30335 -0.500405 -0.568804 -0.608519 -0.823786 -0.643689    -0.749831      0   \n",
       " \n",
       "          condition     source  \n",
       " 0      S2 nomatch,  synthetic  \n",
       " 1      S2 nomatch,  synthetic  \n",
       " 2         S2 match  synthetic  \n",
       " 3         S2 match  synthetic  \n",
       " 4      S2 nomatch,  synthetic  \n",
       " ...            ...        ...  \n",
       " 30331       S1 obj  synthetic  \n",
       " 30332  S2 nomatch,  synthetic  \n",
       " 30333       S1 obj  synthetic  \n",
       " 30334       S1 obj  synthetic  \n",
       " 30335     S2 match  synthetic  \n",
       " \n",
       " [30336 rows x 9 columns],\n",
       "           Delta     Theta     Alpha      Beta     Gamma  total_power  label  \\\n",
       " 0      0.292732  0.249902 -0.610662  0.411409  1.345733     0.284397      1   \n",
       " 1      0.375690  0.290176 -0.585701  2.490218  3.991401     0.927912      1   \n",
       " 2     -0.300488  0.329996 -0.448042  4.006910  4.693634     2.271983      1   \n",
       " 3     -0.124150  0.066128 -0.374884  4.006910  4.693634     2.648213      1   \n",
       " 4     -0.037956 -0.381335 -0.659416 -0.155800  0.179559    -0.249257      1   \n",
       " ...         ...       ...       ...       ...       ...          ...    ...   \n",
       " 60667 -0.267148  0.277506  0.246531  0.269972 -0.420357    -0.119963      0   \n",
       " 60668  4.617263 -0.325678 -0.373086 -0.555334 -0.515239     3.072180      1   \n",
       " 60669 -0.571009 -0.580642  1.052570 -0.690968 -0.548581    -0.437643      0   \n",
       " 60670 -0.548491 -0.700263 -0.637438 -0.862038 -0.570183    -0.807654      0   \n",
       " 60671 -0.500405 -0.568804 -0.608519 -0.823786 -0.643689    -0.749831      0   \n",
       " \n",
       "          condition     source  \n",
       " 0           S1 obj       real  \n",
       " 1           S1 obj       real  \n",
       " 2           S1 obj       real  \n",
       " 3           S1 obj       real  \n",
       " 4           S1 obj       real  \n",
       " ...            ...        ...  \n",
       " 60667       S1 obj  synthetic  \n",
       " 60668  S2 nomatch,  synthetic  \n",
       " 60669       S1 obj  synthetic  \n",
       " 60670       S1 obj  synthetic  \n",
       " 60671     S2 match  synthetic  \n",
       " \n",
       " [60672 rows x 9 columns])"
      ]
     },
     "execution_count": 504,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. MIXUP\n",
    "save_model_dataset(\n",
    "    \"mixup\",\n",
    "    X_real=X_train,\n",
    "    y_real=y_train,\n",
    "    cond_real=conds_train,\n",
    "    X_syn=X_syn_mixup,\n",
    "    y_syn=y_syn_mixup,\n",
    "    cond_syn=cond_syn_mixup\n",
    ")\n",
    "\n",
    "# 2. CORRELATION SAMPLING\n",
    "save_model_dataset(\n",
    "    \"corr\",\n",
    "    X_real=X_train,\n",
    "    y_real=y_train,\n",
    "    cond_real=conds_train,\n",
    "    X_syn=X_syn_corr,\n",
    "    y_syn=y_syn_corr,\n",
    "    cond_syn=cond_syn_corr\n",
    ")\n",
    "\n",
    "# 3. WGAN-GP\n",
    "save_model_dataset(\n",
    "    \"wgangp\",\n",
    "    X_real=X_train,\n",
    "    y_real=y_train,\n",
    "    cond_real=conds_train,\n",
    "    X_syn=X_syn_wgangp,\n",
    "    y_syn=y_syn_wgangp,\n",
    "    cond_syn=cond_syn_wgangp\n",
    ")\n",
    "\n",
    "# 4. GAUSSIAN COPULA\n",
    "save_model_dataset(\n",
    "    \"copula\",\n",
    "    X_real=X_train,\n",
    "    y_real=y_train,\n",
    "    cond_real=conds_train,\n",
    "    X_syn=X_syn_copula,\n",
    "    y_syn=y_syn_copula,\n",
    "    cond_syn=cond_syn_copula\n",
    ")\n",
    "\n",
    "# 5. CLASSWISE INTERPOLATION\n",
    "save_model_dataset(\n",
    "    \"interp\",\n",
    "    X_real=X_train,\n",
    "    y_real=y_train,\n",
    "    cond_real=conds_train,\n",
    "    X_syn=X_syn_interp,\n",
    "    y_syn=y_syn_interp,\n",
    "    cond_syn=cond_syn_interp\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ollama_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
