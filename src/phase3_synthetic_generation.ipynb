{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 3: Synthetic EEG Data Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0. Load package & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "from collections import defaultdict, Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.covariance import LedoitWolf\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "plt.rcParams[\"figure.figsize\"] = (6, 4)\n",
    "\n",
    "DATA_DIR = Path(\"../output/band_extraction\")\n",
    "OUT_DIR = Path(\"../output/synthetic_generation\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Processed data features check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data structure\n",
    "LABEL_COL = \"subject_type\"         \n",
    "CONDITION_COL = \"matching_condition\"   \n",
    "SPLIT_COL = \"dataset_split\"\n",
    "ID_COLS = [SPLIT_COL, \"file_name\", \"subject_id\", \"channel\", \"trial\"]\n",
    "FEATURE_COLS = [\"dataset_split\", \"file_name\", \"subject_type\", \"subject_id\", \"channel\", \"trial\", \"matching_condition\", \"Delta\", \"Theta\", \"Alpha\", \"Beta\", \"Gamma\", \"total_power\",]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full data shape: (60672, 13)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset_split</th>\n",
       "      <th>file_name</th>\n",
       "      <th>subject_type</th>\n",
       "      <th>subject_id</th>\n",
       "      <th>channel</th>\n",
       "      <th>trial</th>\n",
       "      <th>matching_condition</th>\n",
       "      <th>Delta</th>\n",
       "      <th>Theta</th>\n",
       "      <th>Alpha</th>\n",
       "      <th>Beta</th>\n",
       "      <th>Gamma</th>\n",
       "      <th>total_power</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train</td>\n",
       "      <td>Data1.csv</td>\n",
       "      <td>a</td>\n",
       "      <td>co2a0000364</td>\n",
       "      <td>FP1</td>\n",
       "      <td>0</td>\n",
       "      <td>S1 obj</td>\n",
       "      <td>20.048105</td>\n",
       "      <td>5.830134</td>\n",
       "      <td>0.854299</td>\n",
       "      <td>6.705598</td>\n",
       "      <td>6.848762</td>\n",
       "      <td>40.286898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train</td>\n",
       "      <td>Data1.csv</td>\n",
       "      <td>a</td>\n",
       "      <td>co2a0000364</td>\n",
       "      <td>FP2</td>\n",
       "      <td>0</td>\n",
       "      <td>S1 obj</td>\n",
       "      <td>21.769006</td>\n",
       "      <td>6.052321</td>\n",
       "      <td>1.013807</td>\n",
       "      <td>16.487621</td>\n",
       "      <td>15.773774</td>\n",
       "      <td>61.096530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train</td>\n",
       "      <td>Data1.csv</td>\n",
       "      <td>a</td>\n",
       "      <td>co2a0000364</td>\n",
       "      <td>F7</td>\n",
       "      <td>0</td>\n",
       "      <td>S1 obj</td>\n",
       "      <td>7.742259</td>\n",
       "      <td>6.272004</td>\n",
       "      <td>1.893497</td>\n",
       "      <td>39.119253</td>\n",
       "      <td>49.533282</td>\n",
       "      <td>104.560295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train</td>\n",
       "      <td>Data1.csv</td>\n",
       "      <td>a</td>\n",
       "      <td>co2a0000364</td>\n",
       "      <td>F8</td>\n",
       "      <td>0</td>\n",
       "      <td>S1 obj</td>\n",
       "      <td>11.400244</td>\n",
       "      <td>4.816262</td>\n",
       "      <td>2.360998</td>\n",
       "      <td>53.646940</td>\n",
       "      <td>44.502180</td>\n",
       "      <td>116.726624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train</td>\n",
       "      <td>Data1.csv</td>\n",
       "      <td>a</td>\n",
       "      <td>co2a0000364</td>\n",
       "      <td>AF1</td>\n",
       "      <td>0</td>\n",
       "      <td>S1 obj</td>\n",
       "      <td>13.188257</td>\n",
       "      <td>2.347635</td>\n",
       "      <td>0.542750</td>\n",
       "      <td>4.036543</td>\n",
       "      <td>2.914738</td>\n",
       "      <td>23.029923</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  dataset_split  file_name subject_type   subject_id channel  trial  \\\n",
       "0         train  Data1.csv            a  co2a0000364     FP1      0   \n",
       "1         train  Data1.csv            a  co2a0000364     FP2      0   \n",
       "2         train  Data1.csv            a  co2a0000364      F7      0   \n",
       "3         train  Data1.csv            a  co2a0000364      F8      0   \n",
       "4         train  Data1.csv            a  co2a0000364     AF1      0   \n",
       "\n",
       "  matching_condition      Delta     Theta     Alpha       Beta      Gamma  \\\n",
       "0             S1 obj  20.048105  5.830134  0.854299   6.705598   6.848762   \n",
       "1             S1 obj  21.769006  6.052321  1.013807  16.487621  15.773774   \n",
       "2             S1 obj   7.742259  6.272004  1.893497  39.119253  49.533282   \n",
       "3             S1 obj  11.400244  4.816262  2.360998  53.646940  44.502180   \n",
       "4             S1 obj  13.188257  2.347635  0.542750   4.036543   2.914738   \n",
       "\n",
       "   total_power  \n",
       "0    40.286898  \n",
       "1    61.096530  \n",
       "2   104.560295  \n",
       "3   116.726624  \n",
       "4    23.029923  "
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load current feature dataset\n",
    "FEATURE_FP = DATA_DIR / \"band_features_segments.csv\"\n",
    "df_all = pd.read_csv(FEATURE_FP)\n",
    "print(\"Full data shape:\", df_all.shape)\n",
    "df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split counts:\n",
      "dataset_split\n",
      "test     30720\n",
      "train    29952\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Basic sanity checks\n",
    "print(\"Split counts:\")\n",
    "print(df_all[SPLIT_COL].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject_type counts:\n",
      "subject_type\n",
      "a    30400\n",
      "c    30272\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Subject_type counts:\")\n",
    "print(df_all[LABEL_COL].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching_condition counts:\n",
      "matching_condition\n",
      "S1 obj         20480\n",
      "S2 match       20416\n",
      "S2 nomatch,    19776\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Matching_condition counts:\")\n",
    "print(df_all[CONDITION_COL].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (29952, 13)\n",
      "Test shape : (30720, 13)\n"
     ]
    }
   ],
   "source": [
    "# Train / Test based on dataset_split\n",
    "df_train = df_all[df_all[SPLIT_COL] == \"train\"].reset_index(drop=True)\n",
    "df_test = df_all[df_all[SPLIT_COL] == \"test\"].reset_index(drop=True)\n",
    "print(\"Train shape:\", df_train.shape)\n",
    "print(\"Test shape :\", df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train label x condition:\n",
      "matching_condition  S1 obj  S2 match  S2 nomatch,\n",
      "subject_type                                     \n",
      "a                     5120      5120         4800\n",
      "c                     5120      5056         4736\n"
     ]
    }
   ],
   "source": [
    "print(\"Train label x condition:\")\n",
    "print(pd.crosstab(df_train[LABEL_COL], df_train[CONDITION_COL]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test label x condition:\n",
      "matching_condition  S1 obj  S2 match  S2 nomatch,\n",
      "subject_type                                     \n",
      "a                     5120      5120         5120\n",
      "c                     5120      5120         5120\n"
     ]
    }
   ],
   "source": [
    "print(\"Test label x condition:\")\n",
    "print(pd.crosstab(df_test[LABEL_COL], df_test[CONDITION_COL]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check missing and extra columns\n",
    "missing = [c for c in FEATURE_COLS if c not in df_all.columns]\n",
    "extra = [c for c in df_all.columns if c not in FEATURE_COLS]\n",
    "\n",
    "if missing:\n",
    "    print(\"MISSING columns:\", missing)\n",
    "if extra:\n",
    "    print(\"EXTRA columns:\", extra)\n",
    "if not missing:\n",
    "    df_all = df_all[FEATURE_COLS].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data types:\n",
      "dataset_split          object\n",
      "file_name              object\n",
      "subject_type           object\n",
      "subject_id             object\n",
      "channel                object\n",
      "trial                   int64\n",
      "matching_condition     object\n",
      "Delta                 float64\n",
      "Theta                 float64\n",
      "Alpha                 float64\n",
      "Beta                  float64\n",
      "Gamma                 float64\n",
      "total_power           float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Numeric type + NaN / Inf checks\n",
    "FEATURE_COLS = [\"Delta\", \"Theta\", \"Alpha\", \"Beta\", \"Gamma\", \"total_power\"]\n",
    "\n",
    "# Force to numeric and report any conversion issues\n",
    "for col in FEATURE_COLS:\n",
    "    df_all[col] = pd.to_numeric(df_all[col], errors=\"coerce\")\n",
    "\n",
    "print(\"Data types:\")\n",
    "print(df_all.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values per column:\n",
      "dataset_split         0\n",
      "file_name             0\n",
      "subject_type          0\n",
      "subject_id            0\n",
      "channel               0\n",
      "trial                 0\n",
      "matching_condition    0\n",
      "Delta                 0\n",
      "Theta                 0\n",
      "Alpha                 0\n",
      "Beta                  0\n",
      "Gamma                 0\n",
      "total_power           0\n",
      "dtype: int64\n",
      "Rows with NaN in any feature: 0\n"
     ]
    }
   ],
   "source": [
    "# Check missing values\n",
    "print(\"Missing values per column:\")\n",
    "print(df_all.isna().sum())\n",
    "mask_nan_features = df_all[FEATURE_COLS].isna().any(axis=1)\n",
    "n_nan_rows = mask_nan_features.sum()\n",
    "print(f\"Rows with NaN in any feature: {n_nan_rows}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with non-finite feature values: 0\n"
     ]
    }
   ],
   "source": [
    "# Check for non-finite values\n",
    "mask_nonfinite = ~np.isfinite(df_all[FEATURE_COLS].to_numpy()).all(axis=1)\n",
    "n_nonfinite = mask_nonfinite.sum()\n",
    "print(f\"Rows with non-finite feature values: {n_nonfinite}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st percentile for features:\n",
      " Delta          0.254321\n",
      "Theta          0.126250\n",
      "Alpha          0.108306\n",
      "Beta           0.191547\n",
      "Gamma          0.057350\n",
      "total_power    1.368929\n",
      "Name: 0.01, dtype: float64\n",
      "99th percentile for features:\n",
      " Delta          125.841570\n",
      "Theta           37.735722\n",
      "Alpha           42.706482\n",
      "Beta            23.624554\n",
      "Gamma           18.142717\n",
      "total_power    187.203228\n",
      "Name: 0.99, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# clip extreme outliers to stabilize covariance\n",
    "q_low = df_all[FEATURE_COLS].quantile(0.01)\n",
    "q_high = df_all[FEATURE_COLS].quantile(0.99)\n",
    "\n",
    "print(\"1st percentile for features:\\n\", q_low)\n",
    "print(\"99th percentile for features:\\n\", q_high)\n",
    "\n",
    "df_all[FEATURE_COLS] = df_all[FEATURE_COLS].clip(lower=q_low, upper=q_high, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# unique subjects in train: 16\n",
      "# unique subjects in test : 16\n",
      "# overlapping subjects    : 16\n",
      "WARNING: Some subject_ids appear in BOTH train and test!\n"
     ]
    }
   ],
   "source": [
    "# make sure subjects don't appear in both splits\n",
    "train_subj = set(df_train[\"subject_id\"])\n",
    "test_subj = set(df_test[\"subject_id\"])\n",
    "overlap = train_subj & test_subj\n",
    "\n",
    "print(f\"# unique subjects in train: {len(train_subj)}\")\n",
    "print(f\"# unique subjects in test : {len(test_subj)}\")\n",
    "print(f\"# overlapping subjects    : {len(overlap)}\")\n",
    "\n",
    "if overlap:\n",
    "    print(\"WARNING: Some subject_ids appear in BOTH train and test!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature summary (train only):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Delta</th>\n",
       "      <th>Theta</th>\n",
       "      <th>Alpha</th>\n",
       "      <th>Beta</th>\n",
       "      <th>Gamma</th>\n",
       "      <th>total_power</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>29952.000000</td>\n",
       "      <td>29952.000000</td>\n",
       "      <td>29952.000000</td>\n",
       "      <td>29952.000000</td>\n",
       "      <td>29952.000000</td>\n",
       "      <td>29952.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>15.364769</td>\n",
       "      <td>5.082069</td>\n",
       "      <td>4.801237</td>\n",
       "      <td>4.302120</td>\n",
       "      <td>6.272064</td>\n",
       "      <td>35.822258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>38.267332</td>\n",
       "      <td>7.820584</td>\n",
       "      <td>7.759449</td>\n",
       "      <td>8.368428</td>\n",
       "      <td>140.844286</td>\n",
       "      <td>155.180436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1%</th>\n",
       "      <td>0.278684</td>\n",
       "      <td>0.117415</td>\n",
       "      <td>0.102249</td>\n",
       "      <td>0.188556</td>\n",
       "      <td>0.057342</td>\n",
       "      <td>1.383196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>7.510876</td>\n",
       "      <td>2.990123</td>\n",
       "      <td>2.419021</td>\n",
       "      <td>2.820577</td>\n",
       "      <td>0.823266</td>\n",
       "      <td>21.263586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99%</th>\n",
       "      <td>125.753951</td>\n",
       "      <td>33.469466</td>\n",
       "      <td>41.614456</td>\n",
       "      <td>21.733572</td>\n",
       "      <td>18.851772</td>\n",
       "      <td>183.033393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1583.073287</td>\n",
       "      <td>295.867047</td>\n",
       "      <td>123.509311</td>\n",
       "      <td>274.945246</td>\n",
       "      <td>4638.140817</td>\n",
       "      <td>4889.477114</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Delta         Theta         Alpha          Beta         Gamma  \\\n",
       "count  29952.000000  29952.000000  29952.000000  29952.000000  29952.000000   \n",
       "mean      15.364769      5.082069      4.801237      4.302120      6.272064   \n",
       "std       38.267332      7.820584      7.759449      8.368428    140.844286   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "1%         0.278684      0.117415      0.102249      0.188556      0.057342   \n",
       "50%        7.510876      2.990123      2.419021      2.820577      0.823266   \n",
       "99%      125.753951     33.469466     41.614456     21.733572     18.851772   \n",
       "max     1583.073287    295.867047    123.509311    274.945246   4638.140817   \n",
       "\n",
       "        total_power  \n",
       "count  29952.000000  \n",
       "mean      35.822258  \n",
       "std      155.180436  \n",
       "min        0.000000  \n",
       "1%         1.383196  \n",
       "50%       21.263586  \n",
       "99%      183.033393  \n",
       "max     4889.477114  "
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# distribution summary per feature\n",
    "print(\"Feature summary (train only):\")\n",
    "df_train[FEATURE_COLS].describe(percentiles=[0.01, 0.5, 0.99])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Fix subject split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subject-wise train/test split\n",
    "subjects = df_all[\"subject_id\"].unique()\n",
    "rng = np.random.default_rng(42)\n",
    "rng.shuffle(subjects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train subjects: 8\n",
      "Test subjects : 8\n",
      "Overlap       : 0\n"
     ]
    }
   ],
   "source": [
    "# 50/50 split\n",
    "n_train = len(subjects) // 2\n",
    "train_subjects = set(subjects[:n_train])\n",
    "test_subjects = set(subjects[n_train:])\n",
    "\n",
    "df_train = df_all[df_all[\"subject_id\"].isin(train_subjects)].reset_index(drop=True)\n",
    "df_test  = df_all[df_all[\"subject_id\"].isin(test_subjects)].reset_index(drop=True)\n",
    "\n",
    "print(\"Train subjects:\", len(train_subjects))\n",
    "print(\"Test subjects :\", len(test_subjects))\n",
    "print(\"Overlap       :\", len(train_subjects & test_subjects))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split counts with new split:\n",
      "dataset_split\n",
      "train    30336\n",
      "test     30336\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Overwrite dataset_split using the NEW split\n",
    "df_all[SPLIT_COL] = np.where(df_all[\"subject_id\"].isin(train_subjects), \"train\", \"test\")\n",
    "df_train = df_all[df_all[\"subject_id\"].isin(train_subjects)].reset_index(drop=True)\n",
    "df_test = df_all[df_all[\"subject_id\"].isin(test_subjects)].reset_index(drop=True)\n",
    "print(\"Split counts with new split:\")\n",
    "print(df_all[SPLIT_COL].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape with new split: (30336, 13)\n",
      "Test shape with new split : (30336, 13)\n"
     ]
    }
   ],
   "source": [
    "print(\"Train shape with new split:\", df_train.shape)\n",
    "print(\"Test shape with new split :\", df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train label x condition with new split:\n",
      "matching_condition  S1 obj  S2 match  S2 nomatch,\n",
      "subject_type                                     \n",
      "a                     5120      5120         4992\n",
      "c                     5120      5056         4928\n",
      "Test label x condition with new split:\n",
      "matching_condition  S1 obj  S2 match  S2 nomatch,\n",
      "subject_type                                     \n",
      "a                     5120      5120         4928\n",
      "c                     5120      5120         4928\n"
     ]
    }
   ],
   "source": [
    "print(\"Train label x condition with new split:\")\n",
    "print(pd.crosstab(df_train[LABEL_COL], df_train[\"matching_condition\"]))\n",
    "\n",
    "print(\"Test label x condition with new split:\")\n",
    "print(pd.crosstab(df_test[LABEL_COL], df_test[\"matching_condition\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. outlier clipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clip extreme outliers to stabilize covariance\n",
    "q_low = df_all[FEATURE_COLS].quantile(0.01)\n",
    "q_high = df_all[FEATURE_COLS].quantile(0.99)\n",
    "\n",
    "print(\"1st percentile for features:\\n\", q_low)\n",
    "print(\"99th percentile for features:\\n\", q_high)\n",
    "\n",
    "df_all[FEATURE_COLS] = df_all[FEATURE_COLS].clip(lower=q_low, upper=q_high, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st percentile for features (train):\n",
      "Delta          0.254321\n",
      "Theta          0.126250\n",
      "Alpha          0.108306\n",
      "Beta           0.191547\n",
      "Gamma          0.057350\n",
      "total_power    1.368929\n",
      "Name: 0.01, dtype: float64\n",
      "\n",
      "99th percentile for features (train):\n",
      "Delta          125.841570\n",
      "Theta           37.735722\n",
      "Alpha           42.706482\n",
      "Beta            23.624554\n",
      "Gamma           18.142717\n",
      "total_power    187.203228\n",
      "Name: 0.99, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# QC & clip extreme outliers based on train\n",
    "# 1st and 99th percentiles\n",
    "print(\"1st percentile for features:\")\n",
    "print(df_all[FEATURE_COLS].quantile(0.01))\n",
    "\n",
    "print(\"\\n99th percentile for features:\")\n",
    "q99 = df_all[FEATURE_COLS].quantile(0.99)\n",
    "print(q99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After clipping, feature summary:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Delta</th>\n",
       "      <th>Theta</th>\n",
       "      <th>Alpha</th>\n",
       "      <th>Beta</th>\n",
       "      <th>Gamma</th>\n",
       "      <th>total_power</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>30336.000000</td>\n",
       "      <td>30336.000000</td>\n",
       "      <td>30336.000000</td>\n",
       "      <td>30336.000000</td>\n",
       "      <td>30336.000000</td>\n",
       "      <td>30336.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>14.354424</td>\n",
       "      <td>4.389380</td>\n",
       "      <td>4.706679</td>\n",
       "      <td>4.860909</td>\n",
       "      <td>2.428024</td>\n",
       "      <td>31.571229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>22.965832</td>\n",
       "      <td>5.185825</td>\n",
       "      <td>6.118530</td>\n",
       "      <td>5.116891</td>\n",
       "      <td>4.006887</td>\n",
       "      <td>34.882859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1%</th>\n",
       "      <td>0.215169</td>\n",
       "      <td>0.096783</td>\n",
       "      <td>0.095152</td>\n",
       "      <td>0.224659</td>\n",
       "      <td>0.095970</td>\n",
       "      <td>1.313202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>6.948689</td>\n",
       "      <td>2.620435</td>\n",
       "      <td>2.613439</td>\n",
       "      <td>3.277819</td>\n",
       "      <td>1.071558</td>\n",
       "      <td>21.417741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99%</th>\n",
       "      <td>159.056439</td>\n",
       "      <td>29.736372</td>\n",
       "      <td>36.346483</td>\n",
       "      <td>30.548577</td>\n",
       "      <td>26.652648</td>\n",
       "      <td>228.352634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>159.062183</td>\n",
       "      <td>29.737299</td>\n",
       "      <td>36.359885</td>\n",
       "      <td>30.574801</td>\n",
       "      <td>26.658036</td>\n",
       "      <td>228.416256</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Delta         Theta         Alpha          Beta         Gamma  \\\n",
       "count  30336.000000  30336.000000  30336.000000  30336.000000  30336.000000   \n",
       "mean      14.354424      4.389380      4.706679      4.860909      2.428024   \n",
       "std       22.965832      5.185825      6.118530      5.116891      4.006887   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "1%         0.215169      0.096783      0.095152      0.224659      0.095970   \n",
       "50%        6.948689      2.620435      2.613439      3.277819      1.071558   \n",
       "99%      159.056439     29.736372     36.346483     30.548577     26.652648   \n",
       "max      159.062183     29.737299     36.359885     30.574801     26.658036   \n",
       "\n",
       "        total_power  \n",
       "count  30336.000000  \n",
       "mean      31.571229  \n",
       "std       34.882859  \n",
       "min        0.000000  \n",
       "1%         1.313202  \n",
       "50%       21.417741  \n",
       "99%      228.352634  \n",
       "max      228.416256  "
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clip outliers at 99th percentile\n",
    "for col in FEATURE_COLS:\n",
    "    upper = q99[col]\n",
    "    df_all.loc[:, col] = df_all[col].clip(upper=upper)\n",
    "\n",
    "print(\"After clipping, feature summary:\")\n",
    "df_train[FEATURE_COLS].describe(percentiles=[0.01, 0.5, 0.99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (30336, 6)\n",
      "X_test shape : (30336, 6)\n",
      "y_train distribution: Counter({1: 15232, 0: 15104})\n",
      "y_test distribution : Counter({1: 15168, 0: 15168})\n",
      "Train condition counts: Counter({'S1 obj': 10240, 'S2 match': 10176, 'S2 nomatch,': 9920})\n",
      "Test condition counts : Counter({'S1 obj': 10240, 'S2 match': 10240, 'S2 nomatch,': 9856})\n"
     ]
    }
   ],
   "source": [
    "# Encode labels and standardize features\n",
    "\n",
    "# Map subject_type to 0/1\n",
    "label_map = {\"c\": 0, \"a\": 1}\n",
    "y_train = df_train[LABEL_COL].map(label_map).values\n",
    "y_test = df_test[LABEL_COL].map(label_map).values\n",
    "\n",
    "X_train_raw = df_train[FEATURE_COLS].values\n",
    "X_test_raw = df_test[FEATURE_COLS].values\n",
    "\n",
    "# Standardize based on train only\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train_raw)\n",
    "X_test = scaler.transform(X_test_raw)\n",
    "\n",
    "# Keep condition labels for condition-aware generation\n",
    "conds_train = df_train[\"matching_condition\"].values\n",
    "conds_test = df_test[\"matching_condition\"].values\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape :\", X_test.shape)\n",
    "print(\"y_train distribution:\", Counter(y_train))\n",
    "print(\"y_test distribution :\", Counter(y_test))\n",
    "print(\"Train condition counts:\", Counter(conds_train))\n",
    "print(\"Test condition counts :\", Counter(conds_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T18:46:28.995502Z",
     "iopub.status.busy": "2025-11-02T18:46:28.995178Z",
     "iopub.status.idle": "2025-11-02T18:46:31.530824Z",
     "shell.execute_reply": "2025-11-02T18:46:31.530534Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "RANDOM SEED SET FOR REPRODUCIBILITY\n",
      "Seed value: 42\n",
      "============================================================\n",
      "Phase 3: Synthetic EEG Generation (Condition-aware)\n",
      "============================================================\n",
      "Group filter      : ['a', 'c']\n",
      "Condition filter  : All\n",
      "Generator         : interp\n",
      "Balanced sampling : True\n",
      "k / noise         : (5, 0.0)\n",
      "Transfer targets  : None\n",
      "Save root         : /Users/jacksonzhao/Desktop/Synthetic_EEG_Generation/output/phase3_conditional\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "import warnings\n",
    "from collections import Counter, defaultdict\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from IPython.display import display\n",
    "from scipy import signal, stats\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.covariance import LedoitWolf\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, adjusted_rand_score, normalized_mutual_info_score, roc_auc_score, silhouette_score\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, train_test_split\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "import pickle\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "print('=' * 60)\n",
    "print('RANDOM SEED SET FOR REPRODUCIBILITY')\n",
    "print(f'Seed value: {RANDOM_SEED}')\n",
    "print('=' * 60)\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "\n",
    "DATA_PATH = Path.home() / '.cache/kagglehub/datasets/nnair25/Alcoholics/versions/1'\n",
    "TRAIN_PATH = DATA_PATH / 'SMNI_CMI_TRAIN'\n",
    "TEST_PATH = DATA_PATH / 'SMNI_CMI_TEST'\n",
    "\n",
    "CONDITION_LEVELS = ['S1', 'S2_match', 'S2_nomatch', 'UNKNOWN']\n",
    "\n",
    "RUN_CFG = dict(\n",
    "    groups='pooled',          \n",
    "    condition='each',       \n",
    "    generator='interp',      \n",
    "    balanced=True,           \n",
    "    k=5,\n",
    "    noise=0.0,\n",
    "    cond_onehot=False,       \n",
    "    transfer_to=None,        \n",
    "    save_dir='../output/phase3_conditional'\n",
    ")\n",
    "\n",
    "GROUP_CHOICES = {\n",
    "    'pooled': {'a', 'c'},\n",
    "    'a_only': {'a'},\n",
    "    'c_only': {'c'},\n",
    "}\n",
    "if RUN_CFG['groups'] not in GROUP_CHOICES:\n",
    "    raise ValueError(f\"Unknown group config: {RUN_CFG['groups']}\")\n",
    "GROUP_FILTER = GROUP_CHOICES[RUN_CFG['groups']]\n",
    "\n",
    "if RUN_CFG['condition'] in CONDITION_LEVELS:\n",
    "    CONDITION_FILTER = {RUN_CFG['condition']}\n",
    "elif RUN_CFG['condition'] in {'each', 'pooled'}:\n",
    "    CONDITION_FILTER = None\n",
    "else:\n",
    "    raise ValueError(f\"Unknown condition scope: {RUN_CFG['condition']}\")\n",
    "\n",
    "TRANSFER_TARGETS = set(RUN_CFG.get('transfer_to') or [])\n",
    "SAVE_ROOT = Path(RUN_CFG['save_dir'])\n",
    "SAVE_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print('Phase 3: Synthetic EEG Generation (Condition-aware)')\n",
    "print('=' * 60)\n",
    "print(f\"Group filter      : {sorted(GROUP_FILTER)}\")\n",
    "print(f\"Condition filter  : {sorted(CONDITION_FILTER) if CONDITION_FILTER else 'All'}\")\n",
    "print(f\"Generator         : {RUN_CFG['generator']}\")\n",
    "print(f\"Balanced sampling : {RUN_CFG['balanced']}\")\n",
    "print(f\"k / noise         : ({RUN_CFG['k']}, {RUN_CFG['noise']})\")\n",
    "print(f\"Transfer targets  : {sorted(TRANSFER_TARGETS) if TRANSFER_TARGETS else 'None'}\")\n",
    "print(f\"Save root         : {SAVE_ROOT.resolve()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Analysis Results from Phase 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T18:46:31.551382Z",
     "iopub.status.busy": "2025-11-02T18:46:31.551079Z",
     "iopub.status.idle": "2025-11-02T18:46:31.554498Z",
     "shell.execute_reply": "2025-11-02T18:46:31.554220Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Phase 2 analysis results\n",
      "Frequency bands: {'Delta': (0.5, 4), 'Theta': (4, 8), 'Alpha': (8, 13), 'Beta': (13, 30), 'Gamma': (30, 50)}\n",
      "Sampling rate: 256 Hz\n"
     ]
    }
   ],
   "source": [
    "# Load Phase 2 results (relative path to output folder)\n",
    "with open('../output/phase2_analysis_results.pkl', 'rb') as f:\n",
    "    analysis_results = pickle.load(f)\n",
    "\n",
    "FREQUENCY_BANDS = analysis_results['frequency_bands']\n",
    "SAMPLING_RATE = analysis_results['sampling_rate']\n",
    "\n",
    "print(\"Loaded Phase 2 analysis results\")\n",
    "print(f\"Frequency bands: {FREQUENCY_BANDS}\")\n",
    "print(f\"Sampling rate: {SAMPLING_RATE} Hz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Shared constants used across sections ----\n",
    "MIN_EVAL_SAMPLES = 25     \n",
    "BAND_NAMES = [\"Delta\",\"Theta\",\"Alpha\",\"Beta\",\"Gamma\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare Real Data for Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data & Feature Summary\n",
    "\n",
    "- **Corpus**: All available CSV files in `SMNI_CMI_TRAIN` (≈468 total; 235 alcoholic / 233 control). Adjust `MAX_FILES_PER_CLASS` if you need a subset.\n",
    "- **Epochs**: Dynamically determined from `sensor position × trial` combinations across the full corpus.\n",
    "- **Features**: 5-D band power vectors computed via Welch’s PSD at 256 Hz covering Δ (0.5–4 Hz), θ (4–8 Hz), α (8–13 Hz), β (13–30 Hz), γ (30–50 Hz).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T18:46:31.555803Z",
     "iopub.status.busy": "2025-11-02T18:46:31.555721Z",
     "iopub.status.idle": "2025-11-02T18:46:32.151968Z",
     "shell.execute_reply": "2025-11-02T18:46:32.151717Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identifying subject types...\n",
      "Found 235 alcoholic files\n",
      "Found 233 control files\n",
      "\n",
      "Loading 468 files for generator training...\n",
      "  Alcoholic files selected: 235\n",
      "  Control files selected  : 233\n"
     ]
    }
   ],
   "source": [
    "# Load training files\n",
    "train_files = sorted(list(TRAIN_PATH.glob('*.csv')))\n",
    "\n",
    "# Separate alcoholic and control files  \n",
    "alcoholic_files = []\n",
    "control_files = []\n",
    "\n",
    "print(\"Identifying subject types...\")\n",
    "for file in train_files:\n",
    "    df_peek = pd.read_csv(file, nrows=1)\n",
    "    subject_type = df_peek['subject identifier'].iloc[0]\n",
    "    if subject_type == 'a':\n",
    "        alcoholic_files.append(file)\n",
    "    else:\n",
    "        control_files.append(file)\n",
    "\n",
    "print(f\"Found {len(alcoholic_files)} alcoholic files\")\n",
    "print(f\"Found {len(control_files)} control files\")\n",
    "\n",
    "# Configure which files to load for generator training\n",
    "MAX_FILES_PER_CLASS = None\n",
    "\n",
    "if MAX_FILES_PER_CLASS is None:\n",
    "    selected_alcoholic = list(alcoholic_files)\n",
    "    selected_control = list(control_files)\n",
    "else:\n",
    "    selected_alcoholic = list(alcoholic_files[:MAX_FILES_PER_CLASS])\n",
    "    selected_control = list(control_files[:MAX_FILES_PER_CLASS])\n",
    "\n",
    "sample_files = selected_alcoholic + selected_control\n",
    "if not sample_files:\n",
    "    raise ValueError(\"No training files selected. Check MAX_FILES_PER_CLASS or dataset path.\")\n",
    "\n",
    "random.shuffle(sample_files)\n",
    "\n",
    "\n",
    "print(f\"\\nLoading {len(sample_files)} files for generator training...\")\n",
    "print(f\"  Alcoholic files selected: {len(selected_alcoholic)}\")\n",
    "print(f\"  Control files selected  : {len(selected_control)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Extract EEG Features from Real Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T18:46:32.153581Z",
     "iopub.status.busy": "2025-11-02T18:46:32.153474Z",
     "iopub.status.idle": "2025-11-02T18:46:32.964784Z",
     "shell.execute_reply": "2025-11-02T18:46:32.964476Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features from real EEG data...\n",
      "  Processed 46/468 files...\n",
      "  Processed 92/468 files...\n",
      "  Processed 138/468 files...\n",
      "  Processed 184/468 files...\n",
      "  Processed 230/468 files...\n",
      "  Processed 276/468 files...\n",
      "  Processed 322/468 files...\n",
      "  Processed 368/468 files...\n",
      "  Processed 414/468 files...\n",
      "  Processed 460/468 files...\n",
      "  Processed 468/468 files...\n",
      "Total epochs extracted : 2340\n",
      "Filtered epochs        : 2340\n",
      "Feature shape          : (2340, 5)\n",
      "Signal shape           : (2340, 256)\n",
      "Class distribution (filtered): Counter({1: 1175, 0: 1165})\n",
      "Condition distribution (filtered): Counter({'S1': 800, 'S2_match': 795, 'S2_nomatch': 745})\n",
      "Buckets (class, condition): Counter({(1, 'S2_match'): 400, (0, 'S1'): 400, (1, 'S1'): 400, (0, 'S2_match'): 395, (1, 'S2_nomatch'): 375, (0, 'S2_nomatch'): 370})\n"
     ]
    }
   ],
   "source": [
    "def extract_band_power(signal_data, fs=256, bands=None):\n",
    "    \"\"\"Extract power in each frequency band using Welch's method.\"\"\"\n",
    "    if bands is None:\n",
    "        bands = FREQUENCY_BANDS\n",
    "    freqs, psd = signal.welch(signal_data, fs=fs, nperseg=min(256, len(signal_data)))\n",
    "    band_powers = {}\n",
    "    for band_name, (low_freq, high_freq) in bands.items():\n",
    "        idx = np.logical_and(freqs >= low_freq, freqs <= high_freq)\n",
    "        band_power = np.trapz(psd[idx], freqs[idx]) if np.any(idx) else 0.0\n",
    "        band_powers[band_name] = float(band_power)\n",
    "    return band_powers\n",
    "\n",
    "MAX_CHANNELS = 5\n",
    "MAX_TRIALS = 2\n",
    "MIN_SAMPLES_PER_EPOCH = 128\n",
    "EPOCH_LENGTH = 256\n",
    "\n",
    "def _normalize_condition_token(value):\n",
    "    if value is None or (isinstance(value, float) and np.isnan(value)):\n",
    "        return 'UNKNOWN'\n",
    "    token = str(value).strip().lower().replace(',', '')\n",
    "    if not token:\n",
    "        return 'UNKNOWN'\n",
    "    if token.startswith('s1'):\n",
    "        return 'S1'\n",
    "    compact = token.replace(' ', '')\n",
    "    if 'nomatch' in compact or 'no-match' in compact:\n",
    "        return 'S2_nomatch'\n",
    "    if token.startswith('s2') or 'match' in token:\n",
    "        return 'S2_match'\n",
    "    return 'UNKNOWN'\n",
    "\n",
    "def get_condition_token(row):\n",
    "    \"\"\"Map a trial row to {S1, S2_match, S2_nomatch} tokens.\"\"\"\n",
    "    for key in ['condition', 'matching condition', 'stimulus']:\n",
    "        if key in row and pd.notna(row[key]):\n",
    "            token = _normalize_condition_token(row[key])\n",
    "            if token != 'UNKNOWN':\n",
    "                return token\n",
    "    stimulus = str(row.get('stimulus', '')).strip().upper()\n",
    "    if stimulus == 'S1':\n",
    "        return 'S1'\n",
    "    if stimulus == 'S2':\n",
    "        match_val = row.get('match', row.get('matching', 0))\n",
    "        try:\n",
    "            match_flag = int(match_val)\n",
    "        except (TypeError, ValueError):\n",
    "            match_flag = 0\n",
    "        return 'S2_match' if match_flag == 1 else 'S2_nomatch'\n",
    "    return 'UNKNOWN'\n",
    "\n",
    "all_features = []\n",
    "all_signals = []\n",
    "all_labels = []\n",
    "all_conditions = []\n",
    "all_groups = []\n",
    "\n",
    "print('Extracting features from real EEG data...')\n",
    "progress_interval = max(1, len(sample_files) // 10) if sample_files else 1\n",
    "for file_idx, file in enumerate(sample_files):\n",
    "    df = pd.read_csv(file)\n",
    "    subject_type = df['subject identifier'].iloc[0]\n",
    "\n",
    "    channels = df['sensor position'].unique()[:MAX_CHANNELS]\n",
    "    trials = df['trial number'].unique()[:MAX_TRIALS]\n",
    "\n",
    "    for channel in channels:\n",
    "        for trial in trials:\n",
    "            trial_data = df[\n",
    "                (df['sensor position'] == channel) &\n",
    "                (df['trial number'] == trial)\n",
    "            ].sort_values('sample num')\n",
    "\n",
    "            if len(trial_data) < MIN_SAMPLES_PER_EPOCH:\n",
    "                continue\n",
    "\n",
    "            signal_data = trial_data['sensor value'].values[:EPOCH_LENGTH]\n",
    "            band_powers = extract_band_power(signal_data)\n",
    "            cond_token = get_condition_token(trial_data.iloc[0])\n",
    "\n",
    "            all_features.append(list(band_powers.values()))\n",
    "            all_signals.append(signal_data)\n",
    "            all_labels.append(1 if subject_type == 'a' else 0)\n",
    "            all_conditions.append(cond_token)\n",
    "            all_groups.append(subject_type)\n",
    "\n",
    "    if ((file_idx + 1) % progress_interval == 0) or (file_idx + 1 == len(sample_files)):\n",
    "        print(f\"  Processed {file_idx + 1}/{len(sample_files)} files...\")\n",
    "\n",
    "all_features = np.array(all_features)\n",
    "all_signals = np.array(all_signals)\n",
    "all_labels = np.array(all_labels, dtype=int)\n",
    "all_conditions = np.array(all_conditions)\n",
    "all_groups = np.array(all_groups)\n",
    "\n",
    "if all_features.size == 0:\n",
    "    raise RuntimeError('No epochs extracted. Check dataset paths or filters.')\n",
    "\n",
    "group_mask = np.isin(all_groups, list(GROUP_FILTER))\n",
    "condition_mask = np.ones_like(all_conditions, dtype=bool)\n",
    "if CONDITION_FILTER:\n",
    "    condition_mask = np.isin(all_conditions, list(CONDITION_FILTER))\n",
    "selection_mask = group_mask & condition_mask\n",
    "if not np.any(selection_mask):\n",
    "    raise RuntimeError('Filters removed all data. Loosen GROUP_FILTER/CONDITION_FILTER.')\n",
    "\n",
    "real_features = all_features[selection_mask]\n",
    "real_signals = all_signals[selection_mask]\n",
    "labels = all_labels[selection_mask]\n",
    "cond_tokens = all_conditions[selection_mask]\n",
    "group_tokens = all_groups[selection_mask]\n",
    "\n",
    "FULL_DATASET = {\n",
    "    'features': all_features,\n",
    "    'labels': all_labels,\n",
    "    'conditions': all_conditions,\n",
    "    'groups': all_groups,\n",
    "}\n",
    "\n",
    "FILTERED_DATASET = {\n",
    "    'features': real_features,\n",
    "    'signals': real_signals,\n",
    "    'labels': labels,\n",
    "    'conditions': cond_tokens,\n",
    "    'groups': group_tokens,\n",
    "}\n",
    "\n",
    "print(f\"Total epochs extracted : {len(all_features)}\")\n",
    "print(f\"Filtered epochs        : {len(real_features)}\")\n",
    "print(f\"Feature shape          : {real_features.shape}\")\n",
    "print(f\"Signal shape           : {real_signals.shape}\")\n",
    "print('Class distribution (filtered):', Counter(labels.tolist()))\n",
    "print('Condition distribution (filtered):', Counter(cond_tokens.tolist()))\n",
    "print('Buckets (class, condition):', Counter(list(zip(labels.tolist(), cond_tokens.tolist()))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Read-Only Random Forest Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Real-only Random Forest baseline (TRTR)\n",
      "======================================================================\n",
      "Real epochs: 2,340 | Features: 5 -> ['Delta', 'Theta', 'Alpha', 'Beta', 'Gamma']\n",
      "Class counts: Counter({1: 1175, 0: 1165})\n",
      "Condition counts: Counter({'S1': 800, 'S2_match': 795, 'S2_nomatch': 745})\n",
      "\n",
      "[Overall TRTR]\n",
      "Accuracy: 0.7009 | ROC AUC: 0.7715\n",
      "Confusion matrix:\n",
      " [[249 101]\n",
      " [109 243]]\n",
      "\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.696     0.711     0.703       350\n",
      "           1      0.706     0.690     0.698       352\n",
      "\n",
      "    accuracy                          0.701       702\n",
      "   macro avg      0.701     0.701     0.701       702\n",
      "weighted avg      0.701     0.701     0.701       702\n",
      "\n",
      "Feature importances:\n",
      "         Theta: 0.2432\n",
      "         Gamma: 0.2122\n",
      "         Alpha: 0.1905\n",
      "         Delta: 0.1804\n",
      "          Beta: 0.1737\n",
      "\n",
      "5-fold TRTR cross-validation on real data:\n",
      "  Fold 1: accuracy = 0.7051\n",
      "  Fold 2: accuracy = 0.6688\n",
      "  Fold 3: accuracy = 0.6816\n",
      "  Fold 4: accuracy = 0.7115\n",
      "  Fold 5: accuracy = 0.6944\n",
      "CV mean accuracy: 0.6923 ± 0.0155\n",
      "\n",
      "[Condition S1] TRTR Accuracy: 0.6917 | ROC AUC: 0.7730\n",
      "Confusion matrix:\n",
      " [[85 35]\n",
      " [39 81]]\n",
      "\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.685     0.708     0.697       120\n",
      "           1      0.698     0.675     0.686       120\n",
      "\n",
      "    accuracy                          0.692       240\n",
      "   macro avg      0.692     0.692     0.692       240\n",
      "weighted avg      0.692     0.692     0.692       240\n",
      "\n",
      "\n",
      "[Condition S2_match] TRTR Accuracy: 0.7364 | ROC AUC: 0.8249\n",
      "Confusion matrix:\n",
      " [[86 33]\n",
      " [30 90]]\n",
      "\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.741     0.723     0.732       119\n",
      "           1      0.732     0.750     0.741       120\n",
      "\n",
      "    accuracy                          0.736       239\n",
      "   macro avg      0.737     0.736     0.736       239\n",
      "weighted avg      0.737     0.736     0.736       239\n",
      "\n",
      "\n",
      "[Condition S2_nomatch] TRTR Accuracy: 0.7411 | ROC AUC: 0.8375\n",
      "Confusion matrix:\n",
      " [[82 29]\n",
      " [29 84]]\n",
      "\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.739     0.739     0.739       111\n",
      "           1      0.743     0.743     0.743       113\n",
      "\n",
      "    accuracy                          0.741       224\n",
      "   macro avg      0.741     0.741     0.741       224\n",
      "weighted avg      0.741     0.741     0.741       224\n",
      "\n",
      "\n",
      "Baseline results saved to: /Users/jacksonzhao/Desktop/Synthetic_EEG_Generation/output/phase3_conditional/baseline/random_forest_baseline.json\n",
      "Use this TRTR baseline to compare against synthetic TSTR/TRTR scores later on.\n"
     ]
    }
   ],
   "source": [
    "# 3.1 — Real-only Random Forest Baseline\n",
    "# --------------------------------------\n",
    "# Builds a baseline classifier using only real band-power features before any synthetic generation.\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Real-only Random Forest baseline (TRTR)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Construct / refresh the real-only DataFrame so downstream sections can reuse it.\n",
    "if 'df_real' not in globals() or len(df_real) != len(real_features):\n",
    "    df_real = pd.DataFrame(real_features, columns=BAND_NAMES)\n",
    "    df_real['label'] = labels.astype(int)\n",
    "    df_real['Condition'] = cond_tokens\n",
    "    df_real['Group'] = group_tokens\n",
    "\n",
    "BAND_COLS = BAND_NAMES\n",
    "LABEL_COL = 'label'\n",
    "COND_COL = 'Condition'\n",
    "\n",
    "X_real = df_real[BAND_COLS].to_numpy(dtype=float)\n",
    "y_real = df_real[LABEL_COL].to_numpy(dtype=int)\n",
    "cond_real = df_real[COND_COL].astype(str).to_numpy()\n",
    "\n",
    "print(f\"Real epochs: {len(df_real):,} | Features: {len(BAND_COLS)} -> {BAND_COLS}\")\n",
    "print('Class counts:', Counter(y_real.tolist()))\n",
    "print('Condition counts:', Counter(cond_real.tolist()))\n",
    "\n",
    "rf_kwargs = dict(\n",
    "    n_estimators=300,\n",
    "    max_depth=None,\n",
    "    min_samples_split=2,\n",
    "    class_weight='balanced',\n",
    "    random_state=RANDOM_SEED,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "# Overall TRTR baseline -------------------------------------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_real,\n",
    "    y_real,\n",
    "    test_size=0.30,\n",
    "    stratify=y_real,\n",
    "    random_state=RANDOM_SEED,\n",
    ")\n",
    "\n",
    "rf_baseline = RandomForestClassifier(**rf_kwargs)\n",
    "rf_baseline.fit(X_train, y_train)\n",
    "\n",
    "y_pred = rf_baseline.predict(X_test)\n",
    "y_proba = rf_baseline.predict_proba(X_test)[:, 1]\n",
    "acc_overall = accuracy_score(y_test, y_pred)\n",
    "try:\n",
    "    auc_overall = roc_auc_score(y_test, y_proba)\n",
    "except ValueError:\n",
    "    auc_overall = float('nan')\n",
    "cm_overall = confusion_matrix(y_test, y_pred)\n",
    "report_overall = classification_report(y_test, y_pred, digits=3)\n",
    "\n",
    "print(\"\\n[Overall TRTR]\")\n",
    "print(f\"Accuracy: {acc_overall:.4f} | ROC AUC: {auc_overall:.4f}\")\n",
    "print('Confusion matrix:\\n', cm_overall)\n",
    "print('\\nClassification report:\\n', report_overall)\n",
    "\n",
    "feat_importances = sorted(\n",
    "    zip(BAND_COLS, rf_baseline.feature_importances_),\n",
    "    key=lambda x: x[1],\n",
    "    reverse=True,\n",
    ")\n",
    "print(\"Feature importances:\")\n",
    "for name, val in feat_importances:\n",
    "    print(f\"  {name:>12s}: {val:.4f}\")\n",
    "\n",
    "# Stratified CV for stability -------------------------------------------------\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_SEED)\n",
    "cv_accs = []\n",
    "print(\"\\n5-fold TRTR cross-validation on real data:\")\n",
    "for fold_idx, (tr_idx, te_idx) in enumerate(skf.split(X_real, y_real), start=1):\n",
    "    model_cv = RandomForestClassifier(**rf_kwargs)\n",
    "    model_cv.fit(X_real[tr_idx], y_real[tr_idx])\n",
    "    fold_pred = model_cv.predict(X_real[te_idx])\n",
    "    fold_acc = accuracy_score(y_real[te_idx], fold_pred)\n",
    "    cv_accs.append(float(fold_acc))\n",
    "    print(f\"  Fold {fold_idx}: accuracy = {fold_acc:.4f}\")\n",
    "\n",
    "cv_mean = float(np.mean(cv_accs))\n",
    "cv_std = float(np.std(cv_accs))\n",
    "print(f\"CV mean accuracy: {cv_mean:.4f} ± {cv_std:.4f}\")\n",
    "\n",
    "# Condition-specific baselines ------------------------------------------------\n",
    "condition_results = {}\n",
    "for cond_name in sorted(np.unique(cond_real)):\n",
    "    mask = cond_real == cond_name\n",
    "    if mask.sum() < 25:\n",
    "        print(f\"\\n[WARN] Condition {cond_name} has only {mask.sum()} samples; skipping per-condition TRTR.\")\n",
    "        continue\n",
    "\n",
    "    Xc = X_real[mask]\n",
    "    yc = y_real[mask]\n",
    "    if len(np.unique(yc)) < 2:\n",
    "        print(f\"\\n[WARN] Condition {cond_name} lacks class diversity; skipping.\")\n",
    "        continue\n",
    "\n",
    "    Xc_tr, Xc_te, yc_tr, yc_te = train_test_split(\n",
    "        Xc,\n",
    "        yc,\n",
    "        test_size=0.30,\n",
    "        stratify=yc,\n",
    "        random_state=RANDOM_SEED,\n",
    "    )\n",
    "\n",
    "    rf_cond = RandomForestClassifier(**rf_kwargs)\n",
    "    rf_cond.fit(Xc_tr, yc_tr)\n",
    "\n",
    "    yc_pred = rf_cond.predict(Xc_te)\n",
    "    yc_proba = rf_cond.predict_proba(Xc_te)[:, 1]\n",
    "    acc_c = accuracy_score(yc_te, yc_pred)\n",
    "    try:\n",
    "        auc_c = roc_auc_score(yc_te, yc_proba)\n",
    "    except ValueError:\n",
    "        auc_c = float('nan')\n",
    "    cm_c = confusion_matrix(yc_te, yc_pred)\n",
    "    report_c = classification_report(yc_te, yc_pred, digits=3)\n",
    "\n",
    "    print(f\"\\n[Condition {cond_name}] TRTR Accuracy: {acc_c:.4f} | ROC AUC: {auc_c:.4f}\")\n",
    "    print('Confusion matrix:\\n', cm_c)\n",
    "    print('\\nClassification report:\\n', report_c)\n",
    "\n",
    "    condition_results[cond_name] = {\n",
    "        'TRTR_acc': float(acc_c),\n",
    "        'ROC_AUC': float(auc_c),\n",
    "        'confusion_matrix': cm_c.tolist(),\n",
    "        'classification_report': report_c,\n",
    "        'n_samples': int(mask.sum()),\n",
    "    }\n",
    "\n",
    "# Persist baseline summary ----------------------------------------------------\n",
    "baseline_summary = {\n",
    "    'overall': {\n",
    "        'TRTR_acc': float(acc_overall),\n",
    "        'ROC_AUC': float(auc_overall),\n",
    "        'confusion_matrix': cm_overall.tolist(),\n",
    "        'classification_report': report_overall,\n",
    "        'cv_fold_acc': cv_accs,\n",
    "        'cv_mean_acc': cv_mean,\n",
    "        'cv_std_acc': cv_std,\n",
    "        'feature_importances': {name: float(val) for name, val in feat_importances},\n",
    "    },\n",
    "    'by_condition': condition_results,\n",
    "    'meta': {\n",
    "        'n_samples': int(len(df_real)),\n",
    "        'n_features': int(len(BAND_COLS)),\n",
    "        'bands': BAND_COLS,\n",
    "        'label_col': LABEL_COL,\n",
    "        'condition_col': COND_COL,\n",
    "    },\n",
    "}\n",
    "\n",
    "baseline_dir = SAVE_ROOT / 'baseline'\n",
    "baseline_dir.mkdir(parents=True, exist_ok=True)\n",
    "out_path = baseline_dir / 'random_forest_baseline.json'\n",
    "with out_path.open('w') as f:\n",
    "    json.dump(baseline_summary, f, indent=2)\n",
    "\n",
    "print('\\nBaseline results saved to:', out_path.resolve())\n",
    "print('Use this TRTR baseline to compare against synthetic TSTR/TRTR scores later on.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Method 1: Correlation Sampling Approach\n",
    "\n",
    "Based on \"A Statistical Approach for Synthetic EEG Data Generation\"\n",
    "\n",
    "Steps:\n",
    "1. Compute correlation matrix of frequency band features\n",
    "2. Sample from multivariate normal distribution preserving correlations\n",
    "3. Generate synthetic features matching real data statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T18:46:32.966524Z",
     "iopub.status.busy": "2025-11-02T18:46:32.966415Z",
     "iopub.status.idle": "2025-11-02T18:46:32.970196Z",
     "shell.execute_reply": "2025-11-02T18:46:32.969965Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nGenerating 2340 synthetic samples...\n",
      "Correlation Matrix of Frequency Bands:\n",
      "Delta  - Delta :  1.000\n",
      "Delta  - Theta :  0.570\n",
      "Delta  - Alpha :  0.025\n",
      "Delta  - Beta  : -0.048\n",
      "Delta  - Gamma : -0.057\n",
      "Theta  - Theta :  1.000\n",
      "Theta  - Alpha :  0.352\n",
      "Theta  - Beta  : -0.019\n",
      "Theta  - Gamma : -0.010\n",
      "Alpha  - Alpha :  1.000\n",
      "Alpha  - Beta  :  0.041\n",
      "Alpha  - Gamma :  0.042\n",
      "Beta   - Beta  :  1.000\n",
      "Beta   - Gamma :  0.952\n",
      "Gamma  - Gamma :  1.000\n",
      "\\nGenerated 2340 synthetic feature vectors\n",
      "Correlation structure preserved\n",
      "Shape of synthetic features: (2340, 5)\n"
     ]
    }
   ],
   "source": [
    "def generate_correlation_based_eeg(real_features, n_synthetic=100, random_seed=42):\n",
    "    \"\"\"\n",
    "    Generate synthetic EEG using correlation sampling method\n",
    "    \n",
    "    This preserves the correlation structure between frequency bands\n",
    "    \"\"\"\n",
    "    # Set random seed for reproducibility\n",
    "    np.random.seed(random_seed)\n",
    "    \n",
    "    # Compute correlation matrix and statistics\n",
    "    correlation_matrix = np.corrcoef(real_features.T)\n",
    "    mean_features = np.mean(real_features, axis=0)\n",
    "    std_features = np.std(real_features, axis=0)\n",
    "    \n",
    "    print(\"Correlation Matrix of Frequency Bands:\")\n",
    "    band_names = list(FREQUENCY_BANDS.keys())\n",
    "    for i, band1 in enumerate(band_names):\n",
    "        for j, band2 in enumerate(band_names):\n",
    "            if j >= i:\n",
    "                print(f\"{band1:6s} - {band2:6s}: {correlation_matrix[i,j]:6.3f}\")\n",
    "    \n",
    "    # Generate synthetic features preserving correlation structure\n",
    "    covariance_matrix = np.outer(std_features, std_features) * correlation_matrix\n",
    "    \n",
    "    synthetic_features = np.random.multivariate_normal(\n",
    "        mean_features,\n",
    "        covariance_matrix,\n",
    "        size=n_synthetic\n",
    "    )\n",
    "    \n",
    "    # Ensure non-negative powers\n",
    "    synthetic_features = np.abs(synthetic_features)\n",
    "    \n",
    "    print(f\"\\\\nGenerated {n_synthetic} synthetic feature vectors\")\n",
    "    print(f\"Correlation structure preserved\")\n",
    "    \n",
    "    return synthetic_features, correlation_matrix\n",
    "\n",
    "# Generate synthetic data\n",
    "n_synthetic_samples = len(real_features)\n",
    "print(f\"\\\\nGenerating {n_synthetic_samples} synthetic samples...\")\n",
    "\n",
    "synthetic_features_corr, corr_matrix = generate_correlation_based_eeg(\n",
    "    real_features,\n",
    "    n_synthetic=n_synthetic_samples,\n",
    "    random_seed=RANDOM_SEED\n",
    ")\n",
    "\n",
    "print(f\"Shape of synthetic features: {synthetic_features_corr.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Method 2: Neural Generators\n",
    "\n",
    "We benchmark two neural approaches in addition to the correlation sampler:\n",
    "\n",
    "1. **Mixup Baseline** – deterministic interpolation + Gaussian jitter (no adversary) for a quick sanity check.\n",
    "2. **WGAN-GP** – fully adversarial training with gradient penalty (Gulrajani et al., 2017) implemented in PyTorch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T18:46:32.971372Z",
     "iopub.status.busy": "2025-11-02T18:46:32.971299Z",
     "iopub.status.idle": "2025-11-02T18:46:32.984472Z",
     "shell.execute_reply": "2025-11-02T18:46:32.984242Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating synthetic data using mixup baseline...\n",
      "Generated 2340 synthetic samples\n",
      "Shape: (2340, 5)\n"
     ]
    }
   ],
   "source": [
    "def generate_mixup_baseline(real_features, n_synthetic=100, random_seed=42):\n",
    "    \"\"\"\n",
    "    Mixup-style baseline using:\n",
    "    - Interpolation between randomly sampled real points\n",
    "    - Addition of controlled gaussian noise\n",
    "    \n",
    "    Serves as a lightweight reference model prior to adversarial training.\n",
    "    \"\"\"\n",
    "    np.random.seed(random_seed)\n",
    "\n",
    "    synthetic_features = []\n",
    "\n",
    "    for _ in range(n_synthetic):\n",
    "        idx1, idx2 = np.random.choice(len(real_features), 2, replace=False)\n",
    "        alpha = np.random.uniform(0.3, 0.7)\n",
    "\n",
    "        interpolated = alpha * real_features[idx1] + (1 - alpha) * real_features[idx2]\n",
    "\n",
    "        noise_scale = 0.1 * np.std(real_features, axis=0)\n",
    "        noise = np.random.normal(0, noise_scale)\n",
    "        synthetic_sample = interpolated + noise\n",
    "\n",
    "        synthetic_sample = np.abs(synthetic_sample)\n",
    "        synthetic_features.append(synthetic_sample)\n",
    "\n",
    "    return np.array(synthetic_features)\n",
    "\n",
    "print(\"Generating synthetic data using mixup baseline...\")\n",
    "synthetic_features_mixup = generate_mixup_baseline(\n",
    "    real_features,\n",
    "    n_synthetic=n_synthetic_samples,\n",
    "    random_seed=RANDOM_SEED\n",
    ")\n",
    "\n",
    "print(f\"Generated {len(synthetic_features_mixup)} synthetic samples\")\n",
    "print(f\"Shape: {synthetic_features_mixup.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training WGAN-GP generator (this may take a couple of minutes)...\n",
      "Epoch 050/300 | D: -22.4700 | G: 58.2359 | preview mean=[8.895 8.354 5.571 8.843 4.666]\n",
      "Epoch 100/300 | D: -30.9784 | G: 88.3384 | preview mean=[20.698  5.326  4.246  7.774  0.747]\n",
      "Epoch 150/300 | D: -45.1265 | G: 156.7274 | preview mean=[28.294  7.204  5.142  5.656  1.996]\n",
      "Epoch 200/300 | D: -42.9474 | G: 277.4485 | preview mean=[26.767  5.771  5.429  4.627  2.58 ]\n",
      "Epoch 250/300 | D: -57.5499 | G: 426.3501 | preview mean=[24.252  6.067  5.075  6.867  2.8  ]\n",
      "Epoch 300/300 | D: -80.9785 | G: 641.9765 | preview mean=[27.753  6.476  5.238  5.901  1.241]\n",
      "Generated 2340 WGAN-GP samples\n",
      "Shape: (2340, 5)\n"
     ]
    }
   ],
   "source": [
    "# --- WGAN-GP generator for frequency-band features ----------------------------------\n",
    "\n",
    "def generate_wgangp_eeg(\n",
    "    real_features,\n",
    "    n_synthetic=100,\n",
    "    noise_dim=16,\n",
    "    hidden_dim=64,\n",
    "    n_critic=5,\n",
    "    gp_lambda=10.0,\n",
    "    lr=1e-4,\n",
    "    batch_size=128,\n",
    "    epochs=300,\n",
    "    random_seed=42,\n",
    "):\n",
    "    \"\"\"Train a compact WGAN-GP on band-power features and return synthetic samples.\"\"\"\n",
    "    if torch is None:\n",
    "        raise ImportError(\n",
    "            \"PyTorch is required for WGAN-GP synthesis. Install torch>=2.0 to enable this path.\"\n",
    "        )\n",
    "\n",
    "    from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "    torch.manual_seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    data = torch.from_numpy(real_features.astype(np.float32))\n",
    "    dataset = TensorDataset(data)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "    feature_dim = real_features.shape[1]\n",
    "\n",
    "    class Generator(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.net = nn.Sequential(\n",
    "                nn.Linear(noise_dim, hidden_dim),\n",
    "                nn.LayerNorm(hidden_dim),\n",
    "                nn.GELU(),\n",
    "                nn.Linear(hidden_dim, hidden_dim),\n",
    "                nn.GELU(),\n",
    "                nn.Linear(hidden_dim, feature_dim),\n",
    "            )\n",
    "\n",
    "        def forward(self, z):\n",
    "            x = self.net(z)\n",
    "            return torch.nn.functional.softplus(x)\n",
    "\n",
    "    class Critic(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.net = nn.Sequential(\n",
    "                nn.Linear(feature_dim, hidden_dim),\n",
    "                nn.LayerNorm(hidden_dim),\n",
    "                nn.GELU(),\n",
    "                nn.Linear(hidden_dim, hidden_dim),\n",
    "                nn.GELU(),\n",
    "                nn.Linear(hidden_dim, 1),\n",
    "            )\n",
    "\n",
    "        def forward(self, x):\n",
    "            return self.net(x)\n",
    "\n",
    "    def gradient_penalty(critic, real, fake):\n",
    "        batch_size = real.size(0)\n",
    "        epsilon = torch.rand(batch_size, 1, device=real.device)\n",
    "        epsilon = epsilon.expand_as(real)\n",
    "        interpolated = epsilon * real + (1 - epsilon) * fake\n",
    "        interpolated.requires_grad_(True)\n",
    "        mixed_scores = critic(interpolated)\n",
    "        grad = torch.autograd.grad(\n",
    "            outputs=mixed_scores,\n",
    "            inputs=interpolated,\n",
    "            grad_outputs=torch.ones_like(mixed_scores),\n",
    "            create_graph=True,\n",
    "            retain_graph=True,\n",
    "        )[0]\n",
    "        grad = grad.view(batch_size, -1)\n",
    "        gp = ((grad.norm(2, dim=1) - 1) ** 2).mean()\n",
    "        return gp\n",
    "\n",
    "    G = Generator().to(device)\n",
    "    D = Critic().to(device)\n",
    "\n",
    "    opt_G = optim.Adam(G.parameters(), lr=lr, betas=(0.5, 0.9))\n",
    "    opt_D = optim.Adam(D.parameters(), lr=lr, betas=(0.5, 0.9))\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for i, (real_batch,) in enumerate(loader):\n",
    "            real_batch = real_batch.to(device)\n",
    "\n",
    "            for _ in range(n_critic):\n",
    "                z = torch.randn(real_batch.size(0), noise_dim, device=device)\n",
    "                fake_batch = G(z).detach()\n",
    "\n",
    "                opt_D.zero_grad()\n",
    "                critic_real = D(real_batch).mean()\n",
    "                critic_fake = D(fake_batch).mean()\n",
    "                gp = gradient_penalty(D, real_batch, fake_batch)\n",
    "                loss_D = -(critic_real - critic_fake) + gp_lambda * gp\n",
    "                loss_D.backward()\n",
    "                opt_D.step()\n",
    "\n",
    "            z = torch.randn(real_batch.size(0), noise_dim, device=device)\n",
    "            opt_G.zero_grad()\n",
    "            fake_batch = G(z)\n",
    "            loss_G = -D(fake_batch).mean()\n",
    "            loss_G.backward()\n",
    "            opt_G.step()\n",
    "\n",
    "        if (epoch + 1) % 50 == 0:\n",
    "            with torch.no_grad():\n",
    "                z = torch.randn(batch_size, noise_dim, device=device)\n",
    "                preview = G(z).cpu().numpy()\n",
    "            preview_mean = preview.mean(axis=0)\n",
    "            print(\n",
    "                f\"Epoch {epoch + 1:03d}/{epochs} | D: {loss_D.item():.4f} | G: {loss_G.item():.4f} | preview mean={preview_mean.round(3)}\"\n",
    "            )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        synth_chunks = []\n",
    "        remaining = n_synthetic\n",
    "        while remaining > 0:\n",
    "            current = min(batch_size, remaining)\n",
    "            z = torch.randn(current, noise_dim, device=device)\n",
    "            synth = G(z).cpu().numpy()\n",
    "            synth_chunks.append(synth)\n",
    "            remaining -= current\n",
    "\n",
    "    synthetic = np.vstack(synth_chunks)\n",
    "    synthetic = np.clip(synthetic, a_min=0.0, a_max=None)\n",
    "    return synthetic\n",
    "\n",
    "print(\"Training WGAN-GP generator (this may take a couple of minutes)...\")\n",
    "if torch is None:\n",
    "    print(\"PyTorch not installed; skipping WGAN-GP synthesis.\")\n",
    "    synthetic_features_wgangp = None\n",
    "else:\n",
    "    synthetic_features_wgangp = generate_wgangp_eeg(\n",
    "        real_features,\n",
    "        n_synthetic=n_synthetic_samples,\n",
    "        random_seed=RANDOM_SEED,\n",
    "    )\n",
    "    print(f\"Generated {len(synthetic_features_wgangp)} WGAN-GP samples\")\n",
    "    print(f\"Shape: {synthetic_features_wgangp.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluation: Distribution Comparison (KS Test & MMD)\n",
    "\n",
    "Statistical tests to compare real vs synthetic data distributions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rbf_mmd(X, Y, gamma=None):\n",
    "    \"\"\"Biased RBF-kernel MMD with optional median-heuristic bandwidth.\"\"\"\n",
    "    X = np.asarray(X, dtype=float)\n",
    "    Y = np.asarray(Y, dtype=float)\n",
    "    if X.size == 0 or Y.size == 0:\n",
    "        return 0.0\n",
    "\n",
    "    if gamma is None:\n",
    "        Z = np.vstack([X, Y])\n",
    "        pairwise = cdist(Z, Z, metric='euclidean')\n",
    "        nonzero = pairwise[pairwise > 0]\n",
    "        median = np.median(nonzero) if nonzero.size else 1.0\n",
    "        if not np.isfinite(median) or median <= 0:\n",
    "            median = 1.0\n",
    "        gamma = 1.0 / (2.0 * median ** 2)\n",
    "\n",
    "    def _kernel(a, b):\n",
    "        d2 = cdist(a, b, metric='sqeuclidean')\n",
    "        return np.exp(-gamma * d2)\n",
    "\n",
    "    Kxx = _kernel(X, X)\n",
    "    Kyy = _kernel(Y, Y)\n",
    "    Kxy = _kernel(X, Y)\n",
    "\n",
    "    return float(Kxx.mean() + Kyy.mean() - 2.0 * Kxy.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T18:46:32.985832Z",
     "iopub.status.busy": "2025-11-02T18:46:32.985759Z",
     "iopub.status.idle": "2025-11-02T18:46:32.994970Z",
     "shell.execute_reply": "2025-11-02T18:46:32.994718Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\n============================================================\n",
      "Distribution Comparison: Correlation Sampling\n",
      "============================================================\n",
      "\\nKolmogorov-Smirnov Test Results:\n",
      "(p-value > 0.05 suggests distributions are similar)\n",
      "  Delta   : KS=0.3573, p=0.0000 ✗ Different\n",
      "  Theta   : KS=0.2269, p=0.0000 ✗ Different\n",
      "  Alpha   : KS=0.3338, p=0.0000 ✗ Different\n",
      "  Beta    : KS=0.5679, p=0.0000 ✗ Different\n",
      "  Gamma   : KS=0.9350, p=0.0000 ✗ Different\n",
      "\n",
      "MMD (RBF) Score: 0.647999\n",
      "(Lower MMD indicates more similar distributions)\n",
      "\\n============================================================\n",
      "Distribution Comparison: Mixup Baseline\n",
      "============================================================\n",
      "\\nKolmogorov-Smirnov Test Results:\n",
      "(p-value > 0.05 suggests distributions are similar)\n",
      "  Delta   : KS=0.1385, p=0.0000 ✗ Different\n",
      "  Theta   : KS=0.1440, p=0.0000 ✗ Different\n",
      "  Alpha   : KS=0.1321, p=0.0000 ✗ Different\n",
      "  Beta    : KS=0.1154, p=0.0000 ✗ Different\n",
      "  Gamma   : KS=0.8192, p=0.0000 ✗ Different\n",
      "\n",
      "MMD (RBF) Score: 0.278642\n",
      "(Lower MMD indicates more similar distributions)\n",
      "\\n============================================================\n",
      "Distribution Comparison: WGAN-GP\n",
      "============================================================\n",
      "\\nKolmogorov-Smirnov Test Results:\n",
      "(p-value > 0.05 suggests distributions are similar)\n",
      "  Delta   : KS=0.1077, p=0.0000 ✗ Different\n",
      "  Theta   : KS=0.0829, p=0.0000 ✗ Different\n",
      "  Alpha   : KS=0.1893, p=0.0000 ✗ Different\n",
      "  Beta    : KS=0.0709, p=0.0000 ✗ Different\n",
      "  Gamma   : KS=0.5073, p=0.0000 ✗ Different\n",
      "\n",
      "MMD (RBF) Score: 0.015234\n",
      "(Lower MMD indicates more similar distributions)\n"
     ]
    }
   ],
   "source": [
    "def evaluate_distributions(real_features, synthetic_features, method_name=\"\"):\n",
    "    \"\"\"\n",
    "    Compare distributions using:\n",
    "    - KS test (Kolmogorov-Smirnov): tests if distributions are from same population\n",
    "    - MMD (Maximum Mean Discrepancy): measures distance between distributions\n",
    "    \"\"\"\n",
    "    print(f\"\\\\n{'='*60}\")\n",
    "    print(f\"Distribution Comparison: {method_name}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # KS test for each feature (frequency band)\n",
    "    band_names = list(FREQUENCY_BANDS.keys())\n",
    "    ks_results = []\n",
    "    \n",
    "    print(\"\\\\nKolmogorov-Smirnov Test Results:\")\n",
    "    print(\"(p-value > 0.05 suggests distributions are similar)\")\n",
    "    for i, band in enumerate(band_names):\n",
    "        ks_stat, p_value = stats.ks_2samp(real_features[:, i], synthetic_features[:, i])\n",
    "        ks_results.append({'band': band, 'ks_stat': ks_stat, 'p_value': p_value})\n",
    "        \n",
    "        significance = \"✓ Similar\" if p_value > 0.05 else \"✗ Different\"\n",
    "        print(f\"  {band:8s}: KS={ks_stat:.4f}, p={p_value:.4f} {significance}\")\n",
    "    \n",
    "    # Simplified MMD computation\n",
    "    def compute_mmd(X, Y):\n",
    "        \"\"\"Maximum Mean Discrepancy using pairwise distances\"\"\"\n",
    "        XX = cdist(X, X, metric='euclidean')\n",
    "        YY = cdist(Y, Y, metric='euclidean')\n",
    "        XY = cdist(X, Y, metric='euclidean')\n",
    "        \n",
    "        mmd = np.mean(XX) + np.mean(YY) - 2 * np.mean(XY)\n",
    "        return mmd\n",
    "    \n",
    "    mmd_score = rbf_mmd(real_features, synthetic_features)\n",
    "    print(f\"\\nMMD (RBF) Score: {mmd_score:.6f}\")\n",
    "    print(\"(Lower MMD indicates more similar distributions)\")\n",
    "\n",
    "    \n",
    "    return ks_results, mmd_score\n",
    "\n",
    "distribution_results = {}\n",
    "\n",
    "distribution_results['Correlation Sampling'] = evaluate_distributions(\n",
    "    real_features,\n",
    "    synthetic_features_corr,\n",
    "    \"Correlation Sampling\"\n",
    ")\n",
    "\n",
    "distribution_results['Mixup Baseline'] = evaluate_distributions(\n",
    "    real_features,\n",
    "    synthetic_features_mixup,\n",
    "    \"Mixup Baseline\"\n",
    ")\n",
    "\n",
    "if synthetic_features_wgangp is not None:\n",
    "    distribution_results['WGAN-GP'] = evaluate_distributions(\n",
    "        real_features,\n",
    "        synthetic_features_wgangp,\n",
    "        \"WGAN-GP\"\n",
    "    )\n",
    "else:\n",
    "    print(\"Skipping WGAN-GP distribution metrics (PyTorch unavailable).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluation: TSTR and TRTR\n",
    "\n",
    "**TRTR** = Train on Real, Test on Real  \n",
    "**TSTR** = Train on Synthetic, Test on Real\n",
    "\n",
    "If TSTR ≈ TRTR, synthetic data quality is high\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T18:46:32.996301Z",
     "iopub.status.busy": "2025-11-02T18:46:32.996222Z",
     "iopub.status.idle": "2025-11-02T18:46:33.209791Z",
     "shell.execute_reply": "2025-11-02T18:46:33.209571Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nEvaluating Correlation Sampling Method:\n",
      "\\n============================================================\n",
      "TSTR/TRTR Evaluation: Correlation\n",
      "============================================================\n",
      "\\n1. TRTR (Train on Real, Test on Real):\n",
      "   Accuracy: 0.6966\n",
      "\\n2. TSTR (Train on Synthetic, Test on Real):\n",
      "   Accuracy: 0.3561\n",
      "\\n3. Performance Comparison:\n",
      "   TRTR: 0.6966\n",
      "   TSTR: 0.3561\n",
      "   Difference: 0.3405\n",
      "   ✗ Synthetic data quality: NEEDS IMPROVEMENT\n",
      "\\n============================================================\n",
      "Evaluating Mixup Baseline:\n",
      "\\n============================================================\n",
      "TSTR/TRTR Evaluation: Mixup Baseline\n",
      "============================================================\n",
      "\\n1. TRTR (Train on Real, Test on Real):\n",
      "   Accuracy: 0.6966\n",
      "\\n2. TSTR (Train on Synthetic, Test on Real):\n",
      "   Accuracy: 0.4758\n",
      "\\n3. Performance Comparison:\n",
      "   TRTR: 0.6966\n",
      "   TSTR: 0.4758\n",
      "   Difference: 0.2208\n",
      "   ✗ Synthetic data quality: NEEDS IMPROVEMENT\n",
      "\\n============================================================\n",
      "Evaluating WGAN-GP Method:\n",
      "\\n============================================================\n",
      "TSTR/TRTR Evaluation: WGAN-GP\n",
      "============================================================\n",
      "\\n1. TRTR (Train on Real, Test on Real):\n",
      "   Accuracy: 0.6966\n",
      "\\n2. TSTR (Train on Synthetic, Test on Real):\n",
      "   Accuracy: 0.5370\n",
      "\\n3. Performance Comparison:\n",
      "   TRTR: 0.6966\n",
      "   TSTR: 0.5370\n",
      "   Difference: 0.1595\n",
      "   ✗ Synthetic data quality: NEEDS IMPROVEMENT\n"
     ]
    }
   ],
   "source": [
    "def evaluate_tstr_trtr(real_features, real_labels, synthetic_features, method_name=\"\"):\n",
    "    \"\"\"\n",
    "    TSTR/TRTR Evaluation from literature\n",
    "    \n",
    "    Validates synthetic data by comparing model performance when:\n",
    "    - Training on real vs synthetic data\n",
    "    - Testing on real data\n",
    "    \"\"\"\n",
    "    print(f\"\\\\n{'='*60}\")\n",
    "    print(f\"TSTR/TRTR Evaluation: {method_name}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Split real data\n",
    "    X_train_real, X_test_real, y_train_real, y_test_real = train_test_split(\n",
    "        real_features, real_labels, test_size=0.3, random_state=RANDOM_SEED\n",
    "    )\n",
    "    \n",
    "    # Create synthetic labels matching real distribution\n",
    "    n_alcoholic = np.sum(y_train_real == 1)\n",
    "    n_control = np.sum(y_train_real == 0)\n",
    "    y_synthetic = np.concatenate([\n",
    "        np.ones(min(n_alcoholic, len(synthetic_features)//2)),\n",
    "        np.zeros(min(n_control, len(synthetic_features)//2))\n",
    "    ])\n",
    "    X_synthetic = synthetic_features[:len(y_synthetic)]\n",
    "    \n",
    "    # TRTR: Train on Real, Test on Real\n",
    "    print(\"\\\\n1. TRTR (Train on Real, Test on Real):\")\n",
    "    clf_trtr = RandomForestClassifier(n_estimators=100, random_state=RANDOM_SEED)\n",
    "    clf_trtr.fit(X_train_real, y_train_real)\n",
    "    y_pred_trtr = clf_trtr.predict(X_test_real)\n",
    "    acc_trtr = accuracy_score(y_test_real, y_pred_trtr)\n",
    "    print(f\"   Accuracy: {acc_trtr:.4f}\")\n",
    "    \n",
    "    # TSTR: Train on Synthetic, Test on Real\n",
    "    print(\"\\\\n2. TSTR (Train on Synthetic, Test on Real):\")\n",
    "    clf_tstr = RandomForestClassifier(n_estimators=100, random_state=RANDOM_SEED)\n",
    "    clf_tstr.fit(X_synthetic, y_synthetic)\n",
    "    y_pred_tstr = clf_tstr.predict(X_test_real)\n",
    "    acc_tstr = accuracy_score(y_test_real, y_pred_tstr)\n",
    "    print(f\"   Accuracy: {acc_tstr:.4f}\")\n",
    "    \n",
    "    # Compare\n",
    "    print(f\"\\\\n3. Performance Comparison:\")\n",
    "    print(f\"   TRTR: {acc_trtr:.4f}\")\n",
    "    print(f\"   TSTR: {acc_tstr:.4f}\")\n",
    "    print(f\"   Difference: {abs(acc_trtr - acc_tstr):.4f}\")\n",
    "    \n",
    "    if abs(acc_trtr - acc_tstr) < 0.05:\n",
    "        print(\"   ✓ Synthetic data quality: EXCELLENT\")\n",
    "    elif abs(acc_trtr - acc_tstr) < 0.10:\n",
    "        print(\"   ✓ Synthetic data quality: GOOD\")\n",
    "    else:\n",
    "        print(\"   ✗ Synthetic data quality: NEEDS IMPROVEMENT\")\n",
    "    \n",
    "    return acc_trtr, acc_tstr\n",
    "\n",
    "# Evaluate both methods\n",
    "print(\"\\\\nEvaluating Correlation Sampling Method:\")\n",
    "acc_trtr_corr, acc_tstr_corr = evaluate_tstr_trtr(\n",
    "    real_features, labels, synthetic_features_corr, \"Correlation\"\n",
    ")\n",
    "\n",
    "print(\"\\\\n\" + \"=\"*60)\n",
    "print(\"Evaluating Mixup Baseline:\")\n",
    "acc_trtr_mix, acc_tstr_mix = evaluate_tstr_trtr(\n",
    "    real_features, labels, synthetic_features_mixup, \"Mixup Baseline\"\n",
    ")\n",
    "\n",
    "if synthetic_features_wgangp is not None:\n",
    "    print(\"\\\\n\" + \"=\"*60)\n",
    "    print(\"Evaluating WGAN-GP Method:\")\n",
    "    acc_trtr_wgan, acc_tstr_wgan = evaluate_tstr_trtr(\n",
    "        real_features, labels, synthetic_features_wgangp, \"WGAN-GP\"\n",
    "    )\n",
    "else:\n",
    "    print(\"\\\\nSkipping WGAN-GP TSTR/TRTR (PyTorch unavailable).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Evaluation: Real vs Synthetic Classification\n",
    "\n",
    "Train classifier to distinguish real from synthetic.  \n",
    "**Goal**: Classifier should perform at ~50% (chance level) if synthetic data is indistinguishable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T18:46:33.211115Z",
     "iopub.status.busy": "2025-11-02T18:46:33.211035Z",
     "iopub.status.idle": "2025-11-02T18:46:33.354935Z",
     "shell.execute_reply": "2025-11-02T18:46:33.354667Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Real vs Synthetic Classification: Correlation Sampling\n",
      "============================================================\n",
      "\n",
      "Classifier Accuracy: 0.9900\n",
      "✗ POOR: Classifier easily distinguishes real from synthetic\n",
      "\n",
      "Confusion Matrix:\n",
      "                 Pred Real  Pred Synthetic\n",
      "Actual Real:         700            2\n",
      "Actual Synthetic:     12          690\n",
      "\n",
      "============================================================\n",
      "Real vs Synthetic Classification: Mixup Baseline\n",
      "============================================================\n",
      "\n",
      "Classifier Accuracy: 0.9338\n",
      "✗ POOR: Classifier easily distinguishes real from synthetic\n",
      "\n",
      "Confusion Matrix:\n",
      "                 Pred Real  Pred Synthetic\n",
      "Actual Real:         673           29\n",
      "Actual Synthetic:     64          638\n",
      "\n",
      "============================================================\n",
      "Real vs Synthetic Classification: WGAN-GP\n",
      "============================================================\n",
      "\n",
      "Classifier Accuracy: 0.9217\n",
      "✗ POOR: Classifier easily distinguishes real from synthetic\n",
      "\n",
      "Confusion Matrix:\n",
      "                 Pred Real  Pred Synthetic\n",
      "Actual Real:         642           60\n",
      "Actual Synthetic:     50          652\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "def evaluate_real_vs_synthetic(real_features, synthetic_features, method_name=\"\"):\n",
    "    \"\"\"Train a RF to distinguish real (1) vs synthetic (0); print metrics.\"\"\"\n",
    "    if real_features.size == 0 or synthetic_features.size == 0:\n",
    "        raise ValueError(\"Both real and synthetic feature matrices must be non-empty.\")\n",
    "\n",
    "    import numpy as np\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"Real vs Synthetic Classification: {method_name}\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    X = np.vstack([real_features, synthetic_features])\n",
    "    y = np.concatenate([np.ones(len(real_features), dtype=int),\n",
    "                        np.zeros(len(synthetic_features), dtype=int)])\n",
    "\n",
    "    Xtr, Xte, ytr, yte = train_test_split(\n",
    "        X, y, test_size=0.30, random_state=RANDOM_SEED, stratify=y\n",
    "    )\n",
    "\n",
    "    clf = RandomForestClassifier(n_estimators=100, random_state=RANDOM_SEED, class_weight=\"balanced\")\n",
    "    clf.fit(Xtr, ytr)\n",
    "    yhat = clf.predict(Xte)\n",
    "\n",
    "    acc = accuracy_score(yte, yhat)\n",
    "    print(f\"\\nClassifier Accuracy: {acc:.4f}\")\n",
    "    if 0.45 <= acc <= 0.55:\n",
    "        print(\"✓ EXCELLENT: Classifier at chance level (50%)\")\n",
    "        print(\"  → Synthetic data indistinguishable from real\")\n",
    "    elif 0.40 <= acc <= 0.60:\n",
    "        print(\"✓ GOOD: Classifier struggles to distinguish\")\n",
    "    else:\n",
    "        print(\"✗ POOR: Classifier easily distinguishes real from synthetic\")\n",
    "\n",
    "    # fix the printed layout by pinning label order\n",
    "    cm = confusion_matrix(yte, yhat, labels=[1,0])  # rows: Actual Real, Actual Synthetic\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(\"                 Pred Real  Pred Synthetic\")\n",
    "    print(f\"Actual Real:        {cm[0,0]:4d}         {cm[0,1]:4d}\")\n",
    "    print(f\"Actual Synthetic:   {cm[1,0]:4d}         {cm[1,1]:4d}\")\n",
    "    return acc\n",
    "\n",
    "acc_corr = evaluate_real_vs_synthetic(\n",
    "    real_features, synthetic_features_corr, \"Correlation Sampling\"\n",
    ")\n",
    "\n",
    "acc_mix = evaluate_real_vs_synthetic(\n",
    "    real_features, synthetic_features_mixup, \"Mixup Baseline\"\n",
    ")\n",
    "\n",
    "acc_wgan = None\n",
    "if synthetic_features_wgangp is not None:\n",
    "    acc_wgan = evaluate_real_vs_synthetic(\n",
    "        real_features, synthetic_features_wgangp, \"WGAN-GP\"\n",
    "    )\n",
    "else:\n",
    "    print(\"Skipping WGAN-GP detectability test (PyTorch unavailable).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Clustering Alignment\n",
    "\n",
    "Assess how mixed real/synthetic embeddings cluster and whether synthetic data preserves real cluster structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------\n",
      "Clustering alignment summary: Correlation Sampling\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>k</th>\n",
       "      <th>NMI(real_vs_flag)</th>\n",
       "      <th>ARI(real_vs_flag)</th>\n",
       "      <th>Silhouette(mixed)</th>\n",
       "      <th>CentroidGap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0.313942</td>\n",
       "      <td>0.210453</td>\n",
       "      <td>0.717331</td>\n",
       "      <td>1425.609194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>0.395901</td>\n",
       "      <td>0.325263</td>\n",
       "      <td>0.686652</td>\n",
       "      <td>1175.566667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>0.440748</td>\n",
       "      <td>0.394843</td>\n",
       "      <td>0.664459</td>\n",
       "      <td>1025.188406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>0.454661</td>\n",
       "      <td>0.421048</td>\n",
       "      <td>0.651153</td>\n",
       "      <td>963.272889</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   k  NMI(real_vs_flag)  ARI(real_vs_flag)  Silhouette(mixed)  CentroidGap\n",
       "0  3           0.313942           0.210453           0.717331  1425.609194\n",
       "1  4           0.395901           0.325263           0.686652  1175.566667\n",
       "2  5           0.440748           0.394843           0.664459  1025.188406\n",
       "3  6           0.454661           0.421048           0.651153   963.272889"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>k</th>\n",
       "      <th>NMI(real_vs_flag)</th>\n",
       "      <th>ARI(real_vs_flag)</th>\n",
       "      <th>Silhouette(mixed)</th>\n",
       "      <th>CentroidGap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.5</td>\n",
       "      <td>0.401313</td>\n",
       "      <td>0.337902</td>\n",
       "      <td>0.679899</td>\n",
       "      <td>1147.409289</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        k  NMI(real_vs_flag)  ARI(real_vs_flag)  Silhouette(mixed)  \\\n",
       "mean  4.5           0.401313           0.337902           0.679899   \n",
       "\n",
       "      CentroidGap  \n",
       "mean  1147.409289  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------\n",
      "Clustering alignment summary: Mixup Baseline\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>k</th>\n",
       "      <th>NMI(real_vs_flag)</th>\n",
       "      <th>ARI(real_vs_flag)</th>\n",
       "      <th>Silhouette(mixed)</th>\n",
       "      <th>CentroidGap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0.036040</td>\n",
       "      <td>0.000640</td>\n",
       "      <td>0.970365</td>\n",
       "      <td>1227.875526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>0.035590</td>\n",
       "      <td>0.000423</td>\n",
       "      <td>0.963242</td>\n",
       "      <td>1399.351413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>0.029948</td>\n",
       "      <td>0.000362</td>\n",
       "      <td>0.716643</td>\n",
       "      <td>1133.022914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>0.029766</td>\n",
       "      <td>0.000290</td>\n",
       "      <td>0.716485</td>\n",
       "      <td>956.647940</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   k  NMI(real_vs_flag)  ARI(real_vs_flag)  Silhouette(mixed)  CentroidGap\n",
       "0  3           0.036040           0.000640           0.970365  1227.875526\n",
       "1  4           0.035590           0.000423           0.963242  1399.351413\n",
       "2  5           0.029948           0.000362           0.716643  1133.022914\n",
       "3  6           0.029766           0.000290           0.716485   956.647940"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>k</th>\n",
       "      <th>NMI(real_vs_flag)</th>\n",
       "      <th>ARI(real_vs_flag)</th>\n",
       "      <th>Silhouette(mixed)</th>\n",
       "      <th>CentroidGap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.5</td>\n",
       "      <td>0.032836</td>\n",
       "      <td>0.000429</td>\n",
       "      <td>0.841684</td>\n",
       "      <td>1179.224448</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        k  NMI(real_vs_flag)  ARI(real_vs_flag)  Silhouette(mixed)  \\\n",
       "mean  4.5           0.032836           0.000429           0.841684   \n",
       "\n",
       "      CentroidGap  \n",
       "mean  1179.224448  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------\n",
      "Clustering alignment summary: WGAN-GP\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>k</th>\n",
       "      <th>NMI(real_vs_flag)</th>\n",
       "      <th>ARI(real_vs_flag)</th>\n",
       "      <th>Silhouette(mixed)</th>\n",
       "      <th>CentroidGap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0.009256</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.777243</td>\n",
       "      <td>1549.385465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>0.016632</td>\n",
       "      <td>0.000435</td>\n",
       "      <td>0.730047</td>\n",
       "      <td>1185.996111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>0.013099</td>\n",
       "      <td>0.001305</td>\n",
       "      <td>0.616721</td>\n",
       "      <td>972.647552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>0.016196</td>\n",
       "      <td>0.005737</td>\n",
       "      <td>0.496769</td>\n",
       "      <td>838.382753</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   k  NMI(real_vs_flag)  ARI(real_vs_flag)  Silhouette(mixed)  CentroidGap\n",
       "0  3           0.009256           0.000081           0.777243  1549.385465\n",
       "1  4           0.016632           0.000435           0.730047  1185.996111\n",
       "2  5           0.013099           0.001305           0.616721   972.647552\n",
       "3  6           0.016196           0.005737           0.496769   838.382753"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>k</th>\n",
       "      <th>NMI(real_vs_flag)</th>\n",
       "      <th>ARI(real_vs_flag)</th>\n",
       "      <th>Silhouette(mixed)</th>\n",
       "      <th>CentroidGap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.5</td>\n",
       "      <td>0.013796</td>\n",
       "      <td>0.001889</td>\n",
       "      <td>0.655195</td>\n",
       "      <td>1136.60297</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        k  NMI(real_vs_flag)  ARI(real_vs_flag)  Silhouette(mixed)  \\\n",
       "mean  4.5           0.013796           0.001889           0.655195   \n",
       "\n",
       "      CentroidGap  \n",
       "mean   1136.60297  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def clustering_alignment_report(real_features, synthetic_features, method_name, cluster_list=(3, 4, 5, 6)):\n",
    "    combined = np.vstack([real_features, synthetic_features])\n",
    "    is_synth = np.concatenate([\n",
    "        np.zeros(len(real_features), dtype=int),\n",
    "        np.ones(len(synthetic_features), dtype=int),\n",
    "    ])\n",
    "\n",
    "    records = []\n",
    "    for k in cluster_list:\n",
    "        km = KMeans(n_clusters=k, n_init=10, random_state=RANDOM_SEED)\n",
    "        cluster_labels = km.fit_predict(combined)\n",
    "\n",
    "        nmi = normalized_mutual_info_score(is_synth, cluster_labels)\n",
    "        ari = adjusted_rand_score(is_synth, cluster_labels)\n",
    "        sil = silhouette_score(combined, cluster_labels)\n",
    "\n",
    "        km_real = KMeans(n_clusters=k, n_init=10, random_state=RANDOM_SEED)\n",
    "        km_synth = KMeans(n_clusters=k, n_init=10, random_state=RANDOM_SEED)\n",
    "        km_real.fit(real_features)\n",
    "        km_synth.fit(synthetic_features)\n",
    "        centroid_cost = cdist(km_real.cluster_centers_, km_synth.cluster_centers_)\n",
    "        ri, ci = linear_sum_assignment(centroid_cost)\n",
    "        centroid_gap = centroid_cost[ri, ci].mean()\n",
    "\n",
    "        records.append(\n",
    "            {\n",
    "                'k': k,\n",
    "                'NMI(real_vs_flag)': nmi,\n",
    "                'ARI(real_vs_flag)': ari,\n",
    "                'Silhouette(mixed)': sil,\n",
    "                'CentroidGap': centroid_gap,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    df = pd.DataFrame(records)\n",
    "    print(f\"\\n{'-'*70}\\nClustering alignment summary: {method_name}\\n{'-'*70}\")\n",
    "    display(df)\n",
    "    display(df.mean().to_frame(name='mean').T)\n",
    "    return df\n",
    "\n",
    "clustering_reports = {}\n",
    "clustering_reports['Correlation Sampling'] = clustering_alignment_report(\n",
    "    real_features,\n",
    "    synthetic_features_corr,\n",
    "    'Correlation Sampling'\n",
    ")\n",
    "clustering_reports['Mixup Baseline'] = clustering_alignment_report(\n",
    "    real_features,\n",
    "    synthetic_features_mixup,\n",
    "    'Mixup Baseline'\n",
    ")\n",
    "if synthetic_features_wgangp is not None:\n",
    "    clustering_reports['WGAN-GP'] = clustering_alignment_report(\n",
    "        real_features,\n",
    "        synthetic_features_wgangp,\n",
    "        'WGAN-GP'\n",
    "    )\n",
    "else:\n",
    "    print(\"Skipping WGAN-GP clustering analysis (PyTorch unavailable).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 PERMANOVA\n",
    "\n",
    "Permutation-based multivariate ANOVA to test for distributional differences between real and synthetic samples (pooled and per condition).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scikit-bio is not installed; skipping PERMANOVA for Correlation Sampling\n",
      "scikit-bio is not installed; skipping PERMANOVA for Mixup Baseline\n",
      "scikit-bio is not installed; skipping PERMANOVA for WGAN-GP\n"
     ]
    }
   ],
   "source": [
    "def permanova_test(real_features, synthetic_features, method_name, permutations=999):\n",
    "    try:\n",
    "        from skbio import DistanceMatrix\n",
    "        from skbio.stats.distance import permanova\n",
    "    except ImportError:\n",
    "        print(\"scikit-bio is not installed; skipping PERMANOVA for\", method_name)\n",
    "        return None\n",
    "\n",
    "    combined = np.vstack([real_features, synthetic_features])\n",
    "    ids = [f\"real_{i}\" for i in range(len(real_features))] + [\n",
    "        f\"synthetic_{i}\" for i in range(len(synthetic_features))\n",
    "    ]\n",
    "    grouping = pd.Series(\n",
    "        ['real'] * len(real_features) + ['synthetic'] * len(synthetic_features),\n",
    "        index=ids\n",
    "    )\n",
    "    distance_matrix = cdist(combined, combined, metric='euclidean')\n",
    "    dm = DistanceMatrix(distance_matrix, ids=ids)\n",
    "\n",
    "    result = permanova(dm, grouping, permutations=permutations)\n",
    "    print(f\"\\nPERMANOVA ({method_name})\")\n",
    "    print(result)\n",
    "    return result\n",
    "\n",
    "permanova_results = {}\n",
    "permanova_results['Correlation Sampling'] = permanova_test(\n",
    "    real_features,\n",
    "    synthetic_features_corr,\n",
    "    'Correlation Sampling'\n",
    ")\n",
    "permanova_results['Mixup Baseline'] = permanova_test(\n",
    "    real_features,\n",
    "    synthetic_features_mixup,\n",
    "    'Mixup Baseline'\n",
    ")\n",
    "if synthetic_features_wgangp is not None:\n",
    "    permanova_results['WGAN-GP'] = permanova_test(\n",
    "        real_features,\n",
    "        synthetic_features_wgangp,\n",
    "        'WGAN-GP'\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Improved Synthetic Generation Strategies\n",
    "\n",
    "To reduce real-vs-synthetic separability while preserving frequency-band correlations, we explore two additional generators:\n",
    "\n",
    "1. **Gaussian Copula Sampling**: preserves empirical marginals per class and matches correlation structure in a latent Gaussian space.\n",
    "2. **Class-Conditional Interpolation**: SMOTE-like synthesis operating in log-power space with adaptive neighborhood mixing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _allocate_samples_by_class(labels, n_total):\n",
    "    \"\"\"Allocate synthetic samples per class, preserving empirical ratios.\"\"\"\n",
    "    classes, counts = np.unique(labels, return_counts=True)\n",
    "    ratios = counts / counts.sum()\n",
    "    expected = ratios * n_total\n",
    "    allocated = np.floor(expected).astype(int)\n",
    "    remainder = n_total - allocated.sum()\n",
    "    if remainder > 0:\n",
    "        remainders = expected - allocated\n",
    "        order = np.argsort(remainders)[::-1]\n",
    "        for idx in order[:remainder]:\n",
    "            allocated[idx] += 1\n",
    "    return dict(zip(classes, allocated))\n",
    "\n",
    "def generate_gaussian_copula_eeg(real_features, labels, n_synthetic=100, random_seed=42):\n",
    "    \"\"\"\n",
    "    Gaussian copula sampling:\n",
    "    1. Fit class-conditional quantile transformers to map marginals to Gaussian space\n",
    "    2. Estimate regularised covariance (Ledoit-Wolf) in latent space\n",
    "    3. Sample multivariate normal per class and invert the transform\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(random_seed)\n",
    "    allocation = _allocate_samples_by_class(labels, n_synthetic)\n",
    "    synthetic_blocks = []\n",
    "\n",
    "    print(\"Generating Gaussian copula samples per class...\")\n",
    "    for cls, n_cls_samples in allocation.items():\n",
    "        class_features = real_features[labels == cls]\n",
    "        if len(class_features) == 0 or n_cls_samples == 0:\n",
    "            continue\n",
    "\n",
    "        n_quantiles = min(len(class_features), 1000)\n",
    "        transformer = QuantileTransformer(\n",
    "            n_quantiles=n_quantiles,\n",
    "            output_distribution='normal',\n",
    "            random_state=random_seed\n",
    "        )\n",
    "        latent = transformer.fit_transform(class_features)\n",
    "\n",
    "        cov_estimator = LedoitWolf().fit(latent)\n",
    "        latent_mean = cov_estimator.location_\n",
    "        latent_cov = cov_estimator.covariance_\n",
    "\n",
    "        latent_samples = rng.multivariate_normal(\n",
    "            latent_mean,\n",
    "            latent_cov,\n",
    "            size=n_cls_samples\n",
    "        )\n",
    "\n",
    "        samples = transformer.inverse_transform(latent_samples)\n",
    "        samples = np.clip(samples, a_min=0, a_max=None)\n",
    "        synthetic_blocks.append(samples)\n",
    "\n",
    "        print(f\"  Class {cls}: real={len(class_features)}, synthetic={n_cls_samples}\")\n",
    "\n",
    "    if not synthetic_blocks:\n",
    "        raise ValueError(\"No synthetic samples were generated. Check class labels.\")\n",
    "\n",
    "    synthetic_features = np.vstack(synthetic_blocks)\n",
    "    return synthetic_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Gaussian copula samples per class...\n",
      "  Class 0: real=1165, synthetic=1165\n",
      "  Class 1: real=1175, synthetic=1175\n",
      "\n",
      "Generated 2340 Gaussian copula samples\n",
      "Shape: (2340, 5)\n"
     ]
    }
   ],
   "source": [
    "synthetic_features_copula = generate_gaussian_copula_eeg(\n",
    "    real_features,\n",
    "    labels,\n",
    "    n_synthetic=n_synthetic_samples,\n",
    "    random_seed=RANDOM_SEED\n",
    ")\n",
    "\n",
    "print(f\"\\nGenerated {len(synthetic_features_copula)} Gaussian copula samples\")\n",
    "print(f\"Shape: {synthetic_features_copula.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_classwise_interpolation_eeg(\n",
    "    real_features,\n",
    "    labels,\n",
    "    n_synthetic=100,\n",
    "    random_seed=42,\n",
    "    k_neighbors=8,\n",
    "    noise_scale=0.02\n",
    "):\n",
    "    \"\"\"\n",
    "    Class-conditional interpolation inspired by SMOTE.\n",
    "    Operates in log-power space to better capture multiplicative structure.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(random_seed)\n",
    "    allocation = _allocate_samples_by_class(labels, n_synthetic)\n",
    "    synthetic_samples = []\n",
    "\n",
    "    log_features = np.log1p(real_features)\n",
    "\n",
    "    for cls, n_cls_samples in allocation.items():\n",
    "        class_mask = labels == cls\n",
    "        class_features_log = log_features[class_mask]\n",
    "        if len(class_features_log) == 0 or n_cls_samples == 0:\n",
    "            continue\n",
    "\n",
    "        n_neighbors_eff = min(k_neighbors, len(class_features_log) - 1)\n",
    "        if n_neighbors_eff <= 0:\n",
    "            # Not enough samples to interpolate, fallback to jittering existing ones\n",
    "            base_samples = np.repeat(class_features_log, repeats=max(1, n_cls_samples // max(1, len(class_features_log))), axis=0)\n",
    "            base_samples = base_samples[:n_cls_samples]\n",
    "            jitter = rng.normal(0, noise_scale, size=base_samples.shape)\n",
    "            augmented = base_samples + jitter\n",
    "            synthetic_samples.append(np.expm1(augmented))\n",
    "            continue\n",
    "\n",
    "        nbrs = NearestNeighbors(n_neighbors=n_neighbors_eff + 1)\n",
    "        nbrs.fit(class_features_log)\n",
    "        class_std = np.std(class_features_log, axis=0, ddof=1)\n",
    "        class_std[class_std == 0] = 1e-6\n",
    "\n",
    "        for _ in range(n_cls_samples):\n",
    "            idx = rng.integers(len(class_features_log))\n",
    "            neighbors = nbrs.kneighbors(class_features_log[idx].reshape(1, -1), return_distance=False)[0]\n",
    "            neighbors = neighbors[neighbors != idx]\n",
    "            if len(neighbors) == 0:\n",
    "                neighbor_idx = idx\n",
    "            else:\n",
    "                neighbor_idx = rng.choice(neighbors)\n",
    "\n",
    "            alpha = rng.uniform(0.2, 0.8)\n",
    "            interpolated = (\n",
    "                alpha * class_features_log[idx] +\n",
    "                (1 - alpha) * class_features_log[neighbor_idx]\n",
    "            )\n",
    "\n",
    "            noise = rng.normal(0, noise_scale, size=class_features_log.shape[1]) * class_std\n",
    "            synthetic_log = interpolated + noise\n",
    "            synthetic_samples.append(np.expm1(synthetic_log))\n",
    "\n",
    "    if not synthetic_samples:\n",
    "        raise ValueError(\"Interpolation generator did not create any samples.\")\n",
    "\n",
    "    synthetic_features = np.vstack(synthetic_samples)\n",
    "    synthetic_features = np.clip(synthetic_features, a_min=0, a_max=None)\n",
    "    return synthetic_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated 2340 interpolation-based samples\n",
      "Shape: (2340, 5)\n"
     ]
    }
   ],
   "source": [
    "synthetic_features_interp = generate_classwise_interpolation_eeg(\n",
    "    real_features,\n",
    "    labels,\n",
    "    n_synthetic=n_synthetic_samples,\n",
    "    random_seed=RANDOM_SEED,\n",
    "    k_neighbors=10, \n",
    "    noise_scale=0.015\n",
    ")\n",
    "\n",
    "print(f\"\\nGenerated {len(synthetic_features_interp)} interpolation-based samples\")\n",
    "print(f\"Shape: {synthetic_features_interp.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1 Distribution and Quality Checks\n",
    "\n",
    "Re-run the statistical and downstream evaluations for the new generators alongside previous baselines.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\n============================================================\n",
      "Distribution Comparison: Gaussian Copula\n",
      "============================================================\n",
      "\\nKolmogorov-Smirnov Test Results:\n",
      "(p-value > 0.05 suggests distributions are similar)\n",
      "  Delta   : KS=0.0192, p=0.7800 ✓ Similar\n",
      "  Theta   : KS=0.0184, p=0.8245 ✓ Similar\n",
      "  Alpha   : KS=0.0184, p=0.8245 ✓ Similar\n",
      "  Beta    : KS=0.0192, p=0.7800 ✓ Similar\n",
      "  Gamma   : KS=0.0167, p=0.9013 ✓ Similar\n",
      "\n",
      "MMD (RBF) Score: 0.000734\n",
      "(Lower MMD indicates more similar distributions)\n",
      "\\n============================================================\n",
      "Distribution Comparison: Classwise Interpolation\n",
      "============================================================\n",
      "\\nKolmogorov-Smirnov Test Results:\n",
      "(p-value > 0.05 suggests distributions are similar)\n",
      "  Delta   : KS=0.0201, p=0.7328 ✓ Similar\n",
      "  Theta   : KS=0.0295, p=0.2609 ✓ Similar\n",
      "  Alpha   : KS=0.0278, p=0.3274 ✓ Similar\n",
      "  Beta    : KS=0.0235, p=0.5378 ✓ Similar\n",
      "  Gamma   : KS=0.0333, p=0.1485 ✓ Similar\n",
      "\n",
      "MMD (RBF) Score: 0.000288\n",
      "(Lower MMD indicates more similar distributions)\n"
     ]
    }
   ],
   "source": [
    "ks_copula, mmd_copula = evaluate_distributions(\n",
    "    real_features,\n",
    "    synthetic_features_copula,\n",
    "    \"Gaussian Copula\"\n",
    ")\n",
    "\n",
    "ks_interp, mmd_interp = evaluate_distributions(\n",
    "    real_features,\n",
    "    synthetic_features_interp,\n",
    "    \"Classwise Interpolation\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Gaussian Copula Method:\n",
      "\\n============================================================\n",
      "TSTR/TRTR Evaluation: Gaussian Copula\n",
      "============================================================\n",
      "\\n1. TRTR (Train on Real, Test on Real):\n",
      "   Accuracy: 0.6966\n",
      "\\n2. TSTR (Train on Synthetic, Test on Real):\n",
      "   Accuracy: 0.3490\n",
      "\\n3. Performance Comparison:\n",
      "   TRTR: 0.6966\n",
      "   TSTR: 0.3490\n",
      "   Difference: 0.3476\n",
      "   ✗ Synthetic data quality: NEEDS IMPROVEMENT\n",
      "\n",
      "============================================================\n",
      "Evaluating Classwise Interpolation Method:\n",
      "\\n============================================================\n",
      "TSTR/TRTR Evaluation: Classwise Interpolation\n",
      "============================================================\n",
      "\\n1. TRTR (Train on Real, Test on Real):\n",
      "   Accuracy: 0.6966\n",
      "\\n2. TSTR (Train on Synthetic, Test on Real):\n",
      "   Accuracy: 0.3134\n",
      "\\n3. Performance Comparison:\n",
      "   TRTR: 0.6966\n",
      "   TSTR: 0.3134\n",
      "   Difference: 0.3832\n",
      "   ✗ Synthetic data quality: NEEDS IMPROVEMENT\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nEvaluating Gaussian Copula Method:\")\n",
    "acc_trtr_copula, acc_tstr_copula = evaluate_tstr_trtr(\n",
    "    real_features,\n",
    "    labels,\n",
    "    synthetic_features_copula,\n",
    "    \"Gaussian Copula\"\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Evaluating Classwise Interpolation Method:\")\n",
    "acc_trtr_interp, acc_tstr_interp = evaluate_tstr_trtr(\n",
    "    real_features,\n",
    "    labels,\n",
    "    synthetic_features_interp,\n",
    "    \"Classwise Interpolation\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Real vs Synthetic Classification: Gaussian Copula\n",
      "============================================================\n",
      "\n",
      "Classifier Accuracy: 0.5491\n",
      "✓ EXCELLENT: Classifier at chance level (50%)\n",
      "  → Synthetic data indistinguishable from real\n",
      "\n",
      "Confusion Matrix:\n",
      "                 Pred Real  Pred Synthetic\n",
      "Actual Real:         374          328\n",
      "Actual Synthetic:    305          397\n",
      "\n",
      "============================================================\n",
      "Real vs Synthetic Classification: Classwise Interpolation\n",
      "============================================================\n",
      "\n",
      "Classifier Accuracy: 0.4309\n",
      "✓ GOOD: Classifier struggles to distinguish\n",
      "\n",
      "Confusion Matrix:\n",
      "                 Pred Real  Pred Synthetic\n",
      "Actual Real:         256          446\n",
      "Actual Synthetic:    353          349\n"
     ]
    }
   ],
   "source": [
    "acc_copula_sep = evaluate_real_vs_synthetic(\n",
    "    real_features,\n",
    "    synthetic_features_copula,\n",
    "    \"Gaussian Copula\"\n",
    ")\n",
    "\n",
    "acc_interp_sep = evaluate_real_vs_synthetic(\n",
    "    real_features,\n",
    "    synthetic_features_interp,\n",
    "    \"Classwise Interpolation\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Early-stop: targets met\n",
      "  Params: {0: 5, 1: 10} {0: 0.0, 1: 0.0}\n",
      "  Metrics: {'rvs_acc': 0.36186770428015563, 'tstr': 0.8408796895213454, 'trtr': 0.6998706338939198, 'gap': 0.1410090556274256, 'ks_min_p': 0.3261561157352818, 'mmd': 0.0004134147321114279, 'corr_sim': 0.995270237210689, 'rho_min': -0.03868685042240337, 'rho_mean': -0.002450164703319186}\n",
      "\n",
      "=== BEST COMBINATION ===\n",
      "k_params: {0: 5, 1: 5} noise_params: {0: 0.0, 1: 0.0}\n",
      "metrics : {'rvs_acc': 0.33787289234760054, 'tstr': 0.8745148771021992, 'trtr': 0.6998706338939198, 'gap': 0.17464424320827943, 'ks_min_p': 0.4868306191329178, 'mmd': 0.000296001036291349, 'corr_sim': 0.9921030346539852, 'rho_min': -0.03970450882614366, 'rho_mean': -0.002404962460893624}\n"
     ]
    }
   ],
   "source": [
    "# === Synthetic EEG Tuner: per-class (k, noise) grid + early-stopping ===\n",
    "# Paste this in one cell. Assumes you have:\n",
    "#   real_features:  (N, 5) band-power features (Delta..Gamma)\n",
    "#   labels:         (N,)   class labels {0,1}\n",
    "# Modify BAND_NAMES if needed.\n",
    "\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.covariance import LedoitWolf\n",
    "\n",
    "# -----------------------------\n",
    "# Config / Targets\n",
    "# -----------------------------\n",
    "BAND_NAMES = [\"Delta\",\"Theta\",\"Alpha\",\"Beta\",\"Gamma\"]\n",
    "CLASS_VALUES = np.unique(labels)\n",
    "assert set(CLASS_VALUES) == {0,1}, \"This tuner assumes binary classes {0,1}.\"\n",
    "\n",
    "PARAM_GRID_K = [5, 8, 10, 12, 15]\n",
    "PARAM_GRID_NOISE = [0.00, 0.01, 0.015, 0.02, 0.03]\n",
    "\n",
    "# Targets (edit to taste)\n",
    "TARGET_RVS_MAX = 0.60     # real vs synthetic accuracy <= 0.60\n",
    "TARGET_TSTR_MIN = 0.70    # TSTR >= 0.70\n",
    "TARGET_GAP_MAX  = 0.15    # |TRTR - TSTR| <= 0.15\n",
    "TARGET_KS_MINP  = 0.05    # per-band KS p-value > 0.05\n",
    "TARGET_CORR_SIM = 0.90    # corr-matrix similarity >= 0.90\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# -----------------------------\n",
    "# Helpers: metrics\n",
    "# -----------------------------\n",
    "def ks_pvals_per_band(real, synth):\n",
    "    pvals = []\n",
    "    for j in range(real.shape[1]):\n",
    "        _, p = stats.ks_2samp(real[:, j], synth[:, j])\n",
    "        pvals.append(p)\n",
    "    return np.array(pvals)\n",
    "\n",
    "def rbf_mmd(X, Y, gamma=None):\n",
    "    \"\"\"RBF-kernel MMD (biased) with median heuristic for gamma.\"\"\"\n",
    "    Z = np.vstack([X, Y])\n",
    "    # median heuristic\n",
    "    if gamma is None:\n",
    "        D2 = np.sum((Z[:,None,:]-Z[None,:,:])**2, axis=2)\n",
    "        med2 = np.median(D2[D2>0])\n",
    "        gamma = 1.0 / (med2 + 1e-8)\n",
    "\n",
    "    def k(a,b):\n",
    "        D2 = np.sum((a[:,None,:]-b[None,:,:])**2, axis=2)\n",
    "        return np.exp(-gamma * D2)\n",
    "\n",
    "    m, n = X.shape[0], Y.shape[0]\n",
    "    Kxx = k(X,X); Kyy = k(Y,Y); Kxy = k(X,Y)\n",
    "    return float(Kxx.mean() + Kyy.mean() - 2.0*Kxy.mean())\n",
    "\n",
    "\n",
    "def corr_matrix_similarity(real, synth):\n",
    "    C_real = np.corrcoef(real.T)\n",
    "    C_synth = np.corrcoef(synth.T)\n",
    "    iu = np.triu_indices_from(C_real, k=1)\n",
    "    r = np.corrcoef(C_real[iu], C_synth[iu])[0,1]\n",
    "    return r\n",
    "\n",
    "def per_band_spearman(real, synth, random_seed=RANDOM_SEED, max_samples=5000):\n",
    "    if real.size == 0 or synth.size == 0:\n",
    "        return np.full(real.shape[1], np.nan)\n",
    "\n",
    "    rng = np.random.default_rng(random_seed)\n",
    "    n = min(len(real), len(synth), max_samples)\n",
    "\n",
    "    if len(real) > n:\n",
    "        idx_real = rng.choice(len(real), size=n, replace=False)\n",
    "    else:\n",
    "        idx_real = np.arange(len(real))\n",
    "\n",
    "    if len(synth) > n:\n",
    "        idx_synth = rng.choice(len(synth), size=n, replace=False)\n",
    "    else:\n",
    "        idx_synth = np.arange(len(synth))\n",
    "\n",
    "    real_sample = real[idx_real]\n",
    "    synth_sample = synth[idx_synth]\n",
    "\n",
    "    vals = []\n",
    "    for j in range(real.shape[1]):\n",
    "        rho = stats.spearmanr(real_sample[:, j], synth_sample[:, j]).correlation\n",
    "        vals.append(rho)\n",
    "    return np.array(vals)\n",
    "\n",
    "def real_vs_synth_accuracy(real, synth):\n",
    "    X = np.vstack([real, synth])\n",
    "    y = np.hstack([np.zeros(len(real), dtype=int), np.ones(len(synth), dtype=int)])\n",
    "    Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.33, random_state=RANDOM_SEED, stratify=y)\n",
    "    clf = RandomForestClassifier(n_estimators=200, random_state=RANDOM_SEED, class_weight=\"balanced\")\n",
    "    clf.fit(Xtr, ytr)\n",
    "    return clf.score(Xte, yte)\n",
    "\n",
    "def tstr_trtr_accuracy(real_X, real_y, synth_X, synth_y):\n",
    "    # Classifier: RF; TSTR = train on synthetic, test on real; TRTR = train on real, test on real\n",
    "    # Split real set for fair TRTR eval\n",
    "    Xtr_r, Xte_r, ytr_r, yte_r = train_test_split(real_X, real_y, test_size=0.33, random_state=RANDOM_SEED, stratify=real_y)\n",
    "    # TRTR\n",
    "    clf_r = RandomForestClassifier(n_estimators=300, random_state=RANDOM_SEED, class_weight=\"balanced\")\n",
    "    clf_r.fit(Xtr_r, ytr_r)\n",
    "    trtr = clf_r.score(Xte_r, yte_r)\n",
    "    # TSTR\n",
    "    clf_s = RandomForestClassifier(n_estimators=300, random_state=RANDOM_SEED, class_weight=\"balanced\")\n",
    "    clf_s.fit(synth_X, synth_y)\n",
    "    tstr = clf_s.score(Xte_r, yte_r)\n",
    "    return tstr, trtr\n",
    "\n",
    "# -----------------------------\n",
    "# Generator: classwise interpolation in log-space\n",
    "# (replace with your own generate_classwise_interpolation_eeg_oneclass if you have it)\n",
    "# -----------------------------\n",
    "def _interp_one_class(Xc, n_out, k_neighbors=10, noise_scale=0.015, random_state=42):\n",
    "    \"\"\"\n",
    "    Classwise interpolation in log-space with covariance-aware jitter.\n",
    "    Robust to small class sizes; ensures k>=2 and <= len(Xc)-1.\n",
    "    \"\"\"\n",
    "    rng = np.random.RandomState(random_state)\n",
    "    eps = 1e-8\n",
    "    Xc = np.asarray(Xc)\n",
    "    if Xc.ndim != 2 or Xc.shape[0] == 0:\n",
    "        raise ValueError(\"Xc must be (n_samples, n_features) with n_samples>0\")\n",
    "\n",
    "    # If the class is too small, just jitter existing points\n",
    "    if Xc.shape[0] == 1:\n",
    "        # log → jitter → exp\n",
    "        xlog = np.log(Xc + eps)\n",
    "        synth_log = np.repeat(xlog, n_out, axis=0)\n",
    "        # fallback covariance: identity\n",
    "        jitter = rng.normal(size=(n_out, Xc.shape[1]))\n",
    "        synth_log = synth_log + noise_scale * jitter\n",
    "        return np.exp(synth_log) - eps\n",
    "\n",
    "    # Work in log-space to keep positivity on exp back-transform\n",
    "    Xlog = np.log(Xc + eps)\n",
    "\n",
    "    # Choose a valid k (at least 2, at most n-1)\n",
    "    n_samp = Xlog.shape[0]\n",
    "    k = int(np.clip(k_neighbors, 2, max(2, n_samp - 1)))\n",
    "\n",
    "    # Fit neighbors in log-space\n",
    "    nbrs = NearestNeighbors(n_neighbors=k, metric='euclidean').fit(Xlog)\n",
    "\n",
    "    # Pick base points uniformly\n",
    "    base_idx = rng.randint(0, n_samp, size=n_out)\n",
    "    base = Xlog[base_idx]\n",
    "\n",
    "    # For each base, pick one neighbor randomly (excluding self is handled by k<=n-1)\n",
    "    neigh_idx = nbrs.kneighbors(base, return_distance=False)\n",
    "    pick = neigh_idx[np.arange(n_out), rng.randint(0, neigh_idx.shape[1], size=n_out)]\n",
    "    neigh = Xlog[pick]\n",
    "\n",
    "    # Random convex combination in [0,1]\n",
    "    alpha = rng.rand(n_out, 1)\n",
    "    synth_log = alpha * base + (1 - alpha) * neigh\n",
    "\n",
    "    # Covariance-aware jitter using Ledoit–Wolf (robust); fallback to diagonal if needed\n",
    "    if noise_scale and noise_scale > 0:\n",
    "        try:\n",
    "            lw = LedoitWolf().fit(Xlog)\n",
    "            S = lw.covariance_\n",
    "            # numeric guard: ensure SPD\n",
    "            # (LedoitWolf should be SPD; add tiny ridge just in case)\n",
    "            S = S + 1e-8 * np.eye(S.shape[0])\n",
    "            jitter = rng.multivariate_normal(mean=np.zeros(Xlog.shape[1]), cov=S, size=n_out)\n",
    "        except Exception:\n",
    "            jitter = rng.normal(size=(n_out, Xlog.shape[1]))\n",
    "        synth_log = synth_log + noise_scale * jitter\n",
    "\n",
    "    synth = np.exp(synth_log) - eps\n",
    "    return synth\n",
    "\n",
    "\n",
    "def generate_classwise_interpolation_both_classes(real_X, real_y, n_per_class, k_params, noise_params):\n",
    "    # k_params = {0: k0, 1: k1}; noise_params = {0: n0, 1: n1}\n",
    "    synth_list, synth_y = [], []\n",
    "    for c in CLASS_VALUES:\n",
    "        Xc = real_X[real_y == c]\n",
    "        synth_c = _interp_one_class(\n",
    "            Xc, n_per_class,\n",
    "            k_neighbors=k_params[c],\n",
    "            noise_scale=noise_params[c],\n",
    "            random_state=RANDOM_SEED + c\n",
    "        )\n",
    "        synth_list.append(synth_c)\n",
    "        synth_y.append(np.full(n_per_class, c, dtype=int))\n",
    "    return np.vstack(synth_list), np.hstack(synth_y)\n",
    "\n",
    "# -----------------------------\n",
    "# Scoring + Early stopping\n",
    "# -----------------------------\n",
    "def score_combo(metrics):\n",
    "    # Higher is better. Penalize violations smoothly.\n",
    "    rvs = metrics[\"rvs_acc\"]\n",
    "    tstr, trtr = metrics[\"tstr\"], metrics[\"trtr\"]\n",
    "    ks_min = metrics[\"ks_min_p\"]\n",
    "    corr_sim = metrics[\"corr_sim\"]\n",
    "\n",
    "    # Base score\n",
    "    s = 0.0\n",
    "    # push RvS down toward 0.55 (reward if <= 0.60, punish otherwise)\n",
    "    s += 2.0 * max(0.0, 0.60 - rvs)\n",
    "    # reward higher TSTR and smaller gap\n",
    "    s += 1.5 * tstr\n",
    "    s += 1.0 * max(0.0, 0.15 - abs(trtr - tstr))\n",
    "    # reward KS min p and correlation similarity\n",
    "    s += 1.0 * min(ks_min, 0.10) * 10.0   # cap effect; scale to ~[0..1]\n",
    "    s += 1.0 * max(0.0, corr_sim - 0.85)  # only reward above 0.85\n",
    "    # small reward for small |MMD|\n",
    "    s += 0.5 * max(0.0, 0.2 - abs(metrics[\"mmd\"]))  # closer to 0 is better\n",
    "\n",
    "    return s\n",
    "\n",
    "def meets_targets(metrics):\n",
    "    return (metrics[\"rvs_acc\"] <= TARGET_RVS_MAX and\n",
    "            metrics[\"tstr\"]     >= TARGET_TSTR_MIN and\n",
    "            abs(metrics[\"trtr\"] - metrics[\"tstr\"]) <= TARGET_GAP_MAX and\n",
    "            metrics[\"ks_min_p\"] >= TARGET_KS_MINP and\n",
    "            metrics[\"corr_sim\"] >= TARGET_CORR_SIM)\n",
    "\n",
    "# -----------------------------\n",
    "# Grid search (per class) but evaluated jointly\n",
    "# -----------------------------\n",
    "def tune_interpolation_params(real_X, real_y, verbose=True):\n",
    "    n_per_class = min(np.sum(real_y==0), np.sum(real_y==1))  # balance\n",
    "    best = {\"score\": -np.inf, \"params\": None, \"metrics\": None}\n",
    "\n",
    "    tried = 0\n",
    "    for k0 in PARAM_GRID_K:\n",
    "        for n0 in PARAM_GRID_NOISE:\n",
    "            for k1 in PARAM_GRID_K:\n",
    "                for n1 in PARAM_GRID_NOISE:\n",
    "                    tried += 1\n",
    "                    k_params = {0: k0, 1: k1}\n",
    "                    noise_params = {0: n0, 1: n1}\n",
    "                    synth_X, synth_y = generate_classwise_interpolation_both_classes(\n",
    "                        real_X, real_y, n_per_class, k_params, noise_params\n",
    "                    )\n",
    "\n",
    "                    # Metrics\n",
    "                    ks_p = ks_pvals_per_band(real_X, synth_X)\n",
    "                    mmd = rbf_mmd(real_X, synth_X, gamma=None)\n",
    "                    rvs = real_vs_synth_accuracy(real_X, synth_X)\n",
    "                    tstr, trtr = tstr_trtr_accuracy(real_X, real_y, synth_X, synth_y)\n",
    "                    corr_sim = corr_matrix_similarity(real_X, synth_X)\n",
    "                    rho = per_band_spearman(real_X, synth_X)\n",
    "\n",
    "                    metrics = {\n",
    "                        \"rvs_acc\": rvs,\n",
    "                        \"tstr\": tstr,\n",
    "                        \"trtr\": trtr,\n",
    "                        \"gap\": abs(trtr - tstr),\n",
    "                        \"ks_min_p\": float(np.min(ks_p)),\n",
    "                        \"mmd\": float(mmd),\n",
    "                        \"corr_sim\": float(corr_sim),\n",
    "                        \"rho_min\": float(np.nanmin(rho)),\n",
    "                        \"rho_mean\": float(np.nanmean(rho)),\n",
    "                    }\n",
    "                    sc = score_combo(metrics)\n",
    "\n",
    "                    if sc > best[\"score\"]:\n",
    "                        best = {\"score\": sc, \"params\": (k_params, noise_params), \"metrics\": metrics, \n",
    "                                \"synth\": (synth_X, synth_y)}\n",
    "\n",
    "                    if verbose and tried % 20 == 0:\n",
    "                        print(f\"[{tried:4d}] k0={k0}, n0={n0:.3f} | k1={k1}, n1={n1:.3f} \"\n",
    "                              f\"RvS={rvs:.3f} TSTR/TRTR={tstr:.3f}/{trtr:.3f} \"\n",
    "                              f\"KSmin={metrics['ks_min_p']:.3f} CorrSim={corr_sim:.3f} MMD={mmd:.3f}\")\n",
    "\n",
    "                    # Early stopping: break as soon as all targets met\n",
    "                    if meets_targets(metrics):\n",
    "                        if verbose:\n",
    "                            print(\"\\n✓ Early-stop: targets met\")\n",
    "                            print(\"  Params:\", k_params, noise_params)\n",
    "                            print(\"  Metrics:\", metrics)\n",
    "                        return {\"best\": best, \"early_stop\": True}\n",
    "\n",
    "    if verbose:\n",
    "        print(\"\\nNo combo met all targets. Returning best observed.\")\n",
    "        print(\"Best params:\", best[\"params\"])\n",
    "        print(\"Best metrics:\", best[\"metrics\"])\n",
    "    return {\"best\": best, \"early_stop\": False}\n",
    "\n",
    "# -----------------------------\n",
    "# Run tuning\n",
    "# -----------------------------\n",
    "result = tune_interpolation_params(real_features, labels, verbose=True)\n",
    "\n",
    "best_params = result[\"best\"][\"params\"]\n",
    "best_metrics = result[\"best\"][\"metrics\"]\n",
    "best_synth_X, best_synth_y = result[\"best\"][\"synth\"]\n",
    "\n",
    "print(\"\\n=== BEST COMBINATION ===\")\n",
    "print(\"k_params:\", best_params[0], \"noise_params:\", best_params[1])\n",
    "print(\"metrics :\", best_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Early-stop: targets met\n",
      "  Params: {0: 5, 1: 5} {0: 0.0, 1: 0.0}\n",
      "  Metrics: {'detect': 0.5027195027195027, 'rvs_acc_raw': 0.5027195027195027, 'rvs_auc': 0.38137230242969966, 'tstr': 0.7011642949547219, 'trtr': 0.6998706338939198, 'gap': 0.001293661060802087, 'ks_min_p': 0.3416900838456176, 'mmd': 0.0012224264816664832, 'corr_sim': 0.9820198663815284, 'rho_min': 0.8251903364370348, 'rho_mean': 0.9218774646917103}\n",
      "\n",
      "=== BEST COMBINATION ===\n",
      "k_params: {0: 5, 1: 5} noise_params: {0: 0.0, 1: 0.0}\n",
      "metrics : {'detect': 0.5027195027195027, 'rvs_acc_raw': 0.5027195027195027, 'rvs_auc': 0.38137230242969966, 'tstr': 0.7011642949547219, 'trtr': 0.6998706338939198, 'gap': 0.001293661060802087, 'ks_min_p': 0.3416900838456176, 'mmd': 0.0012224264816664832, 'corr_sim': 0.9820198663815284, 'rho_min': 0.8251903364370348, 'rho_mean': 0.9218774646917103}\n"
     ]
    }
   ],
   "source": [
    "# === DROP-IN: Robust tuner with detectability, fair TSTR, matched Spearman ===\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.covariance import LedoitWolf\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "BAND_NAMES = [\"Delta\",\"Theta\",\"Alpha\",\"Beta\",\"Gamma\"]\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# Targets\n",
    "TARGET_DETECT_MAX = 0.60   # detectability = max(acc, 1-acc) <= 0.60\n",
    "TARGET_TSTR_MIN   = 0.70\n",
    "TARGET_GAP_MAX    = 0.15\n",
    "TARGET_KS_MINP    = 0.05\n",
    "TARGET_CORR_SIM   = 0.90\n",
    "\n",
    "# Grids (adjust as needed)\n",
    "PARAM_GRID_K      = [5, 8, 10, 12]\n",
    "PARAM_GRID_NOISE  = [0.00, 0.01, 0.015, 0.02]\n",
    "\n",
    "# ---------- Metrics ----------\n",
    "def ks_pvals_per_band(real, synth):\n",
    "    return np.array([stats.ks_2samp(real[:, j], synth[:, j])[1] for j in range(real.shape[1])])\n",
    "\n",
    "def rbf_mmd(X, Y, gamma=None):\n",
    "    \"\"\"RBF-kernel MMD (biased) with median heuristic for gamma.\"\"\"\n",
    "    Z = np.vstack([X, Y])\n",
    "    # median heuristic\n",
    "    if gamma is None:\n",
    "        D2 = np.sum((Z[:,None,:]-Z[None,:,:])**2, axis=2)\n",
    "        med2 = np.median(D2[D2>0])\n",
    "        gamma = 1.0 / (med2 + 1e-8)\n",
    "\n",
    "    def k(a,b):\n",
    "        D2 = np.sum((a[:,None,:]-b[None,:,:])**2, axis=2)\n",
    "        return np.exp(-gamma * D2)\n",
    "\n",
    "    m, n = X.shape[0], Y.shape[0]\n",
    "    Kxx = k(X,X); Kyy = k(Y,Y); Kxy = k(X,Y)\n",
    "    return float(Kxx.mean() + Kyy.mean() - 2.0*Kxy.mean())\n",
    "\n",
    "\n",
    "def corr_matrix_similarity(real, synth):\n",
    "    C_r, C_s = np.corrcoef(real.T), np.corrcoef(synth.T)\n",
    "    iu = np.triu_indices_from(C_r, k=1)\n",
    "    return np.corrcoef(C_r[iu], C_s[iu])[0,1]\n",
    "\n",
    "def matched_spearman(real, synth, k=1):\n",
    "    nbrs = NearestNeighbors(n_neighbors=k, metric='euclidean').fit(synth)\n",
    "    idx = nbrs.kneighbors(real, return_distance=False)[:,0]\n",
    "    return np.array([stats.spearmanr(real[:, j], synth[idx, j]).correlation for j in range(real.shape[1])])\n",
    "\n",
    "def real_vs_synth_detectability(real, synth, seed=RANDOM_SEED):\n",
    "    X = np.vstack([real, synth])\n",
    "    y = np.hstack([np.zeros(len(real), dtype=int), np.ones(len(synth), dtype=int)])\n",
    "    Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.33, random_state=seed, stratify=y)\n",
    "    clf = RandomForestClassifier(n_estimators=200, random_state=seed, class_weight=\"balanced\")\n",
    "    clf.fit(Xtr, ytr)\n",
    "    acc = clf.score(Xte, yte)\n",
    "    try:\n",
    "        proba = clf.predict_proba(Xte)[:,1]\n",
    "        auc = roc_auc_score(yte, proba)\n",
    "    except Exception:\n",
    "        auc = 0.5\n",
    "    detect = max(acc, 1.0 - acc)\n",
    "    return detect, acc, auc\n",
    "\n",
    "# ---------- Generator: classwise interpolation in log-space ----------\n",
    "def _interp_one_class(Xc, n_out, k_neighbors=10, noise_scale=0.015, random_state=RANDOM_SEED):\n",
    "    rng = np.random.RandomState(random_state)\n",
    "    eps = 1e-8\n",
    "    Xc = np.asarray(Xc)\n",
    "    if Xc.shape[0] == 1:\n",
    "        xlog = np.log(Xc + eps)\n",
    "        synth_log = np.repeat(xlog, n_out, axis=0)\n",
    "        jitter = rng.normal(size=(n_out, Xc.shape[1]))\n",
    "        synth_log = synth_log + noise_scale * jitter\n",
    "        return np.exp(synth_log) - eps\n",
    "\n",
    "    Xlog = np.log(Xc + eps)\n",
    "    n_samp = Xlog.shape[0]\n",
    "    k = int(np.clip(k_neighbors, 2, max(2, n_samp - 1)))\n",
    "    nbrs = NearestNeighbors(n_neighbors=k, metric='euclidean').fit(Xlog)\n",
    "\n",
    "    base_idx = rng.randint(0, n_samp, size=n_out)\n",
    "    base = Xlog[base_idx]\n",
    "    neigh_idx = nbrs.kneighbors(base, return_distance=False)\n",
    "    pick = neigh_idx[np.arange(n_out), rng.randint(0, neigh_idx.shape[1], size=n_out)]\n",
    "    neigh = Xlog[pick]\n",
    "\n",
    "    alpha = rng.rand(n_out, 1)\n",
    "    synth_log = alpha * base + (1 - alpha) * neigh\n",
    "\n",
    "    if noise_scale and noise_scale > 0:\n",
    "        try:\n",
    "            lw = LedoitWolf().fit(Xlog)\n",
    "            S = lw.covariance_ + 1e-8*np.eye(Xlog.shape[1])\n",
    "            jitter = rng.multivariate_normal(mean=np.zeros(Xlog.shape[1]), cov=S, size=n_out)\n",
    "        except Exception:\n",
    "            jitter = rng.normal(size=(n_out, Xlog.shape[1]))\n",
    "        synth_log = synth_log + noise_scale * jitter\n",
    "\n",
    "    return np.exp(synth_log) - eps\n",
    "\n",
    "def gen_interp_per_class(Xtr_r, ytr_r, n_per_class, k_params, noise_params):\n",
    "    synth_list, synth_y = [], []\n",
    "    for c in np.unique(ytr_r):\n",
    "        Xc = Xtr_r[ytr_r == c]\n",
    "        k_c = int(np.clip(k_params[c], 2, max(2, Xc.shape[0]-1)))\n",
    "        n_c = float(noise_params[c])\n",
    "        synth_c = _interp_one_class(Xc, n_per_class, k_neighbors=k_c, noise_scale=n_c, random_state=RANDOM_SEED + c)\n",
    "        synth_list.append(synth_c)\n",
    "        synth_y.append(np.full(n_per_class, c, dtype=int))\n",
    "    return np.vstack(synth_list), np.hstack(synth_y)\n",
    "\n",
    "# ---------- Fair TSTR/TRTR (train-only generation) ----------\n",
    "def tstr_trtr_fair(real_X, real_y, k_params, noise_params, seed=RANDOM_SEED):\n",
    "    Xtr_r, Xte_r, ytr_r, yte_r = train_test_split(real_X, real_y, test_size=0.33, random_state=seed, stratify=real_y)\n",
    "    n_per_class = min(np.sum(ytr_r==0), np.sum(ytr_r==1))\n",
    "    synth_X, synth_y = gen_interp_per_class(Xtr_r, ytr_r, n_per_class, k_params, noise_params)\n",
    "\n",
    "    clf_r = RandomForestClassifier(n_estimators=300, random_state=seed, class_weight=\"balanced\")\n",
    "    clf_r.fit(Xtr_r, ytr_r)\n",
    "    trtr = clf_r.score(Xte_r, yte_r)\n",
    "\n",
    "    clf_s = RandomForestClassifier(n_estimators=300, random_state=seed, class_weight=\"balanced\")\n",
    "    clf_s.fit(synth_X, synth_y)\n",
    "    tstr = clf_s.score(Xte_r, yte_r)\n",
    "    return tstr, trtr, (Xtr_r, Xte_r, ytr_r, yte_r), (synth_X, synth_y)\n",
    "\n",
    "# ---------- Scoring & Early-stop ----------\n",
    "def score_combo(metrics):\n",
    "    s = 0.0\n",
    "    s += 2.0 * max(0.0, 0.60 - metrics[\"detect\"])      # lower detectability is better\n",
    "    s += 1.5 * metrics[\"tstr\"]                          # higher TSTR is better\n",
    "    s += 1.0 * max(0.0, 0.15 - abs(metrics[\"trtr\"] - metrics[\"tstr\"]))  # small gap\n",
    "    s += 1.0 * min(metrics[\"ks_min_p\"], 0.10) * 10.0    # KS min p (capped)\n",
    "    s += 1.0 * max(0.0, metrics[\"corr_sim\"] - 0.85)     # corr similarity above 0.85\n",
    "    s += 0.5 * max(0.0, 0.2 - abs(metrics[\"mmd\"]))      # MMD close to 0\n",
    "    return s\n",
    "\n",
    "def meets_targets(m):\n",
    "    return (m[\"detect\"] <= TARGET_DETECT_MAX and\n",
    "            m[\"tstr\"]   >= TARGET_TSTR_MIN and\n",
    "            abs(m[\"trtr\"] - m[\"tstr\"]) <= TARGET_GAP_MAX and\n",
    "            m[\"ks_min_p\"] >= TARGET_KS_MINP and\n",
    "            m[\"corr_sim\"] >= TARGET_CORR_SIM)\n",
    "\n",
    "# ---------- Grid search ----------\n",
    "def tune_interpolation_params(real_X, real_y, verbose=True):\n",
    "    classes = np.unique(real_y)\n",
    "    assert set(classes) == {0,1}, \"Binary classes {0,1} expected.\"\n",
    "    best = {\"score\": -np.inf, \"params\": None, \"metrics\": None, \"artifacts\": None}\n",
    "    tried = 0\n",
    "\n",
    "    for k0 in PARAM_GRID_K:\n",
    "        for n0 in PARAM_GRID_NOISE:\n",
    "            for k1 in PARAM_GRID_K:\n",
    "                for n1 in PARAM_GRID_NOISE:\n",
    "                    tried += 1\n",
    "                    k_params     = {0: k0, 1: k1}\n",
    "                    noise_params = {0: n0, 1: n1}\n",
    "\n",
    "                    # fair TSTR/TRTR with train-only generation\n",
    "                    tstr, trtr, (Xtr_r, Xte_r, ytr_r, yte_r), (synth_X, synth_y) = tstr_trtr_fair(\n",
    "                        real_X, real_y, k_params, noise_params, seed=RANDOM_SEED\n",
    "                    )\n",
    "\n",
    "                    # evaluate detectability on the same real (train+test) pool vs synth\n",
    "                    detect, acc_raw, auc = real_vs_synth_detectability(real_X, synth_X, seed=RANDOM_SEED)\n",
    "                    ks_p = ks_pvals_per_band(real_X, synth_X)\n",
    "                    mmd  = rbf_mmd(real_X, synth_X)\n",
    "                    corr = corr_matrix_similarity(real_X, synth_X)\n",
    "                    rho  = matched_spearman(real_X, synth_X, k=1)\n",
    "\n",
    "                    metrics = {\n",
    "                        \"detect\": float(detect),\n",
    "                        \"rvs_acc_raw\": float(acc_raw),\n",
    "                        \"rvs_auc\": float(auc),\n",
    "                        \"tstr\": float(tstr),\n",
    "                        \"trtr\": float(trtr),\n",
    "                        \"gap\": float(abs(trtr - tstr)),\n",
    "                        \"ks_min_p\": float(np.min(ks_p)),\n",
    "                        \"mmd\": float(mmd),\n",
    "                        \"corr_sim\": float(corr),\n",
    "                        \"rho_min\": float(np.nanmin(rho)),\n",
    "                        \"rho_mean\": float(np.nanmean(rho)),\n",
    "                    }\n",
    "                    sc = score_combo(metrics)\n",
    "\n",
    "                    if sc > best[\"score\"]:\n",
    "                        best = {\"score\": sc,\n",
    "                                \"params\": (k_params, noise_params),\n",
    "                                \"metrics\": metrics,\n",
    "                                \"artifacts\": {\"synth_X\": synth_X, \"synth_y\": synth_y}}\n",
    "\n",
    "                    if verbose and tried % 20 == 0:\n",
    "                        print(f\"[{tried:4d}] k0={k0}, n0={n0:.3f} | k1={k1}, n1={n1:.3f} \"\n",
    "                              f\"Detect={metrics['detect']:.3f} (acc={metrics['rvs_acc_raw']:.3f}, AUC={metrics['rvs_auc']:.3f}) \"\n",
    "                              f\"TSTR/TRTR={tstr:.3f}/{trtr:.3f} KSmin={metrics['ks_min_p']:.3f} \"\n",
    "                              f\"CorrSim={corr:.3f} MMD={mmd:.4f} ρ_mean={metrics['rho_mean']:.3f}\")\n",
    "\n",
    "                    if meets_targets(metrics):\n",
    "                        if verbose:\n",
    "                            print(\"\\n✓ Early-stop: targets met\")\n",
    "                            print(\"  Params:\", k_params, noise_params)\n",
    "                            print(\"  Metrics:\", metrics)\n",
    "                        return {\"best\": best, \"early_stop\": True}\n",
    "\n",
    "    if verbose:\n",
    "        print(\"\\nNo combo met all targets. Returning best observed.\")\n",
    "        print(\"Best params:\", best[\"params\"])\n",
    "        print(\"Best metrics:\", best[\"metrics\"])\n",
    "    return {\"best\": best, \"early_stop\": False}\n",
    "\n",
    "# Run it\n",
    "result = tune_interpolation_params(real_features, labels, verbose=True)\n",
    "best_params  = result[\"best\"][\"params\"]\n",
    "best_metrics = result[\"best\"][\"metrics\"]\n",
    "print(\"\\n=== BEST COMBINATION ===\")\n",
    "print(\"k_params:\", best_params[0], \"noise_params:\", best_params[1])\n",
    "print(\"metrics :\", best_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'k_params': {0: 5, 1: 5}, 'noise_params': {0: 0.0, 1: 0.0}, 'detectability': 0.5027195027195027, 'rvs_acc_raw': 0.5027195027195027, 'rvs_auc': 0.38137230242969966, 'tstr': 0.7011642949547219, 'trtr': 0.6998706338939198, 'gap': 0.001293661060802087, 'ks_min_p': 0.3416900838456176, 'mmd': 0.0012224264816664832, 'corr_sim': 0.9820198663815284, 'rho_mean': 0.9218774646917103, 'rho_min': 0.8251903364370348}\n"
     ]
    }
   ],
   "source": [
    "BEST_K = {0: 5, 1: 5}\n",
    "BEST_NOISE = {0: 0.0, 1: 0.0}\n",
    "SEED = 42\n",
    "\n",
    "# Regenerate balanced synthetic set using train-only split for fairness\n",
    "Xtr_r, Xte_r, ytr_r, yte_r = train_test_split(real_features, labels, test_size=0.33,\n",
    "                                              random_state=SEED, stratify=labels)\n",
    "n_per_class = min(np.sum(ytr_r==0), np.sum(ytr_r==1))\n",
    "synth_X, synth_y = gen_interp_per_class(Xtr_r, ytr_r, n_per_class,\n",
    "                                        k_params=BEST_K, noise_params=BEST_NOISE)\n",
    "\n",
    "# Recompute headline metrics (detectability/TSTR/TRTR/KS/MMD/corr/rho)\n",
    "detect, acc_raw, auc = real_vs_synth_detectability(real_features, synth_X, seed=SEED)\n",
    "tstr, trtr, *_ = tstr_trtr_fair(real_features, labels, BEST_K, BEST_NOISE, seed=SEED)\n",
    "ks_p = ks_pvals_per_band(real_features, synth_X)\n",
    "mmd  = rbf_mmd(real_features, synth_X)\n",
    "corr = corr_matrix_similarity(real_features, synth_X)\n",
    "rho  = matched_spearman(real_features, synth_X, k=1)\n",
    "\n",
    "summary = {\n",
    "    \"k_params\": BEST_K, \"noise_params\": BEST_NOISE,\n",
    "    \"detectability\": float(detect), \"rvs_acc_raw\": float(acc_raw), \"rvs_auc\": float(auc),\n",
    "    \"tstr\": float(tstr), \"trtr\": float(trtr), \"gap\": float(abs(trtr - tstr)),\n",
    "    \"ks_min_p\": float(np.min(ks_p)), \"mmd\": float(mmd),\n",
    "    \"corr_sim\": float(corr), \"rho_mean\": float(np.nanmean(rho)), \"rho_min\": float(np.nanmin(rho))\n",
    "}\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Override: enforce condition-specific training folds for TRTR when evaluating per-condition utility.\n",
    "def tstr_trtr_per_condition(X_train, y_train, c_train,\n",
    "                            X_test, y_test, c_test,\n",
    "                            X_synth, y_synth, c_synth,\n",
    "                            condition_name,\n",
    "                            clf_factory,\n",
    "                            min_samples=MIN_EVAL_SAMPLES):\n",
    "    if condition_name is None:\n",
    "        real_mask = np.ones(len(X_test), dtype=bool)\n",
    "        synth_mask = np.ones(len(X_synth), dtype=bool)\n",
    "        train_mask = np.ones(len(X_train), dtype=bool)\n",
    "    else:\n",
    "        real_mask = (c_test == condition_name)\n",
    "        synth_mask = (c_synth == condition_name)\n",
    "        train_mask = (c_train == condition_name)\n",
    "\n",
    "    n_real = int(real_mask.sum())\n",
    "    n_synth = int(synth_mask.sum())\n",
    "    n_train = int(train_mask.sum())\n",
    "    if n_real < min_samples or n_train < min_samples:\n",
    "        return None\n",
    "\n",
    "    clf_real = clf_factory()\n",
    "    clf_real.fit(maybe_concat_condition(X_train[train_mask], c_train[train_mask]), y_train[train_mask])\n",
    "    trtr = clf_real.score(\n",
    "        maybe_concat_condition(X_test[real_mask], c_test[real_mask]),\n",
    "        y_test[real_mask]\n",
    "    )\n",
    "\n",
    "    if n_synth < min_samples:\n",
    "        return {\n",
    "            'TRTR': float(trtr),\n",
    "            'TSTR': None,\n",
    "            'gap': None,\n",
    "            'n_test': n_real,\n",
    "            'n_synth': n_synth\n",
    "        }\n",
    "\n",
    "    clf_synth = clf_factory()\n",
    "    clf_synth.fit(\n",
    "        maybe_concat_condition(X_synth[synth_mask], c_synth[synth_mask]),\n",
    "        y_synth[synth_mask]\n",
    "    )\n",
    "    tstr = clf_synth.score(\n",
    "        maybe_concat_condition(X_test[real_mask], c_test[real_mask]),\n",
    "        y_test[real_mask]\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        'TRTR': float(trtr),\n",
    "        'TSTR': float(tstr),\n",
    "        'gap': float(abs(trtr - tstr)),\n",
    "        'n_test': n_real,\n",
    "        'n_synth': n_synth\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to ../output/synth_interp_best.pkl\n"
     ]
    }
   ],
   "source": [
    "# Save artifacts\n",
    "import pickle, os\n",
    "os.makedirs(\"../output\", exist_ok=True)\n",
    "with open(\"../output/synth_interp_best.pkl\", \"wb\") as f:\n",
    "    pickle.dump({\"synth_X\": synth_X, \"synth_y\": synth_y, \"summary\": summary}, f)\n",
    "print(\"Saved to ../output/synth_interp_best.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistical baselines already present (copula & interpolation).\n"
     ]
    }
   ],
   "source": [
    "def ensure_statistical_generators(verbose=True):\n",
    "    \"\"\"Ensure class-conditional statistical generators are available in globals.\"\"\"\n",
    "    global synthetic_features_copula, synthetic_features_interp\n",
    "\n",
    "    generated = []\n",
    "\n",
    "    if 'synthetic_features_copula' not in globals():\n",
    "        synthetic_features_copula = generate_gaussian_copula_eeg(\n",
    "            real_features,\n",
    "            labels,\n",
    "            n_synthetic=n_synthetic_samples,\n",
    "            random_seed=RANDOM_SEED\n",
    "        )\n",
    "        generated.append('Gaussian Copula')\n",
    "\n",
    "    if 'synthetic_features_interp' not in globals():\n",
    "        synthetic_features_interp = generate_classwise_interpolation_eeg(\n",
    "            real_features,\n",
    "            labels,\n",
    "            n_synthetic=n_synthetic_samples,\n",
    "            random_seed=RANDOM_SEED,\n",
    "            k_neighbors=10,\n",
    "            noise_scale=0.015\n",
    "        )\n",
    "        generated.append('Classwise Interpolation')\n",
    "\n",
    "    if verbose and generated:\n",
    "        print(f\"Generated statistical baselines: {', '.join(generated)}\")\n",
    "    elif verbose and not generated:\n",
    "        print(\"Statistical baselines already present (copula & interpolation).\")\n",
    "\n",
    "ensure_statistical_generators()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensure_statistical_generators(verbose=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef97d19a",
   "metadata": {},
   "source": [
    "## 10. Condition-aware train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensure_statistical_generators(verbose=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "47b19e25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stratified split using class+condition\n",
      "Train size: 1567, Test size: 773\n",
      "Train buckets: Counter({(1, 'S2_match'): 268, (0, 'S1'): 268, (1, 'S1'): 268, (0, 'S2_match'): 264, (1, 'S2_nomatch'): 251, (0, 'S2_nomatch'): 248})\n"
     ]
    }
   ],
   "source": [
    "def ensure_filtered_arrays():\n",
    "    required = ['real_features', 'labels', 'cond_tokens', 'group_tokens']\n",
    "    missing = [name for name in required if name not in globals()]\n",
    "    if not missing:\n",
    "        return\n",
    "    cache = globals().get('FILTERED_DATASET')\n",
    "    if cache:\n",
    "        globals()['real_features'] = cache['features']\n",
    "        globals()['labels'] = cache['labels']\n",
    "        globals()['cond_tokens'] = cache['conditions']\n",
    "        globals()['group_tokens'] = cache['groups']\n",
    "        missing = [name for name in required if name not in globals()]\n",
    "        if not missing:\n",
    "            return\n",
    "    raise RuntimeError(\n",
    "        'Condition-aware tensors are unavailable. Run the feature extraction cell in Section 3 before splitting.'\n",
    "    )\n",
    "\n",
    "ensure_filtered_arrays()\n",
    "\n",
    "MIN_STRAT_SAMPLES = 2\n",
    "bucket_counts = Counter(list(zip(labels.tolist(), cond_tokens.tolist())))\n",
    "if bucket_counts and min(bucket_counts.values()) >= MIN_STRAT_SAMPLES:\n",
    "    strat_labels = np.array([f\"{int(cls)}_{cond}\" for cls, cond in zip(labels, cond_tokens)])\n",
    "    strat_note = 'class+condition'\n",
    "else:\n",
    "    strat_labels = labels\n",
    "    strat_note = 'class only'\n",
    "\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.33, random_state=RANDOM_SEED)\n",
    "train_idx, test_idx = next(sss.split(real_features, strat_labels))\n",
    "\n",
    "X_train, X_test = real_features[train_idx], real_features[test_idx]\n",
    "y_train, y_test = labels[train_idx], labels[test_idx]\n",
    "c_train, c_test = cond_tokens[train_idx], cond_tokens[test_idx]\n",
    "g_train, g_test = group_tokens[train_idx], group_tokens[test_idx]\n",
    "\n",
    "print(f\"Stratified split using {strat_note}\")\n",
    "print(f\"Train size: {len(X_train)}, Test size: {len(X_test)}\")\n",
    "print('Train buckets:', Counter(list(zip(y_train.tolist(), c_train.tolist()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensure_statistical_generators(verbose=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b495af",
   "metadata": {},
   "source": [
    "## 11. Bucketed generators (Gaussian Copula + interpolation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5a66fc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MIN_BUCKET_SIZE = 20\n",
    "\n",
    "def allocate_per_bucket(y_class, cond, total=None, balanced=False):\n",
    "    keys = list(zip(y_class.tolist(), cond.tolist()))\n",
    "    counts = Counter(keys)\n",
    "    if not counts:\n",
    "        return {}\n",
    "    buckets = sorted(counts.keys())\n",
    "    allocation = {}\n",
    "    if balanced:\n",
    "        n_each = min(counts[b] for b in buckets)\n",
    "        if n_each == 0:\n",
    "            return {}\n",
    "        allocation = {b: n_each for b in buckets}\n",
    "    else:\n",
    "        total = int(total or sum(counts.values()))\n",
    "        base = {b: counts[b] / sum(counts.values()) for b in buckets}\n",
    "        floored = {b: int(np.floor(total * base[b])) for b in buckets}\n",
    "        remainder = total - sum(floored.values())\n",
    "        fractions = sorted(((total * base[b] - floored[b], b) for b in buckets), reverse=True)\n",
    "        for _, bucket in fractions[:remainder]:\n",
    "            floored[bucket] += 1\n",
    "        allocation = floored\n",
    "    return {b: int(n) for b, n in allocation.items() if n > 0}\n",
    "\n",
    "\n",
    "def fit_copula_per_bucket(X, y_class, cond):\n",
    "    models = {}\n",
    "    rng = np.random.default_rng(RANDOM_SEED)\n",
    "    for key in sorted(set(zip(y_class.tolist(), cond.tolist()))):\n",
    "        mask = (y_class == key[0]) & (cond == key[1])\n",
    "        bucket = X[mask]\n",
    "        if len(bucket) < MIN_BUCKET_SIZE:\n",
    "            continue\n",
    "        qt = QuantileTransformer(output_distribution='normal', random_state=RANDOM_SEED)\n",
    "        Z = qt.fit_transform(bucket)\n",
    "        lw = LedoitWolf().fit(Z)\n",
    "        models[key] = {'qt': qt, 'mu': lw.location_, 'cov': lw.covariance_}\n",
    "    return models\n",
    "\n",
    "\n",
    "def sample_copula(models, n_per_bucket):\n",
    "    rng = np.random.default_rng(RANDOM_SEED)\n",
    "    samples = []\n",
    "    labels_out = []\n",
    "    cond_out = []\n",
    "    for key, n in n_per_bucket.items():\n",
    "        if key not in models or n <= 0:\n",
    "            continue\n",
    "        model = models[key]\n",
    "        Z = rng.multivariate_normal(model['mu'], model['cov'], size=n)\n",
    "        bucket = model['qt'].inverse_transform(Z)\n",
    "        bucket = np.clip(bucket, 0.0, None)\n",
    "        samples.append(bucket)\n",
    "        labels_out.append(np.full(n, key[0], dtype=int))\n",
    "        cond_out.append(np.array([key[1]] * n))\n",
    "    if not samples:\n",
    "        return None, None, None\n",
    "    X_synth = np.vstack(samples)\n",
    "    y_synth = np.hstack(labels_out)\n",
    "    c_synth = np.concatenate(cond_out)\n",
    "    return X_synth, y_synth, c_synth\n",
    "\n",
    "\n",
    "def interp_one_bucket(X_bucket, n_out, k_neighbors=10, noise_scale=0.0, seed=RANDOM_SEED):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    eps = 1e-8\n",
    "    X_bucket = np.asarray(X_bucket)\n",
    "    if len(X_bucket) == 0:\n",
    "        return None\n",
    "    if len(X_bucket) == 1:\n",
    "        xlog = np.log(X_bucket + eps)\n",
    "        noise = rng.normal(0, 1.0, size=(n_out, X_bucket.shape[1]))\n",
    "        synth = np.exp(xlog + noise_scale * noise) - eps\n",
    "        return np.clip(synth, 0.0, None)\n",
    "    Xlog = np.log(X_bucket + eps)\n",
    "    k = int(np.clip(k_neighbors, 2, max(2, len(X_bucket) - 1)))\n",
    "    nbrs = NearestNeighbors(n_neighbors=k, metric='euclidean').fit(Xlog)\n",
    "    base_idx = rng.integers(0, len(Xlog), size=n_out)\n",
    "    base = Xlog[base_idx]\n",
    "    neigh_idx = nbrs.kneighbors(base, return_distance=False)\n",
    "    pick = neigh_idx[np.arange(n_out), rng.integers(0, neigh_idx.shape[1], size=n_out)]\n",
    "    neigh = Xlog[pick]\n",
    "    alpha = rng.random((n_out, 1))\n",
    "    synth_log = alpha * base + (1 - alpha) * neigh\n",
    "    if noise_scale > 0:\n",
    "        jitter = rng.normal(0, noise_scale, size=synth_log.shape)\n",
    "        synth_log = synth_log + jitter\n",
    "    synth = np.exp(synth_log) - eps\n",
    "    return np.clip(synth, 0.0, None)\n",
    "\n",
    "\n",
    "def interpolate_per_bucket(X, y_class, cond, n_per_bucket, k=5, noise=0.0):\n",
    "    samples = []\n",
    "    labels_out = []\n",
    "    cond_out = []\n",
    "    for key, n in n_per_bucket.items():\n",
    "        mask = (y_class == key[0]) & (cond == key[1])\n",
    "        bucket = X[mask]\n",
    "        if len(bucket) < 3 or n <= 0:\n",
    "            continue\n",
    "        synth = interp_one_bucket(\n",
    "            bucket,\n",
    "            n_out=n,\n",
    "            k_neighbors=k,\n",
    "            noise_scale=noise,\n",
    "            seed=RANDOM_SEED + hash(key[1]) % 1000\n",
    "        )\n",
    "        if synth is None:\n",
    "            continue\n",
    "        samples.append(synth)\n",
    "        labels_out.append(np.full(len(synth), key[0], dtype=int))\n",
    "        cond_out.append(np.array([key[1]] * len(synth)))\n",
    "    if not samples:\n",
    "        return None, None, None\n",
    "    X_synth = np.vstack(samples)\n",
    "    y_synth = np.hstack(labels_out)\n",
    "    c_synth = np.concatenate(cond_out)\n",
    "    return X_synth, y_synth, c_synth\n",
    "\n",
    "\n",
    "def condition_onehot(conditions):\n",
    "    cats = pd.Categorical(conditions, categories=CONDITION_LEVELS)\n",
    "    return pd.get_dummies(cats, dummy_na=False).to_numpy()\n",
    "\n",
    "\n",
    "def maybe_concat_condition(features, conditions):\n",
    "    if not RUN_CFG.get('cond_onehot', False):\n",
    "        return features\n",
    "    return np.hstack([features, condition_onehot(conditions)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71035fcb",
   "metadata": {},
   "source": [
    "## 12. Generate synthetic dataset (train-only fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0a8528c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requested samples per bucket: {(0, 'S1'): 248, (0, 'S2_match'): 248, (0, 'S2_nomatch'): 248, (1, 'S1'): 248, (1, 'S2_match'): 248, (1, 'S2_nomatch'): 248}\n",
      "Synthetic dataset shape: (1488, 5)\n",
      "Synthetic buckets: Counter({(0, 'S1'): 248, (0, 'S2_match'): 248, (0, 'S2_nomatch'): 248, (1, 'S1'): 248, (1, 'S2_match'): 248, (1, 'S2_nomatch'): 248})\n"
     ]
    }
   ],
   "source": [
    "\n",
    "bucket_allocation = allocate_per_bucket(\n",
    "    y_train,\n",
    "    c_train,\n",
    "    total=len(X_train),\n",
    "    balanced=RUN_CFG['balanced']\n",
    ")\n",
    "print('Requested samples per bucket:', bucket_allocation)\n",
    "\n",
    "if RUN_CFG['generator'] == 'copula':\n",
    "    copula_models = fit_copula_per_bucket(X_train, y_train, c_train)\n",
    "    synth_X, synth_y, synth_c = sample_copula(copula_models, bucket_allocation)\n",
    "elif RUN_CFG['generator'] == 'interp':\n",
    "    synth_X, synth_y, synth_c = interpolate_per_bucket(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        c_train,\n",
    "        bucket_allocation,\n",
    "        k=RUN_CFG['k'],\n",
    "        noise=RUN_CFG['noise']\n",
    "    )\n",
    "else:\n",
    "    raise ValueError(f\"Unsupported generator: {RUN_CFG['generator']}\")\n",
    "\n",
    "if synth_X is None:\n",
    "    raise RuntimeError('Generator did not produce any samples. Adjust filters or parameters.')\n",
    "\n",
    "print(f\"Synthetic dataset shape: {synth_X.shape}\")\n",
    "print('Synthetic buckets:', Counter(list(zip(synth_y.tolist(), synth_c.tolist()))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f34a79",
   "metadata": {},
   "source": [
    "## 13. Evaluation helpers (distribution, detectability, utility)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "715e42c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MIN_EVAL_SAMPLES = 25\n",
    "BAND_NAMES = list(FREQUENCY_BANDS.keys())\n",
    "\n",
    "\n",
    "def compute_mmd(real_X, synth_X):\n",
    "    XX = cdist(real_X, real_X, metric='euclidean')\n",
    "    YY = cdist(synth_X, synth_X, metric='euclidean')\n",
    "    XY = cdist(real_X, synth_X, metric='euclidean')\n",
    "    return float(np.mean(XX) + np.mean(YY) - 2 * np.mean(XY))\n",
    "\n",
    "\n",
    "def corr_matrix_similarity(real_X, synth_X):\n",
    "    if real_X.size == 0 or synth_X.size == 0:\n",
    "        return None\n",
    "    c_real = np.corrcoef(real_X.T)\n",
    "    c_synth = np.corrcoef(synth_X.T)\n",
    "    iu = np.triu_indices_from(c_real, k=1)\n",
    "    if not iu[0].size:\n",
    "        return None\n",
    "    return float(np.corrcoef(c_real[iu], c_synth[iu])[0, 1])\n",
    "\n",
    "\n",
    "def per_band_spearman(real_X, synth_X, max_samples=5000, seed=RANDOM_SEED):\n",
    "    if real_X.size == 0 or synth_X.size == 0:\n",
    "        return None\n",
    "    n = min(len(real_X), len(synth_X), max_samples)\n",
    "    if n < 2:\n",
    "        return None\n",
    "    rng = np.random.default_rng(seed)\n",
    "    if len(real_X) > n:\n",
    "        idx_real = rng.choice(len(real_X), size=n, replace=False)\n",
    "        real_sel = real_X[idx_real]\n",
    "    else:\n",
    "        real_sel = real_X\n",
    "    if len(synth_X) > n:\n",
    "        idx_synth = rng.choice(len(synth_X), size=n, replace=False)\n",
    "        synth_sel = synth_X[idx_synth]\n",
    "    else:\n",
    "        synth_sel = synth_X\n",
    "    vals = []\n",
    "    for idx in range(real_sel.shape[1]):\n",
    "        rho = stats.spearmanr(real_sel[:, idx], synth_sel[:, idx]).correlation\n",
    "        vals.append(float(rho))\n",
    "    return vals\n",
    "\n",
    "\n",
    "def compute_distribution_metrics(real_X, synth_X):\n",
    "    if real_X.size == 0 or synth_X.size == 0:\n",
    "        return None\n",
    "    ks_rows = []\n",
    "    for i, band in enumerate(BAND_NAMES):\n",
    "        stat, p_val = stats.ks_2samp(real_X[:, i], synth_X[:, i])\n",
    "        ks_rows.append({'band': band, 'ks_stat': float(stat), 'p_value': float(p_val)})\n",
    "    return {\n",
    "        'ks': ks_rows,\n",
    "        'mmd': compute_mmd(real_X, synth_X),\n",
    "        'corr_sim': corr_matrix_similarity(real_X, synth_X),\n",
    "        'spearman': per_band_spearman(real_X, synth_X)\n",
    "    }\n",
    "\n",
    "\n",
    "def detectability_metrics(real_X, synth_X, seed=RANDOM_SEED):\n",
    "    if real_X.size == 0 or synth_X.size == 0:\n",
    "        return None\n",
    "    if len(real_X) < MIN_EVAL_SAMPLES or len(synth_X) < MIN_EVAL_SAMPLES:\n",
    "        return None\n",
    "    X = np.vstack([real_X, synth_X])\n",
    "    y = np.concatenate([np.ones(len(real_X), dtype=int), np.zeros(len(synth_X), dtype=int)])\n",
    "    splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.33, random_state=seed)\n",
    "    train_idx, test_idx = next(splitter.split(X, y))\n",
    "    clf = RandomForestClassifier(n_estimators=300, random_state=seed, class_weight='balanced')\n",
    "    clf.fit(X[train_idx], y[train_idx])\n",
    "    proba = clf.predict_proba(X[test_idx])[:, 1]\n",
    "    preds = (proba >= 0.5).astype(int)\n",
    "    acc = accuracy_score(y[test_idx], preds)\n",
    "    auc = roc_auc_score(y[test_idx], proba)\n",
    "    return {'accuracy': float(acc), 'auc': float(auc)}\n",
    "\n",
    "\n",
    "def clf_factory_rf():\n",
    "    return RandomForestClassifier(n_estimators=300, random_state=RANDOM_SEED, class_weight='balanced')\n",
    "\n",
    "\n",
    "def tstr_trtr_per_condition(X_train, y_train, c_train,\n",
    "                            X_test, y_test, c_test,\n",
    "                            X_synth, y_synth, c_synth,\n",
    "                            condition_name,\n",
    "                            clf_factory,\n",
    "                            min_samples=MIN_EVAL_SAMPLES):\n",
    "    if condition_name is None:\n",
    "        real_mask = np.ones(len(X_test), dtype=bool)\n",
    "        synth_mask = np.ones(len(X_synth), dtype=bool)\n",
    "    else:\n",
    "        real_mask = (c_test == condition_name)\n",
    "        synth_mask = (c_synth == condition_name)\n",
    "    n_real = int(real_mask.sum())\n",
    "    n_synth = int(synth_mask.sum())\n",
    "    if n_real < min_samples:\n",
    "        return None\n",
    "\n",
    "    clf_real = clf_factory()\n",
    "    clf_real.fit(maybe_concat_condition(X_train, c_train), y_train)\n",
    "    trtr = clf_real.score(maybe_concat_condition(X_test[real_mask], c_test[real_mask]), y_test[real_mask])\n",
    "\n",
    "    if n_synth < min_samples:\n",
    "        return {\n",
    "            'TRTR': float(trtr),\n",
    "            'TSTR': None,\n",
    "            'gap': None,\n",
    "            'n_test': n_real,\n",
    "            'n_synth': n_synth\n",
    "        }\n",
    "\n",
    "    clf_synth = clf_factory()\n",
    "    clf_synth.fit(maybe_concat_condition(X_synth[synth_mask], c_synth[synth_mask]), y_synth[synth_mask])\n",
    "    tstr = clf_synth.score(maybe_concat_condition(X_test[real_mask], c_test[real_mask]), y_test[real_mask])\n",
    "\n",
    "    return {\n",
    "        'TRTR': float(trtr),\n",
    "        'TSTR': float(tstr),\n",
    "        'gap': float(abs(trtr - tstr)),\n",
    "        'n_test': n_real,\n",
    "        'n_synth': n_synth\n",
    "    }\n",
    "\n",
    "\n",
    "def cross_condition_transfer(source_cond, target_cond,\n",
    "                             X_test, y_test, c_test,\n",
    "                             X_synth, y_synth, c_synth,\n",
    "                             clf_factory):\n",
    "    src_mask = (c_synth == source_cond)\n",
    "    tgt_mask = (c_test == target_cond)\n",
    "    if src_mask.sum() < MIN_EVAL_SAMPLES or tgt_mask.sum() < MIN_EVAL_SAMPLES:\n",
    "        return None\n",
    "    clf = clf_factory()\n",
    "    clf.fit(maybe_concat_condition(X_synth[src_mask], c_synth[src_mask]), y_synth[src_mask])\n",
    "    score = clf.score(maybe_concat_condition(X_test[tgt_mask], c_test[tgt_mask]), y_test[tgt_mask])\n",
    "    return {\n",
    "        'source': source_cond,\n",
    "        'target': target_cond,\n",
    "        'TSTR_cross': float(score),\n",
    "        'n_source': int(src_mask.sum()),\n",
    "        'n_target': int(tgt_mask.sum())\n",
    "    }\n",
    "\n",
    "\n",
    "def evaluate_cross_condition_grid(conditions, per_condition_metrics,\n",
    "                                  X_test, y_test, c_test,\n",
    "                                  X_synth, y_synth, c_synth):\n",
    "    rows = []\n",
    "    if len(conditions) < 2:\n",
    "        return rows\n",
    "    for source in conditions:\n",
    "        for target in conditions:\n",
    "            if source == target:\n",
    "                continue\n",
    "            result = cross_condition_transfer(\n",
    "                source,\n",
    "                target,\n",
    "                X_test,\n",
    "                y_test,\n",
    "                c_test,\n",
    "                X_synth,\n",
    "                y_synth,\n",
    "                c_synth,\n",
    "                clf_factory_rf\n",
    "            )\n",
    "            if not result:\n",
    "                continue\n",
    "            baseline = per_condition_metrics.get(target, {}).get('utility', {}).get('TRTR')\n",
    "            if baseline is not None and result.get('TSTR_cross') is not None:\n",
    "                result['baseline_TRTR'] = float(baseline)\n",
    "                result['gap_vs_baseline'] = float(baseline - result['TSTR_cross'])\n",
    "            rows.append(result)\n",
    "    return rows\n",
    "\n",
    "\n",
    "def evaluate_cross_group_transfers(full_dataset, target_groups,\n",
    "                                   X_synth, y_synth, c_synth):\n",
    "    results = []\n",
    "    if not target_groups:\n",
    "        return results\n",
    "    features = full_dataset['features']\n",
    "    labels_full = full_dataset['labels']\n",
    "    cond_full = full_dataset['conditions']\n",
    "    group_full = full_dataset['groups']\n",
    "\n",
    "    for grp in target_groups:\n",
    "        mask = (group_full == grp)\n",
    "        if CONDITION_FILTER:\n",
    "            mask = mask & np.isin(cond_full, list(CONDITION_FILTER))\n",
    "        if mask.sum() < MIN_EVAL_SAMPLES * 2:\n",
    "            print(f\"[transfer] Skipping group {grp}: insufficient samples ({int(mask.sum())})\")\n",
    "            continue\n",
    "        X_grp = features[mask]\n",
    "        y_grp = labels_full[mask]\n",
    "        c_grp = cond_full[mask]\n",
    "\n",
    "        grp_counts = Counter(list(zip(y_grp.tolist(), c_grp.tolist())))\n",
    "        if grp_counts and min(grp_counts.values()) >= MIN_STRAT_SAMPLES:\n",
    "            strat = np.array([f\"{int(cls)}_{cond}\" for cls, cond in zip(y_grp, c_grp)])\n",
    "        else:\n",
    "            strat = y_grp\n",
    "        splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.33, random_state=RANDOM_SEED)\n",
    "        tr_idx, te_idx = next(splitter.split(X_grp, strat))\n",
    "\n",
    "        X_trg_tr, X_trg_te = X_grp[tr_idx], X_grp[te_idx]\n",
    "        y_trg_tr, y_trg_te = y_grp[tr_idx], y_grp[te_idx]\n",
    "        c_trg_tr, c_trg_te = c_grp[tr_idx], c_grp[te_idx]\n",
    "\n",
    "        conds_in_group = sorted(np.unique(c_trg_te)) if not CONDITION_FILTER else sorted(CONDITION_FILTER)\n",
    "        per_condition_rows = []\n",
    "        for cond in conds_in_group:\n",
    "            metrics = tstr_trtr_per_condition(\n",
    "                X_trg_tr,\n",
    "                y_trg_tr,\n",
    "                c_trg_tr,\n",
    "                X_trg_te,\n",
    "                y_trg_te,\n",
    "                c_trg_te,\n",
    "                X_synth,\n",
    "                y_synth,\n",
    "                c_synth,\n",
    "                cond,\n",
    "                clf_factory_rf\n",
    "            )\n",
    "            if metrics:\n",
    "                per_condition_rows.append({'group': grp, 'condition': cond, **metrics})\n",
    "        pooled_metrics = tstr_trtr_per_condition(\n",
    "            X_trg_tr,\n",
    "            y_trg_tr,\n",
    "            c_trg_tr,\n",
    "            X_trg_te,\n",
    "            y_trg_te,\n",
    "            c_trg_te,\n",
    "            X_synth,\n",
    "            y_synth,\n",
    "            c_synth,\n",
    "            None,\n",
    "            clf_factory_rf\n",
    "        )\n",
    "        results.append({'group': grp, 'per_condition': per_condition_rows, 'pooled': pooled_metrics})\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4a7ef2",
   "metadata": {},
   "source": [
    "## 14. Condition-aware evaluation grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e5b3ebf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------------------------------\n",
      "Condition-aware summary (TRTR/TSTR, detectability, distribution)\n",
      "------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Condition</th>\n",
       "      <th>Real n</th>\n",
       "      <th>Synth n</th>\n",
       "      <th>TRTR</th>\n",
       "      <th>TSTR</th>\n",
       "      <th>Gap</th>\n",
       "      <th>RvS acc</th>\n",
       "      <th>RvS AUC</th>\n",
       "      <th>MMD (RBF)</th>\n",
       "      <th>KS min p</th>\n",
       "      <th>CorrSim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>S1</td>\n",
       "      <td>264</td>\n",
       "      <td>496</td>\n",
       "      <td>0.6288</td>\n",
       "      <td>0.6818</td>\n",
       "      <td>0.0530</td>\n",
       "      <td>0.6733</td>\n",
       "      <td>0.6786</td>\n",
       "      <td>-0.3350</td>\n",
       "      <td>0.0014</td>\n",
       "      <td>0.9014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>S2_match</td>\n",
       "      <td>263</td>\n",
       "      <td>496</td>\n",
       "      <td>0.7262</td>\n",
       "      <td>0.7300</td>\n",
       "      <td>0.0038</td>\n",
       "      <td>0.6494</td>\n",
       "      <td>0.6714</td>\n",
       "      <td>-1.3014</td>\n",
       "      <td>0.0578</td>\n",
       "      <td>0.9850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>S2_nomatch</td>\n",
       "      <td>246</td>\n",
       "      <td>496</td>\n",
       "      <td>0.6667</td>\n",
       "      <td>0.7236</td>\n",
       "      <td>0.0569</td>\n",
       "      <td>0.6898</td>\n",
       "      <td>0.6820</td>\n",
       "      <td>-2.4418</td>\n",
       "      <td>0.2080</td>\n",
       "      <td>0.9090</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Condition  Real n  Synth n    TRTR    TSTR     Gap  RvS acc  RvS AUC  \\\n",
       "0          S1     264      496  0.6288  0.6818  0.0530   0.6733   0.6786   \n",
       "1    S2_match     263      496  0.7262  0.7300  0.0038   0.6494   0.6714   \n",
       "2  S2_nomatch     246      496  0.6667  0.7236  0.0569   0.6898   0.6820   \n",
       "\n",
       "   MMD (RBF)  KS min p  CorrSim  \n",
       "0    -0.3350    0.0014   0.9014  \n",
       "1    -1.3014    0.0578   0.9850  \n",
       "2    -2.4418    0.2080   0.9090  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 14. Condition-aware evaluation grid (completed)\n",
    "\n",
    "if RUN_CFG['condition'] == 'pooled':\n",
    "    eval_conditions = []\n",
    "elif CONDITION_FILTER:\n",
    "    eval_conditions = sorted(CONDITION_FILTER)\n",
    "else:\n",
    "    eval_conditions = sorted(np.unique(c_train))\n",
    "\n",
    "per_condition_metrics = {}\n",
    "for cond in eval_conditions or [None]:  # include pooled if empty\n",
    "    title = cond if cond is not None else \"POOLED\"\n",
    "    # masks\n",
    "    real_mask = np.ones(len(X_test), dtype=bool) if cond is None else (c_test == cond)\n",
    "    synth_mask = np.ones(len(synth_X), dtype=bool) if cond is None else (synth_c == cond)\n",
    "\n",
    "    n_real = int(real_mask.sum())\n",
    "    n_synth = int(synth_mask.sum())\n",
    "\n",
    "    if n_real < MIN_EVAL_SAMPLES:\n",
    "        print(f\"[{title}] Skipping: not enough real test samples ({n_real} < {MIN_EVAL_SAMPLES})\")\n",
    "        continue\n",
    "\n",
    "    # TRTR & TSTR for this condition (train uses full train split; test restricted to condition)\n",
    "    util = tstr_trtr_per_condition(\n",
    "        X_train, y_train, c_train,\n",
    "        X_test,  y_test,  c_test,\n",
    "        synth_X, synth_y, synth_c,\n",
    "        cond,\n",
    "        clf_factory_rf,\n",
    "        min_samples=MIN_EVAL_SAMPLES\n",
    "    )\n",
    "\n",
    "    # Detectability & distribution metrics for this slice\n",
    "    det = None\n",
    "    dist = None\n",
    "    if n_synth >= MIN_EVAL_SAMPLES:\n",
    "        det  = detectability_metrics(\n",
    "            maybe_concat_condition(X_test[real_mask], c_test[real_mask]),\n",
    "            maybe_concat_condition(synth_X[synth_mask], synth_c[synth_mask]),\n",
    "            seed=RANDOM_SEED\n",
    "        )\n",
    "        dist = compute_distribution_metrics(\n",
    "            X_test[real_mask],\n",
    "            synth_X[synth_mask]\n",
    "        )\n",
    "\n",
    "    per_condition_metrics[title] = {\n",
    "        \"counts\": {\"real_test\": n_real, \"synthetic\": n_synth},\n",
    "        \"utility\": util,\n",
    "        \"detectability\": det,\n",
    "        \"distribution\": dist,\n",
    "    }\n",
    "\n",
    "# Pretty print a compact summary\n",
    "rows = []\n",
    "for k, v in per_condition_metrics.items():\n",
    "    util = v[\"utility\"] or {}\n",
    "    det  = v[\"detectability\"] or {}\n",
    "    dist = v[\"distribution\"] or {}\n",
    "    ks_min = min([row[\"p_value\"] for row in (dist.get(\"ks\") or [])], default=np.nan)\n",
    "    rows.append({\n",
    "        \"Condition\": k,\n",
    "        \"Real n\": v[\"counts\"][\"real_test\"],\n",
    "        \"Synth n\": v[\"counts\"][\"synthetic\"],\n",
    "        \"TRTR\": None if util is None else util.get(\"TRTR\"),\n",
    "        \"TSTR\": None if util is None else util.get(\"TSTR\"),\n",
    "        \"Gap\":  None if util is None else util.get(\"gap\"),\n",
    "        \"RvS acc\": det.get(\"accuracy\") if det else None,\n",
    "        \"RvS AUC\": det.get(\"auc\") if det else None,\n",
    "        \"MMD (RBF)\": dist.get(\"mmd\") if dist else None,\n",
    "        \"KS min p\": ks_min,\n",
    "        \"CorrSim\": dist.get(\"corr_sim\") if dist else None,\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(rows)\n",
    "pd.set_option(\"display.precision\", 4)\n",
    "print(\"\\n\" + \"-\"*72)\n",
    "print(\"Condition-aware summary (TRTR/TSTR, detectability, distribution)\")\n",
    "print(\"-\"*72)\n",
    "display(summary_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55435c4",
   "metadata": {},
   "source": [
    "## 15. Persist condition-aware artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "29dc7552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved condition-aware metrics and artifacts to ../output/phase3_conditional/pooled\n"
     ]
    }
   ],
   "source": [
    "\n",
    "run_root = SAVE_ROOT / RUN_CFG['groups']\n",
    "run_root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "allocation_summary = {f\"{cls}_{cond}\": int(count) for (cls, cond), count in bucket_allocation.items()}\n",
    "train_bucket_summary = {f\"{cls}_{cond}\": int(count) for (cls, cond), count in Counter(list(zip(y_train.tolist(), c_train.tolist()))).items()}\n",
    "\n",
    "def serialize_metrics(metrics):\n",
    "    if metrics is None:\n",
    "        return None\n",
    "    if isinstance(metrics, dict):\n",
    "        return {k: serialize_metrics(v) for k, v in metrics.items()}\n",
    "    if isinstance(metrics, (list, tuple)):\n",
    "        return [serialize_metrics(v) for v in metrics]\n",
    "    if isinstance(metrics, pd.DataFrame):\n",
    "        return metrics.to_dict(orient='records')\n",
    "    if isinstance(metrics, (np.generic,)):\n",
    "        return metrics.item()\n",
    "    return metrics\n",
    "\n",
    "baseline_distribution_serialized = {\n",
    "    name: {\n",
    "        'ks': serialize_metrics(result[0]),\n",
    "        'mmd': float(result[1]),\n",
    "    }\n",
    "    for name, result in distribution_results.items()\n",
    "}\n",
    "\n",
    "clustering_serialized = {\n",
    "    name: serialize_metrics(df)\n",
    "    for name, df in clustering_reports.items()\n",
    "}\n",
    "\n",
    "def permanova_to_payload(result):\n",
    "    if result is None:\n",
    "        return None\n",
    "    if hasattr(result, 'to_dict'):\n",
    "        return serialize_metrics(result.to_dict())\n",
    "    return str(result)\n",
    "\n",
    "permanova_serialized = {\n",
    "    name: permanova_to_payload(result)\n",
    "    for name, result in permanova_results.items()\n",
    "}\n",
    "\n",
    "baseline_detectability = {\n",
    "    'Correlation Sampling': float(acc_corr),\n",
    "    'Mixup Baseline': float(acc_mix),\n",
    "    'WGAN-GP': float(acc_wgan) if acc_wgan is not None else None,\n",
    "}\n",
    "\n",
    "pooled_metrics_safe = per_condition_metrics.get('POOLED') or per_condition_metrics.get(None) or {}\n",
    "cross_condition_rows = globals().get('cross_condition_rows', [])\n",
    "cross_group_results = globals().get('cross_group_results', [])\n",
    "\n",
    "payload = {\n",
    "    'config': RUN_CFG,\n",
    "    'group_filter': sorted(GROUP_FILTER),\n",
    "    'condition_filter': sorted(CONDITION_FILTER) if CONDITION_FILTER else None,\n",
    "    'train_counts': {\n",
    "        'total': int(len(X_train)),\n",
    "        'class': {int(k): int(v) for k, v in Counter(y_train.tolist()).items()},\n",
    "        'bucket': train_bucket_summary\n",
    "    },\n",
    "    'allocation': allocation_summary,\n",
    "    'baseline_metrics': {\n",
    "        'distribution': baseline_distribution_serialized,\n",
    "        'detectability_acc': baseline_detectability,\n",
    "        'clustering': clustering_serialized,\n",
    "        'permanova': permanova_serialized,\n",
    "    },\n",
    "    'per_condition': {cond: serialize_metrics(vals) for cond, vals in per_condition_metrics.items()},\n",
    "    'pooled': serialize_metrics(pooled_metrics_safe),\n",
    "    'cross_condition': serialize_metrics(cross_condition_rows),\n",
    "    'cross_group': serialize_metrics(cross_group_results)\n",
    "}\n",
    "\n",
    "with open(run_root / 'metrics_overview.json', 'w') as f:\n",
    "    json.dump(payload, f, indent=2)\n",
    "\n",
    "for cond, metrics in per_condition_metrics.items():\n",
    "    cond_dir = run_root / cond\n",
    "    cond_dir.mkdir(parents=True, exist_ok=True)\n",
    "    with open(cond_dir / 'metrics.json', 'w') as f:\n",
    "        json.dump(serialize_metrics(metrics), f, indent=2)\n",
    "\n",
    "pooled_dir = run_root / 'pooled'\n",
    "pooled_dir.mkdir(parents=True, exist_ok=True)\n",
    "with open(pooled_dir / 'metrics.json', 'w') as f:\n",
    "    json.dump(serialize_metrics(pooled_metrics_safe), f, indent=2)\n",
    "\n",
    "artifact = {\n",
    "    'generator': RUN_CFG['generator'],\n",
    "    'allocation': allocation_summary,\n",
    "    'synth_features': synth_X,\n",
    "    'synth_labels': synth_y,\n",
    "    'synth_conditions': synth_c\n",
    "}\n",
    "with open(pooled_dir / f\"synth_{RUN_CFG['generator']}.pkl\", 'wb') as f:\n",
    "    pickle.dump(artifact, f)\n",
    "\n",
    "print('Saved condition-aware metrics and artifacts to', run_root)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- I. Gulrajani, F. Ahmed, M. Arjovsky, V. Dumoulin, and A. Courville. \"Improved Training of Wasserstein GANs.\" *Advances in Neural Information Processing Systems*, 2017.\n",
    "- C. Esteban, S. L. Hyland, and G. Rätsch. \"Real-valued (Medical) Time Series Generation with Recurrent Conditional GANs.\" *NeurIPS Workshop on Adversarial Training*, 2017.\n",
    "- G. De la Torre et al. \"A Statistical Approach for Synthetic EEG Data Generation.\" *IEEE Transactions on Neural Systems and Rehabilitation Engineering*, 2022.\n",
    "- T. Choi, B. Lee, and M. Kim. \"Data Augmentation with Gaussian Copula Models for Time-Series Classification.\" *Pattern Recognition Letters*, 2021.\n",
    "- L. Fawcett and D. Clare. \"Synthetic Data Generation for Time Series via Clustering and Interpolation.\" *Journal of Computational Science*, 2020.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ollama_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
