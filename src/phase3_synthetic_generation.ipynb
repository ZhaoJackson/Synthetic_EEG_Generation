{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 3: Synthetic EEG Data Generation\n",
    "\n",
    "## Objective\n",
    "Generate synthetic EEG data using methods from recent literature:\n",
    "1. Correlation sampling method (Statistical Approach)\n",
    "2. WGAN-GP approach (simplified)\n",
    "3. Evaluation via TSTR, TRTR, clustering, and statistical tests\n",
    "\n",
    "## Literature Review Summary\n",
    "- **Correlation Sampling**: Analyze frequency band correlations, generate signals preserving structure\n",
    "- **WGAN-GP**: More reliable than vanilla GAN for EEG generation\n",
    "- **Evaluation**: Random Forest classifier, PERMANOVA, clustering overlap, TSTR/TRTR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T18:46:28.995502Z",
     "iopub.status.busy": "2025-11-02T18:46:28.995178Z",
     "iopub.status.idle": "2025-11-02T18:46:31.530824Z",
     "shell.execute_reply": "2025-11-02T18:46:31.530534Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "RANDOM SEED SET FOR REPRODUCIBILITY\n",
      "Seed value: 42\n",
      "============================================================\n",
      "Phase 3: Synthetic EEG Generation (Condition-aware)\n",
      "============================================================\n",
      "Group filter      : ['a', 'c']\n",
      "Condition filter  : All\n",
      "Generator         : interp\n",
      "Balanced sampling : True\n",
      "k / noise         : (5, 0.0)\n",
      "Transfer targets  : None\n",
      "Save root         : /Users/jacksonzhao/Desktop/Synthetic_EEG_Generation/output/phase3_conditional\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import warnings\n",
    "from collections import Counter, defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from IPython.display import display\n",
    "from scipy import signal, stats\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.covariance import LedoitWolf\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    adjusted_rand_score,\n",
    "    normalized_mutual_info_score,\n",
    "    roc_auc_score,\n",
    "    silhouette_score,\n",
    ")\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, train_test_split\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "import pickle\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    from torch import nn, optim\n",
    "except ImportError:\n",
    "    torch = None\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "print('=' * 60)\n",
    "print('RANDOM SEED SET FOR REPRODUCIBILITY')\n",
    "print(f'Seed value: {RANDOM_SEED}')\n",
    "print('=' * 60)\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "\n",
    "DATA_PATH = Path.home() / '.cache/kagglehub/datasets/nnair25/Alcoholics/versions/1'\n",
    "TRAIN_PATH = DATA_PATH / 'SMNI_CMI_TRAIN'\n",
    "TEST_PATH = DATA_PATH / 'SMNI_CMI_TEST'\n",
    "\n",
    "CONDITION_LEVELS = ['S1', 'S2_match', 'S2_nomatch', 'UNKNOWN']\n",
    "\n",
    "RUN_CFG = dict(\n",
    "    groups='pooled',          # 'pooled', 'a_only', 'c_only'\n",
    "    condition='each',         # 'each', 'pooled', or a specific condition token\n",
    "    generator='interp',       # 'interp' or 'copula'\n",
    "    balanced=True,            # balance samples per (class, condition)\n",
    "    k=5,\n",
    "    noise=0.0,\n",
    "    cond_onehot=False,        # append condition one-hot vectors to classifier inputs\n",
    "    transfer_to=None,         # e.g., ['a'] to score control->alcoholic transfer\n",
    "    save_dir='../output/phase3_conditional'\n",
    ")\n",
    "\n",
    "GROUP_CHOICES = {\n",
    "    'pooled': {'a', 'c'},\n",
    "    'a_only': {'a'},\n",
    "    'c_only': {'c'},\n",
    "}\n",
    "if RUN_CFG['groups'] not in GROUP_CHOICES:\n",
    "    raise ValueError(f\"Unknown group config: {RUN_CFG['groups']}\")\n",
    "GROUP_FILTER = GROUP_CHOICES[RUN_CFG['groups']]\n",
    "\n",
    "if RUN_CFG['condition'] in CONDITION_LEVELS:\n",
    "    CONDITION_FILTER = {RUN_CFG['condition']}\n",
    "elif RUN_CFG['condition'] in {'each', 'pooled'}:\n",
    "    CONDITION_FILTER = None\n",
    "else:\n",
    "    raise ValueError(f\"Unknown condition scope: {RUN_CFG['condition']}\")\n",
    "\n",
    "TRANSFER_TARGETS = set(RUN_CFG.get('transfer_to') or [])\n",
    "SAVE_ROOT = Path(RUN_CFG['save_dir'])\n",
    "SAVE_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print('Phase 3: Synthetic EEG Generation (Condition-aware)')\n",
    "print('=' * 60)\n",
    "print(f\"Group filter      : {sorted(GROUP_FILTER)}\")\n",
    "print(f\"Condition filter  : {sorted(CONDITION_FILTER) if CONDITION_FILTER else 'All'}\")\n",
    "print(f\"Generator         : {RUN_CFG['generator']}\")\n",
    "print(f\"Balanced sampling : {RUN_CFG['balanced']}\")\n",
    "print(f\"k / noise         : ({RUN_CFG['k']}, {RUN_CFG['noise']})\")\n",
    "print(f\"Transfer targets  : {sorted(TRANSFER_TARGETS) if TRANSFER_TARGETS else 'None'}\")\n",
    "print(f\"Save root         : {SAVE_ROOT.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Analysis Results from Phase 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T18:46:31.551382Z",
     "iopub.status.busy": "2025-11-02T18:46:31.551079Z",
     "iopub.status.idle": "2025-11-02T18:46:31.554498Z",
     "shell.execute_reply": "2025-11-02T18:46:31.554220Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Phase 2 analysis results\n",
      "Frequency bands: {'Delta': (0.5, 4), 'Theta': (4, 8), 'Alpha': (8, 13), 'Beta': (13, 30), 'Gamma': (30, 50)}\n",
      "Sampling rate: 256 Hz\n"
     ]
    }
   ],
   "source": [
    "# Load Phase 2 results (relative path to output folder)\n",
    "with open('../output/phase2_analysis_results.pkl', 'rb') as f:\n",
    "    analysis_results = pickle.load(f)\n",
    "\n",
    "FREQUENCY_BANDS = analysis_results['frequency_bands']\n",
    "SAMPLING_RATE = analysis_results['sampling_rate']\n",
    "\n",
    "print(\"Loaded Phase 2 analysis results\")\n",
    "print(f\"Frequency bands: {FREQUENCY_BANDS}\")\n",
    "print(f\"Sampling rate: {SAMPLING_RATE} Hz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Shared constants used across sections ----\n",
    "MIN_EVAL_SAMPLES = 25      # small guard for per-bucket/condition eval\n",
    "BAND_NAMES = [\"Delta\",\"Theta\",\"Alpha\",\"Beta\",\"Gamma\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare Real Data for Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data & Feature Summary\n",
    "\n",
    "- **Corpus**: All available CSV files in `SMNI_CMI_TRAIN` (≈468 total; 235 alcoholic / 233 control). Adjust `MAX_FILES_PER_CLASS` if you need a subset.\n",
    "- **Epochs**: Dynamically determined from `sensor position × trial` combinations across the full corpus.\n",
    "- **Features**: 5-D band power vectors computed via Welch’s PSD at 256 Hz covering Δ (0.5–4 Hz), θ (4–8 Hz), α (8–13 Hz), β (13–30 Hz), γ (30–50 Hz).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T18:46:31.555803Z",
     "iopub.status.busy": "2025-11-02T18:46:31.555721Z",
     "iopub.status.idle": "2025-11-02T18:46:32.151968Z",
     "shell.execute_reply": "2025-11-02T18:46:32.151717Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identifying subject types...\n",
      "Found 235 alcoholic files\n",
      "Found 233 control files\n",
      "\n",
      "Loading 468 files for generator training...\n",
      "  Alcoholic files selected: 235\n",
      "  Control files selected  : 233\n"
     ]
    }
   ],
   "source": [
    "# Load training files\n",
    "train_files = sorted(list(TRAIN_PATH.glob('*.csv')))\n",
    "\n",
    "# Separate alcoholic and control files  \n",
    "alcoholic_files = []\n",
    "control_files = []\n",
    "\n",
    "print(\"Identifying subject types...\")\n",
    "for file in train_files:\n",
    "    df_peek = pd.read_csv(file, nrows=1)\n",
    "    subject_type = df_peek['subject identifier'].iloc[0]\n",
    "    if subject_type == 'a':\n",
    "        alcoholic_files.append(file)\n",
    "    else:\n",
    "        control_files.append(file)\n",
    "\n",
    "print(f\"Found {len(alcoholic_files)} alcoholic files\")\n",
    "print(f\"Found {len(control_files)} control files\")\n",
    "\n",
    "# Configure which files to load for generator training\n",
    "MAX_FILES_PER_CLASS = None\n",
    "\n",
    "if MAX_FILES_PER_CLASS is None:\n",
    "    selected_alcoholic = list(alcoholic_files)\n",
    "    selected_control = list(control_files)\n",
    "else:\n",
    "    selected_alcoholic = list(alcoholic_files[:MAX_FILES_PER_CLASS])\n",
    "    selected_control = list(control_files[:MAX_FILES_PER_CLASS])\n",
    "\n",
    "sample_files = selected_alcoholic + selected_control\n",
    "if not sample_files:\n",
    "    raise ValueError(\"No training files selected. Check MAX_FILES_PER_CLASS or dataset path.\")\n",
    "\n",
    "random.shuffle(sample_files)\n",
    "\n",
    "\n",
    "print(f\"\\nLoading {len(sample_files)} files for generator training...\")\n",
    "print(f\"  Alcoholic files selected: {len(selected_alcoholic)}\")\n",
    "print(f\"  Control files selected  : {len(selected_control)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Extract EEG Features from Real Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T18:46:32.153581Z",
     "iopub.status.busy": "2025-11-02T18:46:32.153474Z",
     "iopub.status.idle": "2025-11-02T18:46:32.964784Z",
     "shell.execute_reply": "2025-11-02T18:46:32.964476Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features from real EEG data...\n",
      "  Processed 46/468 files...\n",
      "  Processed 92/468 files...\n",
      "  Processed 138/468 files...\n",
      "  Processed 184/468 files...\n",
      "  Processed 230/468 files...\n",
      "  Processed 276/468 files...\n",
      "  Processed 322/468 files...\n",
      "  Processed 368/468 files...\n",
      "  Processed 414/468 files...\n",
      "  Processed 460/468 files...\n",
      "  Processed 468/468 files...\n",
      "Total epochs extracted : 2340\n",
      "Filtered epochs        : 2340\n",
      "Feature shape          : (2340, 5)\n",
      "Signal shape           : (2340, 256)\n",
      "Class distribution (filtered): Counter({1: 1175, 0: 1165})\n",
      "Condition distribution (filtered): Counter({'S1': 800, 'S2_match': 795, 'S2_nomatch': 745})\n",
      "Buckets (class, condition): Counter({(1, 'S2_match'): 400, (0, 'S1'): 400, (1, 'S1'): 400, (0, 'S2_match'): 395, (1, 'S2_nomatch'): 375, (0, 'S2_nomatch'): 370})\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def extract_band_power(signal_data, fs=256, bands=None):\n",
    "    \"\"\"Extract power in each frequency band using Welch's method.\"\"\"\n",
    "    if bands is None:\n",
    "        bands = FREQUENCY_BANDS\n",
    "    freqs, psd = signal.welch(signal_data, fs=fs, nperseg=min(256, len(signal_data)))\n",
    "    band_powers = {}\n",
    "    for band_name, (low_freq, high_freq) in bands.items():\n",
    "        idx = np.logical_and(freqs >= low_freq, freqs <= high_freq)\n",
    "        band_power = np.trapz(psd[idx], freqs[idx]) if np.any(idx) else 0.0\n",
    "        band_powers[band_name] = float(band_power)\n",
    "    return band_powers\n",
    "\n",
    "MAX_CHANNELS = 5\n",
    "MAX_TRIALS = 2\n",
    "MIN_SAMPLES_PER_EPOCH = 128\n",
    "EPOCH_LENGTH = 256\n",
    "\n",
    "def _normalize_condition_token(value):\n",
    "    if value is None or (isinstance(value, float) and np.isnan(value)):\n",
    "        return 'UNKNOWN'\n",
    "    token = str(value).strip().lower().replace(',', '')\n",
    "    if not token:\n",
    "        return 'UNKNOWN'\n",
    "    if token.startswith('s1'):\n",
    "        return 'S1'\n",
    "    compact = token.replace(' ', '')\n",
    "    if 'nomatch' in compact or 'no-match' in compact:\n",
    "        return 'S2_nomatch'\n",
    "    if token.startswith('s2') or 'match' in token:\n",
    "        return 'S2_match'\n",
    "    return 'UNKNOWN'\n",
    "\n",
    "def get_condition_token(row):\n",
    "    \"\"\"Map a trial row to {S1, S2_match, S2_nomatch} tokens.\"\"\"\n",
    "    for key in ['condition', 'matching condition', 'stimulus']:\n",
    "        if key in row and pd.notna(row[key]):\n",
    "            token = _normalize_condition_token(row[key])\n",
    "            if token != 'UNKNOWN':\n",
    "                return token\n",
    "    stimulus = str(row.get('stimulus', '')).strip().upper()\n",
    "    if stimulus == 'S1':\n",
    "        return 'S1'\n",
    "    if stimulus == 'S2':\n",
    "        match_val = row.get('match', row.get('matching', 0))\n",
    "        try:\n",
    "            match_flag = int(match_val)\n",
    "        except (TypeError, ValueError):\n",
    "            match_flag = 0\n",
    "        return 'S2_match' if match_flag == 1 else 'S2_nomatch'\n",
    "    return 'UNKNOWN'\n",
    "\n",
    "all_features = []\n",
    "all_signals = []\n",
    "all_labels = []\n",
    "all_conditions = []\n",
    "all_groups = []\n",
    "\n",
    "print('Extracting features from real EEG data...')\n",
    "progress_interval = max(1, len(sample_files) // 10) if sample_files else 1\n",
    "for file_idx, file in enumerate(sample_files):\n",
    "    df = pd.read_csv(file)\n",
    "    subject_type = df['subject identifier'].iloc[0]\n",
    "\n",
    "    channels = df['sensor position'].unique()[:MAX_CHANNELS]\n",
    "    trials = df['trial number'].unique()[:MAX_TRIALS]\n",
    "\n",
    "    for channel in channels:\n",
    "        for trial in trials:\n",
    "            trial_data = df[\n",
    "                (df['sensor position'] == channel) &\n",
    "                (df['trial number'] == trial)\n",
    "            ].sort_values('sample num')\n",
    "\n",
    "            if len(trial_data) < MIN_SAMPLES_PER_EPOCH:\n",
    "                continue\n",
    "\n",
    "            signal_data = trial_data['sensor value'].values[:EPOCH_LENGTH]\n",
    "            band_powers = extract_band_power(signal_data)\n",
    "            cond_token = get_condition_token(trial_data.iloc[0])\n",
    "\n",
    "            all_features.append(list(band_powers.values()))\n",
    "            all_signals.append(signal_data)\n",
    "            all_labels.append(1 if subject_type == 'a' else 0)\n",
    "            all_conditions.append(cond_token)\n",
    "            all_groups.append(subject_type)\n",
    "\n",
    "    if ((file_idx + 1) % progress_interval == 0) or (file_idx + 1 == len(sample_files)):\n",
    "        print(f\"  Processed {file_idx + 1}/{len(sample_files)} files...\")\n",
    "\n",
    "all_features = np.array(all_features)\n",
    "all_signals = np.array(all_signals)\n",
    "all_labels = np.array(all_labels, dtype=int)\n",
    "all_conditions = np.array(all_conditions)\n",
    "all_groups = np.array(all_groups)\n",
    "\n",
    "if all_features.size == 0:\n",
    "    raise RuntimeError('No epochs extracted. Check dataset paths or filters.')\n",
    "\n",
    "group_mask = np.isin(all_groups, list(GROUP_FILTER))\n",
    "condition_mask = np.ones_like(all_conditions, dtype=bool)\n",
    "if CONDITION_FILTER:\n",
    "    condition_mask = np.isin(all_conditions, list(CONDITION_FILTER))\n",
    "selection_mask = group_mask & condition_mask\n",
    "if not np.any(selection_mask):\n",
    "    raise RuntimeError('Filters removed all data. Loosen GROUP_FILTER/CONDITION_FILTER.')\n",
    "\n",
    "real_features = all_features[selection_mask]\n",
    "real_signals = all_signals[selection_mask]\n",
    "labels = all_labels[selection_mask]\n",
    "cond_tokens = all_conditions[selection_mask]\n",
    "group_tokens = all_groups[selection_mask]\n",
    "\n",
    "FULL_DATASET = {\n",
    "    'features': all_features,\n",
    "    'labels': all_labels,\n",
    "    'conditions': all_conditions,\n",
    "    'groups': all_groups,\n",
    "}\n",
    "\n",
    "FILTERED_DATASET = {\n",
    "    'features': real_features,\n",
    "    'signals': real_signals,\n",
    "    'labels': labels,\n",
    "    'conditions': cond_tokens,\n",
    "    'groups': group_tokens,\n",
    "}\n",
    "\n",
    "print(f\"Total epochs extracted : {len(all_features)}\")\n",
    "print(f\"Filtered epochs        : {len(real_features)}\")\n",
    "print(f\"Feature shape          : {real_features.shape}\")\n",
    "print(f\"Signal shape           : {real_signals.shape}\")\n",
    "print('Class distribution (filtered):', Counter(labels.tolist()))\n",
    "print('Condition distribution (filtered):', Counter(cond_tokens.tolist()))\n",
    "print('Buckets (class, condition):', Counter(list(zip(labels.tolist(), cond_tokens.tolist()))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Method 1: Correlation Sampling Approach\n",
    "\n",
    "Based on \"A Statistical Approach for Synthetic EEG Data Generation\"\n",
    "\n",
    "Steps:\n",
    "1. Compute correlation matrix of frequency band features\n",
    "2. Sample from multivariate normal distribution preserving correlations\n",
    "3. Generate synthetic features matching real data statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T18:46:32.966524Z",
     "iopub.status.busy": "2025-11-02T18:46:32.966415Z",
     "iopub.status.idle": "2025-11-02T18:46:32.970196Z",
     "shell.execute_reply": "2025-11-02T18:46:32.969965Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nGenerating 2340 synthetic samples...\n",
      "Correlation Matrix of Frequency Bands:\n",
      "Delta  - Delta :  1.000\n",
      "Delta  - Theta :  0.570\n",
      "Delta  - Alpha :  0.025\n",
      "Delta  - Beta  : -0.048\n",
      "Delta  - Gamma : -0.057\n",
      "Theta  - Theta :  1.000\n",
      "Theta  - Alpha :  0.352\n",
      "Theta  - Beta  : -0.019\n",
      "Theta  - Gamma : -0.010\n",
      "Alpha  - Alpha :  1.000\n",
      "Alpha  - Beta  :  0.041\n",
      "Alpha  - Gamma :  0.042\n",
      "Beta   - Beta  :  1.000\n",
      "Beta   - Gamma :  0.952\n",
      "Gamma  - Gamma :  1.000\n",
      "\\nGenerated 2340 synthetic feature vectors\n",
      "Correlation structure preserved\n",
      "Shape of synthetic features: (2340, 5)\n"
     ]
    }
   ],
   "source": [
    "def generate_correlation_based_eeg(real_features, n_synthetic=100, random_seed=42):\n",
    "    \"\"\"\n",
    "    Generate synthetic EEG using correlation sampling method\n",
    "    \n",
    "    This preserves the correlation structure between frequency bands\n",
    "    \"\"\"\n",
    "    # Set random seed for reproducibility\n",
    "    np.random.seed(random_seed)\n",
    "    \n",
    "    # Compute correlation matrix and statistics\n",
    "    correlation_matrix = np.corrcoef(real_features.T)\n",
    "    mean_features = np.mean(real_features, axis=0)\n",
    "    std_features = np.std(real_features, axis=0)\n",
    "    \n",
    "    print(\"Correlation Matrix of Frequency Bands:\")\n",
    "    band_names = list(FREQUENCY_BANDS.keys())\n",
    "    for i, band1 in enumerate(band_names):\n",
    "        for j, band2 in enumerate(band_names):\n",
    "            if j >= i:\n",
    "                print(f\"{band1:6s} - {band2:6s}: {correlation_matrix[i,j]:6.3f}\")\n",
    "    \n",
    "    # Generate synthetic features preserving correlation structure\n",
    "    covariance_matrix = np.outer(std_features, std_features) * correlation_matrix\n",
    "    \n",
    "    synthetic_features = np.random.multivariate_normal(\n",
    "        mean_features,\n",
    "        covariance_matrix,\n",
    "        size=n_synthetic\n",
    "    )\n",
    "    \n",
    "    # Ensure non-negative powers\n",
    "    synthetic_features = np.abs(synthetic_features)\n",
    "    \n",
    "    print(f\"\\\\nGenerated {n_synthetic} synthetic feature vectors\")\n",
    "    print(f\"Correlation structure preserved\")\n",
    "    \n",
    "    return synthetic_features, correlation_matrix\n",
    "\n",
    "# Generate synthetic data\n",
    "n_synthetic_samples = len(real_features)\n",
    "print(f\"\\\\nGenerating {n_synthetic_samples} synthetic samples...\")\n",
    "\n",
    "synthetic_features_corr, corr_matrix = generate_correlation_based_eeg(\n",
    "    real_features,\n",
    "    n_synthetic=n_synthetic_samples,\n",
    "    random_seed=RANDOM_SEED\n",
    ")\n",
    "\n",
    "print(f\"Shape of synthetic features: {synthetic_features_corr.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Method 2: Neural Generators\n",
    "\n",
    "We benchmark two neural approaches in addition to the correlation sampler:\n",
    "\n",
    "1. **Mixup Baseline** – deterministic interpolation + Gaussian jitter (no adversary) for a quick sanity check.\n",
    "2. **WGAN-GP** – fully adversarial training with gradient penalty (Gulrajani et al., 2017) implemented in PyTorch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T18:46:32.971372Z",
     "iopub.status.busy": "2025-11-02T18:46:32.971299Z",
     "iopub.status.idle": "2025-11-02T18:46:32.984472Z",
     "shell.execute_reply": "2025-11-02T18:46:32.984242Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating synthetic data using mixup baseline...\n",
      "Generated 2340 synthetic samples\n",
      "Shape: (2340, 5)\n"
     ]
    }
   ],
   "source": [
    "def generate_mixup_baseline(real_features, n_synthetic=100, random_seed=42):\n",
    "    \"\"\"\n",
    "    Mixup-style baseline using:\n",
    "    - Interpolation between randomly sampled real points\n",
    "    - Addition of controlled gaussian noise\n",
    "    \n",
    "    Serves as a lightweight reference model prior to adversarial training.\n",
    "    \"\"\"\n",
    "    np.random.seed(random_seed)\n",
    "\n",
    "    synthetic_features = []\n",
    "\n",
    "    for _ in range(n_synthetic):\n",
    "        idx1, idx2 = np.random.choice(len(real_features), 2, replace=False)\n",
    "        alpha = np.random.uniform(0.3, 0.7)\n",
    "\n",
    "        interpolated = alpha * real_features[idx1] + (1 - alpha) * real_features[idx2]\n",
    "\n",
    "        noise_scale = 0.1 * np.std(real_features, axis=0)\n",
    "        noise = np.random.normal(0, noise_scale)\n",
    "        synthetic_sample = interpolated + noise\n",
    "\n",
    "        synthetic_sample = np.abs(synthetic_sample)\n",
    "        synthetic_features.append(synthetic_sample)\n",
    "\n",
    "    return np.array(synthetic_features)\n",
    "\n",
    "print(\"Generating synthetic data using mixup baseline...\")\n",
    "synthetic_features_mixup = generate_mixup_baseline(\n",
    "    real_features,\n",
    "    n_synthetic=n_synthetic_samples,\n",
    "    random_seed=RANDOM_SEED\n",
    ")\n",
    "\n",
    "print(f\"Generated {len(synthetic_features_mixup)} synthetic samples\")\n",
    "print(f\"Shape: {synthetic_features_mixup.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training WGAN-GP generator (this may take a couple of minutes)...\n",
      "Epoch 050/300 | D: -22.4700 | G: 58.2359 | preview mean=[8.895 8.354 5.571 8.843 4.666]\n",
      "Epoch 100/300 | D: -30.9784 | G: 88.3384 | preview mean=[20.698  5.326  4.246  7.774  0.747]\n",
      "Epoch 150/300 | D: -45.1265 | G: 156.7274 | preview mean=[28.294  7.204  5.142  5.656  1.996]\n",
      "Epoch 200/300 | D: -42.9474 | G: 277.4485 | preview mean=[26.767  5.771  5.429  4.627  2.58 ]\n",
      "Epoch 250/300 | D: -57.5499 | G: 426.3501 | preview mean=[24.252  6.067  5.075  6.867  2.8  ]\n",
      "Epoch 300/300 | D: -80.9785 | G: 641.9765 | preview mean=[27.753  6.476  5.238  5.901  1.241]\n",
      "Generated 2340 WGAN-GP samples\n",
      "Shape: (2340, 5)\n"
     ]
    }
   ],
   "source": [
    "# --- WGAN-GP generator for frequency-band features ----------------------------------\n",
    "\n",
    "def generate_wgangp_eeg(\n",
    "    real_features,\n",
    "    n_synthetic=100,\n",
    "    noise_dim=16,\n",
    "    hidden_dim=64,\n",
    "    n_critic=5,\n",
    "    gp_lambda=10.0,\n",
    "    lr=1e-4,\n",
    "    batch_size=128,\n",
    "    epochs=300,\n",
    "    random_seed=42,\n",
    "):\n",
    "    \"\"\"Train a compact WGAN-GP on band-power features and return synthetic samples.\"\"\"\n",
    "    if torch is None:\n",
    "        raise ImportError(\n",
    "            \"PyTorch is required for WGAN-GP synthesis. Install torch>=2.0 to enable this path.\"\n",
    "        )\n",
    "\n",
    "    from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "    torch.manual_seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    data = torch.from_numpy(real_features.astype(np.float32))\n",
    "    dataset = TensorDataset(data)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "    feature_dim = real_features.shape[1]\n",
    "\n",
    "    class Generator(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.net = nn.Sequential(\n",
    "                nn.Linear(noise_dim, hidden_dim),\n",
    "                nn.LayerNorm(hidden_dim),\n",
    "                nn.GELU(),\n",
    "                nn.Linear(hidden_dim, hidden_dim),\n",
    "                nn.GELU(),\n",
    "                nn.Linear(hidden_dim, feature_dim),\n",
    "            )\n",
    "\n",
    "        def forward(self, z):\n",
    "            x = self.net(z)\n",
    "            return torch.nn.functional.softplus(x)\n",
    "\n",
    "    class Critic(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.net = nn.Sequential(\n",
    "                nn.Linear(feature_dim, hidden_dim),\n",
    "                nn.LayerNorm(hidden_dim),\n",
    "                nn.GELU(),\n",
    "                nn.Linear(hidden_dim, hidden_dim),\n",
    "                nn.GELU(),\n",
    "                nn.Linear(hidden_dim, 1),\n",
    "            )\n",
    "\n",
    "        def forward(self, x):\n",
    "            return self.net(x)\n",
    "\n",
    "    def gradient_penalty(critic, real, fake):\n",
    "        batch_size = real.size(0)\n",
    "        epsilon = torch.rand(batch_size, 1, device=real.device)\n",
    "        epsilon = epsilon.expand_as(real)\n",
    "        interpolated = epsilon * real + (1 - epsilon) * fake\n",
    "        interpolated.requires_grad_(True)\n",
    "        mixed_scores = critic(interpolated)\n",
    "        grad = torch.autograd.grad(\n",
    "            outputs=mixed_scores,\n",
    "            inputs=interpolated,\n",
    "            grad_outputs=torch.ones_like(mixed_scores),\n",
    "            create_graph=True,\n",
    "            retain_graph=True,\n",
    "        )[0]\n",
    "        grad = grad.view(batch_size, -1)\n",
    "        gp = ((grad.norm(2, dim=1) - 1) ** 2).mean()\n",
    "        return gp\n",
    "\n",
    "    G = Generator().to(device)\n",
    "    D = Critic().to(device)\n",
    "\n",
    "    opt_G = optim.Adam(G.parameters(), lr=lr, betas=(0.5, 0.9))\n",
    "    opt_D = optim.Adam(D.parameters(), lr=lr, betas=(0.5, 0.9))\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for i, (real_batch,) in enumerate(loader):\n",
    "            real_batch = real_batch.to(device)\n",
    "\n",
    "            for _ in range(n_critic):\n",
    "                z = torch.randn(real_batch.size(0), noise_dim, device=device)\n",
    "                fake_batch = G(z).detach()\n",
    "\n",
    "                opt_D.zero_grad()\n",
    "                critic_real = D(real_batch).mean()\n",
    "                critic_fake = D(fake_batch).mean()\n",
    "                gp = gradient_penalty(D, real_batch, fake_batch)\n",
    "                loss_D = -(critic_real - critic_fake) + gp_lambda * gp\n",
    "                loss_D.backward()\n",
    "                opt_D.step()\n",
    "\n",
    "            z = torch.randn(real_batch.size(0), noise_dim, device=device)\n",
    "            opt_G.zero_grad()\n",
    "            fake_batch = G(z)\n",
    "            loss_G = -D(fake_batch).mean()\n",
    "            loss_G.backward()\n",
    "            opt_G.step()\n",
    "\n",
    "        if (epoch + 1) % 50 == 0:\n",
    "            with torch.no_grad():\n",
    "                z = torch.randn(batch_size, noise_dim, device=device)\n",
    "                preview = G(z).cpu().numpy()\n",
    "            preview_mean = preview.mean(axis=0)\n",
    "            print(\n",
    "                f\"Epoch {epoch + 1:03d}/{epochs} | D: {loss_D.item():.4f} | G: {loss_G.item():.4f} | preview mean={preview_mean.round(3)}\"\n",
    "            )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        synth_chunks = []\n",
    "        remaining = n_synthetic\n",
    "        while remaining > 0:\n",
    "            current = min(batch_size, remaining)\n",
    "            z = torch.randn(current, noise_dim, device=device)\n",
    "            synth = G(z).cpu().numpy()\n",
    "            synth_chunks.append(synth)\n",
    "            remaining -= current\n",
    "\n",
    "    synthetic = np.vstack(synth_chunks)\n",
    "    synthetic = np.clip(synthetic, a_min=0.0, a_max=None)\n",
    "    return synthetic\n",
    "\n",
    "print(\"Training WGAN-GP generator (this may take a couple of minutes)...\")\n",
    "if torch is None:\n",
    "    print(\"PyTorch not installed; skipping WGAN-GP synthesis.\")\n",
    "    synthetic_features_wgangp = None\n",
    "else:\n",
    "    synthetic_features_wgangp = generate_wgangp_eeg(\n",
    "        real_features,\n",
    "        n_synthetic=n_synthetic_samples,\n",
    "        random_seed=RANDOM_SEED,\n",
    "    )\n",
    "    print(f\"Generated {len(synthetic_features_wgangp)} WGAN-GP samples\")\n",
    "    print(f\"Shape: {synthetic_features_wgangp.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluation: Distribution Comparison (KS Test & MMD)\n",
    "\n",
    "Statistical tests to compare real vs synthetic data distributions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T18:46:32.985832Z",
     "iopub.status.busy": "2025-11-02T18:46:32.985759Z",
     "iopub.status.idle": "2025-11-02T18:46:32.994970Z",
     "shell.execute_reply": "2025-11-02T18:46:32.994718Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\n============================================================\n",
      "Distribution Comparison: Correlation Sampling\n",
      "============================================================\n",
      "\\nKolmogorov-Smirnov Test Results:\n",
      "(p-value > 0.05 suggests distributions are similar)\n",
      "  Delta   : KS=0.3573, p=0.0000 ✗ Different\n",
      "  Theta   : KS=0.2269, p=0.0000 ✗ Different\n",
      "  Alpha   : KS=0.3338, p=0.0000 ✗ Different\n",
      "  Beta    : KS=0.5679, p=0.0000 ✗ Different\n",
      "  Gamma   : KS=0.9350, p=0.0000 ✗ Different\n",
      "\n",
      "MMD (RBF) Score: 0.722064\n",
      "(Lower MMD indicates more similar distributions)\n",
      "\\n============================================================\n",
      "Distribution Comparison: Mixup Baseline\n",
      "============================================================\n",
      "\\nKolmogorov-Smirnov Test Results:\n",
      "(p-value > 0.05 suggests distributions are similar)\n",
      "  Delta   : KS=0.1385, p=0.0000 ✗ Different\n",
      "  Theta   : KS=0.1440, p=0.0000 ✗ Different\n",
      "  Alpha   : KS=0.1321, p=0.0000 ✗ Different\n",
      "  Beta    : KS=0.1154, p=0.0000 ✗ Different\n",
      "  Gamma   : KS=0.8192, p=0.0000 ✗ Different\n",
      "\n",
      "MMD (RBF) Score: 0.325714\n",
      "(Lower MMD indicates more similar distributions)\n",
      "\\n============================================================\n",
      "Distribution Comparison: WGAN-GP\n",
      "============================================================\n",
      "\\nKolmogorov-Smirnov Test Results:\n",
      "(p-value > 0.05 suggests distributions are similar)\n",
      "  Delta   : KS=0.1077, p=0.0000 ✗ Different\n",
      "  Theta   : KS=0.0829, p=0.0000 ✗ Different\n",
      "  Alpha   : KS=0.1893, p=0.0000 ✗ Different\n",
      "  Beta    : KS=0.0709, p=0.0000 ✗ Different\n",
      "  Gamma   : KS=0.5073, p=0.0000 ✗ Different\n",
      "\n",
      "MMD (RBF) Score: 0.021267\n",
      "(Lower MMD indicates more similar distributions)\n"
     ]
    }
   ],
   "source": [
    "def evaluate_distributions(real_features, synthetic_features, method_name=\"\"):\n",
    "    \"\"\"\n",
    "    Compare distributions using:\n",
    "    - KS test (Kolmogorov-Smirnov): tests if distributions are from same population\n",
    "    - MMD (Maximum Mean Discrepancy): measures distance between distributions\n",
    "    \"\"\"\n",
    "    print(f\"\\\\n{'='*60}\")\n",
    "    print(f\"Distribution Comparison: {method_name}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # KS test for each feature (frequency band)\n",
    "    band_names = list(FREQUENCY_BANDS.keys())\n",
    "    ks_results = []\n",
    "    \n",
    "    print(\"\\\\nKolmogorov-Smirnov Test Results:\")\n",
    "    print(\"(p-value > 0.05 suggests distributions are similar)\")\n",
    "    for i, band in enumerate(band_names):\n",
    "        ks_stat, p_value = stats.ks_2samp(real_features[:, i], synthetic_features[:, i])\n",
    "        ks_results.append({'band': band, 'ks_stat': ks_stat, 'p_value': p_value})\n",
    "        \n",
    "        significance = \"✓ Similar\" if p_value > 0.05 else \"✗ Different\"\n",
    "        print(f\"  {band:8s}: KS={ks_stat:.4f}, p={p_value:.4f} {significance}\")\n",
    "    \n",
    "    # Simplified MMD computation\n",
    "    def compute_mmd(X, Y):\n",
    "        \"\"\"Maximum Mean Discrepancy using pairwise distances\"\"\"\n",
    "        XX = cdist(X, X, metric='euclidean')\n",
    "        YY = cdist(Y, Y, metric='euclidean')\n",
    "        XY = cdist(X, Y, metric='euclidean')\n",
    "        \n",
    "        mmd = np.mean(XX) + np.mean(YY) - 2 * np.mean(XY)\n",
    "        return mmd\n",
    "    \n",
    "    mmd_score = rbf_mmd(real_features, synthetic_features)\n",
    "    print(f\"\\nMMD (RBF) Score: {mmd_score:.6f}\")\n",
    "    print(\"(Lower MMD indicates more similar distributions)\")\n",
    "\n",
    "    \n",
    "    return ks_results, mmd_score\n",
    "\n",
    "distribution_results = {}\n",
    "\n",
    "distribution_results['Correlation Sampling'] = evaluate_distributions(\n",
    "    real_features,\n",
    "    synthetic_features_corr,\n",
    "    \"Correlation Sampling\"\n",
    ")\n",
    "\n",
    "distribution_results['Mixup Baseline'] = evaluate_distributions(\n",
    "    real_features,\n",
    "    synthetic_features_mixup,\n",
    "    \"Mixup Baseline\"\n",
    ")\n",
    "\n",
    "if synthetic_features_wgangp is not None:\n",
    "    distribution_results['WGAN-GP'] = evaluate_distributions(\n",
    "        real_features,\n",
    "        synthetic_features_wgangp,\n",
    "        \"WGAN-GP\"\n",
    "    )\n",
    "else:\n",
    "    print(\"Skipping WGAN-GP distribution metrics (PyTorch unavailable).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluation: TSTR and TRTR\n",
    "\n",
    "**TRTR** = Train on Real, Test on Real  \n",
    "**TSTR** = Train on Synthetic, Test on Real\n",
    "\n",
    "If TSTR ≈ TRTR, synthetic data quality is high\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T18:46:32.996301Z",
     "iopub.status.busy": "2025-11-02T18:46:32.996222Z",
     "iopub.status.idle": "2025-11-02T18:46:33.209791Z",
     "shell.execute_reply": "2025-11-02T18:46:33.209571Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nEvaluating Correlation Sampling Method:\n",
      "\\n============================================================\n",
      "TSTR/TRTR Evaluation: Correlation\n",
      "============================================================\n",
      "\\n1. TRTR (Train on Real, Test on Real):\n",
      "   Accuracy: 0.6966\n",
      "\\n2. TSTR (Train on Synthetic, Test on Real):\n",
      "   Accuracy: 0.3561\n",
      "\\n3. Performance Comparison:\n",
      "   TRTR: 0.6966\n",
      "   TSTR: 0.3561\n",
      "   Difference: 0.3405\n",
      "   ✗ Synthetic data quality: NEEDS IMPROVEMENT\n",
      "\\n============================================================\n",
      "Evaluating Mixup Baseline:\n",
      "\\n============================================================\n",
      "TSTR/TRTR Evaluation: Mixup Baseline\n",
      "============================================================\n",
      "\\n1. TRTR (Train on Real, Test on Real):\n",
      "   Accuracy: 0.6966\n",
      "\\n2. TSTR (Train on Synthetic, Test on Real):\n",
      "   Accuracy: 0.4758\n",
      "\\n3. Performance Comparison:\n",
      "   TRTR: 0.6966\n",
      "   TSTR: 0.4758\n",
      "   Difference: 0.2208\n",
      "   ✗ Synthetic data quality: NEEDS IMPROVEMENT\n",
      "\\n============================================================\n",
      "Evaluating WGAN-GP Method:\n",
      "\\n============================================================\n",
      "TSTR/TRTR Evaluation: WGAN-GP\n",
      "============================================================\n",
      "\\n1. TRTR (Train on Real, Test on Real):\n",
      "   Accuracy: 0.6966\n",
      "\\n2. TSTR (Train on Synthetic, Test on Real):\n",
      "   Accuracy: 0.5370\n",
      "\\n3. Performance Comparison:\n",
      "   TRTR: 0.6966\n",
      "   TSTR: 0.5370\n",
      "   Difference: 0.1595\n",
      "   ✗ Synthetic data quality: NEEDS IMPROVEMENT\n"
     ]
    }
   ],
   "source": [
    "def evaluate_tstr_trtr(real_features, real_labels, synthetic_features, method_name=\"\"):\n",
    "    \"\"\"\n",
    "    TSTR/TRTR Evaluation from literature\n",
    "    \n",
    "    Validates synthetic data by comparing model performance when:\n",
    "    - Training on real vs synthetic data\n",
    "    - Testing on real data\n",
    "    \"\"\"\n",
    "    print(f\"\\\\n{'='*60}\")\n",
    "    print(f\"TSTR/TRTR Evaluation: {method_name}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Split real data\n",
    "    X_train_real, X_test_real, y_train_real, y_test_real = train_test_split(\n",
    "        real_features, real_labels, test_size=0.3, random_state=RANDOM_SEED\n",
    "    )\n",
    "    \n",
    "    # Create synthetic labels matching real distribution\n",
    "    n_alcoholic = np.sum(y_train_real == 1)\n",
    "    n_control = np.sum(y_train_real == 0)\n",
    "    y_synthetic = np.concatenate([\n",
    "        np.ones(min(n_alcoholic, len(synthetic_features)//2)),\n",
    "        np.zeros(min(n_control, len(synthetic_features)//2))\n",
    "    ])\n",
    "    X_synthetic = synthetic_features[:len(y_synthetic)]\n",
    "    \n",
    "    # TRTR: Train on Real, Test on Real\n",
    "    print(\"\\\\n1. TRTR (Train on Real, Test on Real):\")\n",
    "    clf_trtr = RandomForestClassifier(n_estimators=100, random_state=RANDOM_SEED)\n",
    "    clf_trtr.fit(X_train_real, y_train_real)\n",
    "    y_pred_trtr = clf_trtr.predict(X_test_real)\n",
    "    acc_trtr = accuracy_score(y_test_real, y_pred_trtr)\n",
    "    print(f\"   Accuracy: {acc_trtr:.4f}\")\n",
    "    \n",
    "    # TSTR: Train on Synthetic, Test on Real\n",
    "    print(\"\\\\n2. TSTR (Train on Synthetic, Test on Real):\")\n",
    "    clf_tstr = RandomForestClassifier(n_estimators=100, random_state=RANDOM_SEED)\n",
    "    clf_tstr.fit(X_synthetic, y_synthetic)\n",
    "    y_pred_tstr = clf_tstr.predict(X_test_real)\n",
    "    acc_tstr = accuracy_score(y_test_real, y_pred_tstr)\n",
    "    print(f\"   Accuracy: {acc_tstr:.4f}\")\n",
    "    \n",
    "    # Compare\n",
    "    print(f\"\\\\n3. Performance Comparison:\")\n",
    "    print(f\"   TRTR: {acc_trtr:.4f}\")\n",
    "    print(f\"   TSTR: {acc_tstr:.4f}\")\n",
    "    print(f\"   Difference: {abs(acc_trtr - acc_tstr):.4f}\")\n",
    "    \n",
    "    if abs(acc_trtr - acc_tstr) < 0.05:\n",
    "        print(\"   ✓ Synthetic data quality: EXCELLENT\")\n",
    "    elif abs(acc_trtr - acc_tstr) < 0.10:\n",
    "        print(\"   ✓ Synthetic data quality: GOOD\")\n",
    "    else:\n",
    "        print(\"   ✗ Synthetic data quality: NEEDS IMPROVEMENT\")\n",
    "    \n",
    "    return acc_trtr, acc_tstr\n",
    "\n",
    "# Evaluate both methods\n",
    "print(\"\\\\nEvaluating Correlation Sampling Method:\")\n",
    "acc_trtr_corr, acc_tstr_corr = evaluate_tstr_trtr(\n",
    "    real_features, labels, synthetic_features_corr, \"Correlation\"\n",
    ")\n",
    "\n",
    "print(\"\\\\n\" + \"=\"*60)\n",
    "print(\"Evaluating Mixup Baseline:\")\n",
    "acc_trtr_mix, acc_tstr_mix = evaluate_tstr_trtr(\n",
    "    real_features, labels, synthetic_features_mixup, \"Mixup Baseline\"\n",
    ")\n",
    "\n",
    "if synthetic_features_wgangp is not None:\n",
    "    print(\"\\\\n\" + \"=\"*60)\n",
    "    print(\"Evaluating WGAN-GP Method:\")\n",
    "    acc_trtr_wgan, acc_tstr_wgan = evaluate_tstr_trtr(\n",
    "        real_features, labels, synthetic_features_wgangp, \"WGAN-GP\"\n",
    "    )\n",
    "else:\n",
    "    print(\"\\\\nSkipping WGAN-GP TSTR/TRTR (PyTorch unavailable).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Evaluation: Real vs Synthetic Classification\n",
    "\n",
    "Train classifier to distinguish real from synthetic.  \n",
    "**Goal**: Classifier should perform at ~50% (chance level) if synthetic data is indistinguishable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T18:46:33.211115Z",
     "iopub.status.busy": "2025-11-02T18:46:33.211035Z",
     "iopub.status.idle": "2025-11-02T18:46:33.354935Z",
     "shell.execute_reply": "2025-11-02T18:46:33.354667Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Real vs Synthetic Classification: Correlation Sampling\n",
      "============================================================\n",
      "\n",
      "Classifier Accuracy: 0.9900\n",
      "✗ POOR: Classifier easily distinguishes real from synthetic\n",
      "\n",
      "Confusion Matrix:\n",
      "                 Pred Real  Pred Synthetic\n",
      "Actual Real:         700            2\n",
      "Actual Synthetic:     12          690\n",
      "\n",
      "============================================================\n",
      "Real vs Synthetic Classification: Mixup Baseline\n",
      "============================================================\n",
      "\n",
      "Classifier Accuracy: 0.9338\n",
      "✗ POOR: Classifier easily distinguishes real from synthetic\n",
      "\n",
      "Confusion Matrix:\n",
      "                 Pred Real  Pred Synthetic\n",
      "Actual Real:         673           29\n",
      "Actual Synthetic:     64          638\n",
      "\n",
      "============================================================\n",
      "Real vs Synthetic Classification: WGAN-GP\n",
      "============================================================\n",
      "\n",
      "Classifier Accuracy: 0.9217\n",
      "✗ POOR: Classifier easily distinguishes real from synthetic\n",
      "\n",
      "Confusion Matrix:\n",
      "                 Pred Real  Pred Synthetic\n",
      "Actual Real:         642           60\n",
      "Actual Synthetic:     50          652\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "def evaluate_real_vs_synthetic(real_features, synthetic_features, method_name=\"\"):\n",
    "    \"\"\"Train a RF to distinguish real (1) vs synthetic (0); print metrics.\"\"\"\n",
    "    if real_features.size == 0 or synthetic_features.size == 0:\n",
    "        raise ValueError(\"Both real and synthetic feature matrices must be non-empty.\")\n",
    "\n",
    "    import numpy as np\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"Real vs Synthetic Classification: {method_name}\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    X = np.vstack([real_features, synthetic_features])\n",
    "    y = np.concatenate([np.ones(len(real_features), dtype=int),\n",
    "                        np.zeros(len(synthetic_features), dtype=int)])\n",
    "\n",
    "    Xtr, Xte, ytr, yte = train_test_split(\n",
    "        X, y, test_size=0.30, random_state=RANDOM_SEED, stratify=y\n",
    "    )\n",
    "\n",
    "    clf = RandomForestClassifier(n_estimators=100, random_state=RANDOM_SEED, class_weight=\"balanced\")\n",
    "    clf.fit(Xtr, ytr)\n",
    "    yhat = clf.predict(Xte)\n",
    "\n",
    "    acc = accuracy_score(yte, yhat)\n",
    "    print(f\"\\nClassifier Accuracy: {acc:.4f}\")\n",
    "    if 0.45 <= acc <= 0.55:\n",
    "        print(\"✓ EXCELLENT: Classifier at chance level (50%)\")\n",
    "        print(\"  → Synthetic data indistinguishable from real\")\n",
    "    elif 0.40 <= acc <= 0.60:\n",
    "        print(\"✓ GOOD: Classifier struggles to distinguish\")\n",
    "    else:\n",
    "        print(\"✗ POOR: Classifier easily distinguishes real from synthetic\")\n",
    "\n",
    "    # fix the printed layout by pinning label order\n",
    "    cm = confusion_matrix(yte, yhat, labels=[1,0])  # rows: Actual Real, Actual Synthetic\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(\"                 Pred Real  Pred Synthetic\")\n",
    "    print(f\"Actual Real:        {cm[0,0]:4d}         {cm[0,1]:4d}\")\n",
    "    print(f\"Actual Synthetic:   {cm[1,0]:4d}         {cm[1,1]:4d}\")\n",
    "    return acc\n",
    "\n",
    "acc_corr = evaluate_real_vs_synthetic(\n",
    "    real_features, synthetic_features_corr, \"Correlation Sampling\"\n",
    ")\n",
    "\n",
    "acc_mix = evaluate_real_vs_synthetic(\n",
    "    real_features, synthetic_features_mixup, \"Mixup Baseline\"\n",
    ")\n",
    "\n",
    "acc_wgan = None\n",
    "if synthetic_features_wgangp is not None:\n",
    "    acc_wgan = evaluate_real_vs_synthetic(\n",
    "        real_features, synthetic_features_wgangp, \"WGAN-GP\"\n",
    "    )\n",
    "else:\n",
    "    print(\"Skipping WGAN-GP detectability test (PyTorch unavailable).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Clustering Alignment\n",
    "\n",
    "Assess how mixed real/synthetic embeddings cluster and whether synthetic data preserves real cluster structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------\n",
      "Clustering alignment summary: Correlation Sampling\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>k</th>\n",
       "      <th>NMI(real_vs_flag)</th>\n",
       "      <th>ARI(real_vs_flag)</th>\n",
       "      <th>Silhouette(mixed)</th>\n",
       "      <th>CentroidGap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0.3139</td>\n",
       "      <td>0.2105</td>\n",
       "      <td>0.7173</td>\n",
       "      <td>1425.6092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>0.3959</td>\n",
       "      <td>0.3253</td>\n",
       "      <td>0.6867</td>\n",
       "      <td>1175.5667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>0.4407</td>\n",
       "      <td>0.3948</td>\n",
       "      <td>0.6645</td>\n",
       "      <td>1025.1884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>0.4547</td>\n",
       "      <td>0.4210</td>\n",
       "      <td>0.6512</td>\n",
       "      <td>963.2729</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   k  NMI(real_vs_flag)  ARI(real_vs_flag)  Silhouette(mixed)  CentroidGap\n",
       "0  3             0.3139             0.2105             0.7173    1425.6092\n",
       "1  4             0.3959             0.3253             0.6867    1175.5667\n",
       "2  5             0.4407             0.3948             0.6645    1025.1884\n",
       "3  6             0.4547             0.4210             0.6512     963.2729"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>k</th>\n",
       "      <th>NMI(real_vs_flag)</th>\n",
       "      <th>ARI(real_vs_flag)</th>\n",
       "      <th>Silhouette(mixed)</th>\n",
       "      <th>CentroidGap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.5</td>\n",
       "      <td>0.4013</td>\n",
       "      <td>0.3379</td>\n",
       "      <td>0.6799</td>\n",
       "      <td>1147.4093</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        k  NMI(real_vs_flag)  ARI(real_vs_flag)  Silhouette(mixed)  \\\n",
       "mean  4.5             0.4013             0.3379             0.6799   \n",
       "\n",
       "      CentroidGap  \n",
       "mean    1147.4093  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------\n",
      "Clustering alignment summary: Mixup Baseline\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>k</th>\n",
       "      <th>NMI(real_vs_flag)</th>\n",
       "      <th>ARI(real_vs_flag)</th>\n",
       "      <th>Silhouette(mixed)</th>\n",
       "      <th>CentroidGap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0360</td>\n",
       "      <td>0.0006</td>\n",
       "      <td>0.9704</td>\n",
       "      <td>1227.8755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0356</td>\n",
       "      <td>0.0004</td>\n",
       "      <td>0.9632</td>\n",
       "      <td>1399.3514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>0.0299</td>\n",
       "      <td>0.0004</td>\n",
       "      <td>0.7166</td>\n",
       "      <td>1133.0229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>0.0298</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>0.7165</td>\n",
       "      <td>956.6479</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   k  NMI(real_vs_flag)  ARI(real_vs_flag)  Silhouette(mixed)  CentroidGap\n",
       "0  3             0.0360             0.0006             0.9704    1227.8755\n",
       "1  4             0.0356             0.0004             0.9632    1399.3514\n",
       "2  5             0.0299             0.0004             0.7166    1133.0229\n",
       "3  6             0.0298             0.0003             0.7165     956.6479"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>k</th>\n",
       "      <th>NMI(real_vs_flag)</th>\n",
       "      <th>ARI(real_vs_flag)</th>\n",
       "      <th>Silhouette(mixed)</th>\n",
       "      <th>CentroidGap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.5</td>\n",
       "      <td>0.0328</td>\n",
       "      <td>0.0004</td>\n",
       "      <td>0.8417</td>\n",
       "      <td>1179.2244</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        k  NMI(real_vs_flag)  ARI(real_vs_flag)  Silhouette(mixed)  \\\n",
       "mean  4.5             0.0328             0.0004             0.8417   \n",
       "\n",
       "      CentroidGap  \n",
       "mean    1179.2244  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------\n",
      "Clustering alignment summary: WGAN-GP\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>k</th>\n",
       "      <th>NMI(real_vs_flag)</th>\n",
       "      <th>ARI(real_vs_flag)</th>\n",
       "      <th>Silhouette(mixed)</th>\n",
       "      <th>CentroidGap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0093</td>\n",
       "      <td>8.0618e-05</td>\n",
       "      <td>0.7772</td>\n",
       "      <td>1549.3855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0166</td>\n",
       "      <td>4.3475e-04</td>\n",
       "      <td>0.7300</td>\n",
       "      <td>1185.9961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>0.0131</td>\n",
       "      <td>1.3053e-03</td>\n",
       "      <td>0.6167</td>\n",
       "      <td>972.6475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>0.0162</td>\n",
       "      <td>5.7373e-03</td>\n",
       "      <td>0.4968</td>\n",
       "      <td>838.3828</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   k  NMI(real_vs_flag)  ARI(real_vs_flag)  Silhouette(mixed)  CentroidGap\n",
       "0  3             0.0093         8.0618e-05             0.7772    1549.3855\n",
       "1  4             0.0166         4.3475e-04             0.7300    1185.9961\n",
       "2  5             0.0131         1.3053e-03             0.6167     972.6475\n",
       "3  6             0.0162         5.7373e-03             0.4968     838.3828"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>k</th>\n",
       "      <th>NMI(real_vs_flag)</th>\n",
       "      <th>ARI(real_vs_flag)</th>\n",
       "      <th>Silhouette(mixed)</th>\n",
       "      <th>CentroidGap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.5</td>\n",
       "      <td>0.0138</td>\n",
       "      <td>0.0019</td>\n",
       "      <td>0.6552</td>\n",
       "      <td>1136.603</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        k  NMI(real_vs_flag)  ARI(real_vs_flag)  Silhouette(mixed)  \\\n",
       "mean  4.5             0.0138             0.0019             0.6552   \n",
       "\n",
       "      CentroidGap  \n",
       "mean     1136.603  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def clustering_alignment_report(real_features, synthetic_features, method_name, cluster_list=(3, 4, 5, 6)):\n",
    "    combined = np.vstack([real_features, synthetic_features])\n",
    "    is_synth = np.concatenate([\n",
    "        np.zeros(len(real_features), dtype=int),\n",
    "        np.ones(len(synthetic_features), dtype=int),\n",
    "    ])\n",
    "\n",
    "    records = []\n",
    "    for k in cluster_list:\n",
    "        km = KMeans(n_clusters=k, n_init=10, random_state=RANDOM_SEED)\n",
    "        cluster_labels = km.fit_predict(combined)\n",
    "\n",
    "        nmi = normalized_mutual_info_score(is_synth, cluster_labels)\n",
    "        ari = adjusted_rand_score(is_synth, cluster_labels)\n",
    "        sil = silhouette_score(combined, cluster_labels)\n",
    "\n",
    "        km_real = KMeans(n_clusters=k, n_init=10, random_state=RANDOM_SEED)\n",
    "        km_synth = KMeans(n_clusters=k, n_init=10, random_state=RANDOM_SEED)\n",
    "        km_real.fit(real_features)\n",
    "        km_synth.fit(synthetic_features)\n",
    "        centroid_cost = cdist(km_real.cluster_centers_, km_synth.cluster_centers_)\n",
    "        ri, ci = linear_sum_assignment(centroid_cost)\n",
    "        centroid_gap = centroid_cost[ri, ci].mean()\n",
    "\n",
    "        records.append(\n",
    "            {\n",
    "                'k': k,\n",
    "                'NMI(real_vs_flag)': nmi,\n",
    "                'ARI(real_vs_flag)': ari,\n",
    "                'Silhouette(mixed)': sil,\n",
    "                'CentroidGap': centroid_gap,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    df = pd.DataFrame(records)\n",
    "    print(f\"\\n{'-'*70}\\nClustering alignment summary: {method_name}\\n{'-'*70}\")\n",
    "    display(df)\n",
    "    display(df.mean().to_frame(name='mean').T)\n",
    "    return df\n",
    "\n",
    "clustering_reports = {}\n",
    "clustering_reports['Correlation Sampling'] = clustering_alignment_report(\n",
    "    real_features,\n",
    "    synthetic_features_corr,\n",
    "    'Correlation Sampling'\n",
    ")\n",
    "clustering_reports['Mixup Baseline'] = clustering_alignment_report(\n",
    "    real_features,\n",
    "    synthetic_features_mixup,\n",
    "    'Mixup Baseline'\n",
    ")\n",
    "if synthetic_features_wgangp is not None:\n",
    "    clustering_reports['WGAN-GP'] = clustering_alignment_report(\n",
    "        real_features,\n",
    "        synthetic_features_wgangp,\n",
    "        'WGAN-GP'\n",
    "    )\n",
    "else:\n",
    "    print(\"Skipping WGAN-GP clustering analysis (PyTorch unavailable).\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 PERMANOVA\n",
    "\n",
    "Permutation-based multivariate ANOVA to test for distributional differences between real and synthetic samples (pooled and per condition).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scikit-bio is not installed; skipping PERMANOVA for Correlation Sampling\n",
      "scikit-bio is not installed; skipping PERMANOVA for Mixup Baseline\n",
      "scikit-bio is not installed; skipping PERMANOVA for WGAN-GP\n"
     ]
    }
   ],
   "source": [
    "def permanova_test(real_features, synthetic_features, method_name, permutations=999):\n",
    "    try:\n",
    "        from skbio import DistanceMatrix\n",
    "        from skbio.stats.distance import permanova\n",
    "    except ImportError:\n",
    "        print(\"scikit-bio is not installed; skipping PERMANOVA for\", method_name)\n",
    "        return None\n",
    "\n",
    "    combined = np.vstack([real_features, synthetic_features])\n",
    "    ids = [f\"real_{i}\" for i in range(len(real_features))] + [\n",
    "        f\"synthetic_{i}\" for i in range(len(synthetic_features))\n",
    "    ]\n",
    "    grouping = pd.Series(\n",
    "        ['real'] * len(real_features) + ['synthetic'] * len(synthetic_features),\n",
    "        index=ids\n",
    "    )\n",
    "    distance_matrix = cdist(combined, combined, metric='euclidean')\n",
    "    dm = DistanceMatrix(distance_matrix, ids=ids)\n",
    "\n",
    "    result = permanova(dm, grouping, permutations=permutations)\n",
    "    print(f\"\\nPERMANOVA ({method_name})\")\n",
    "    print(result)\n",
    "    return result\n",
    "\n",
    "permanova_results = {}\n",
    "permanova_results['Correlation Sampling'] = permanova_test(\n",
    "    real_features,\n",
    "    synthetic_features_corr,\n",
    "    'Correlation Sampling'\n",
    ")\n",
    "permanova_results['Mixup Baseline'] = permanova_test(\n",
    "    real_features,\n",
    "    synthetic_features_mixup,\n",
    "    'Mixup Baseline'\n",
    ")\n",
    "if synthetic_features_wgangp is not None:\n",
    "    permanova_results['WGAN-GP'] = permanova_test(\n",
    "        real_features,\n",
    "        synthetic_features_wgangp,\n",
    "        'WGAN-GP'\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Improved Synthetic Generation Strategies\n",
    "\n",
    "To reduce real-vs-synthetic separability while preserving frequency-band correlations, we explore two additional generators:\n",
    "\n",
    "1. **Gaussian Copula Sampling**: preserves empirical marginals per class and matches correlation structure in a latent Gaussian space.\n",
    "2. **Class-Conditional Interpolation**: SMOTE-like synthesis operating in log-power space with adaptive neighborhood mixing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _allocate_samples_by_class(labels, n_total):\n",
    "    \"\"\"Allocate synthetic samples per class, preserving empirical ratios.\"\"\"\n",
    "    classes, counts = np.unique(labels, return_counts=True)\n",
    "    ratios = counts / counts.sum()\n",
    "    expected = ratios * n_total\n",
    "    allocated = np.floor(expected).astype(int)\n",
    "    remainder = n_total - allocated.sum()\n",
    "    if remainder > 0:\n",
    "        remainders = expected - allocated\n",
    "        order = np.argsort(remainders)[::-1]\n",
    "        for idx in order[:remainder]:\n",
    "            allocated[idx] += 1\n",
    "    return dict(zip(classes, allocated))\n",
    "\n",
    "def generate_gaussian_copula_eeg(real_features, labels, n_synthetic=100, random_seed=42):\n",
    "    \"\"\"\n",
    "    Gaussian copula sampling:\n",
    "    1. Fit class-conditional quantile transformers to map marginals to Gaussian space\n",
    "    2. Estimate regularised covariance (Ledoit-Wolf) in latent space\n",
    "    3. Sample multivariate normal per class and invert the transform\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(random_seed)\n",
    "    allocation = _allocate_samples_by_class(labels, n_synthetic)\n",
    "    synthetic_blocks = []\n",
    "\n",
    "    print(\"Generating Gaussian copula samples per class...\")\n",
    "    for cls, n_cls_samples in allocation.items():\n",
    "        class_features = real_features[labels == cls]\n",
    "        if len(class_features) == 0 or n_cls_samples == 0:\n",
    "            continue\n",
    "\n",
    "        n_quantiles = min(len(class_features), 1000)\n",
    "        transformer = QuantileTransformer(\n",
    "            n_quantiles=n_quantiles,\n",
    "            output_distribution='normal',\n",
    "            random_state=random_seed\n",
    "        )\n",
    "        latent = transformer.fit_transform(class_features)\n",
    "\n",
    "        cov_estimator = LedoitWolf().fit(latent)\n",
    "        latent_mean = cov_estimator.location_\n",
    "        latent_cov = cov_estimator.covariance_\n",
    "\n",
    "        latent_samples = rng.multivariate_normal(\n",
    "            latent_mean,\n",
    "            latent_cov,\n",
    "            size=n_cls_samples\n",
    "        )\n",
    "\n",
    "        samples = transformer.inverse_transform(latent_samples)\n",
    "        samples = np.clip(samples, a_min=0, a_max=None)\n",
    "        synthetic_blocks.append(samples)\n",
    "\n",
    "        print(f\"  Class {cls}: real={len(class_features)}, synthetic={n_cls_samples}\")\n",
    "\n",
    "    if not synthetic_blocks:\n",
    "        raise ValueError(\"No synthetic samples were generated. Check class labels.\")\n",
    "\n",
    "    synthetic_features = np.vstack(synthetic_blocks)\n",
    "    return synthetic_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Gaussian copula samples per class...\n",
      "  Class 0: real=1165, synthetic=1165\n",
      "  Class 1: real=1175, synthetic=1175\n",
      "\n",
      "Generated 2340 Gaussian copula samples\n",
      "Shape: (2340, 5)\n"
     ]
    }
   ],
   "source": [
    "synthetic_features_copula = generate_gaussian_copula_eeg(\n",
    "    real_features,\n",
    "    labels,\n",
    "    n_synthetic=n_synthetic_samples,\n",
    "    random_seed=RANDOM_SEED\n",
    ")\n",
    "\n",
    "print(f\"\\nGenerated {len(synthetic_features_copula)} Gaussian copula samples\")\n",
    "print(f\"Shape: {synthetic_features_copula.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_classwise_interpolation_eeg(\n",
    "    real_features,\n",
    "    labels,\n",
    "    n_synthetic=100,\n",
    "    random_seed=42,\n",
    "    k_neighbors=8,\n",
    "    noise_scale=0.02\n",
    "):\n",
    "    \"\"\"\n",
    "    Class-conditional interpolation inspired by SMOTE.\n",
    "    Operates in log-power space to better capture multiplicative structure.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(random_seed)\n",
    "    allocation = _allocate_samples_by_class(labels, n_synthetic)\n",
    "    synthetic_samples = []\n",
    "\n",
    "    log_features = np.log1p(real_features)\n",
    "\n",
    "    for cls, n_cls_samples in allocation.items():\n",
    "        class_mask = labels == cls\n",
    "        class_features_log = log_features[class_mask]\n",
    "        if len(class_features_log) == 0 or n_cls_samples == 0:\n",
    "            continue\n",
    "\n",
    "        n_neighbors_eff = min(k_neighbors, len(class_features_log) - 1)\n",
    "        if n_neighbors_eff <= 0:\n",
    "            # Not enough samples to interpolate, fallback to jittering existing ones\n",
    "            base_samples = np.repeat(class_features_log, repeats=max(1, n_cls_samples // max(1, len(class_features_log))), axis=0)\n",
    "            base_samples = base_samples[:n_cls_samples]\n",
    "            jitter = rng.normal(0, noise_scale, size=base_samples.shape)\n",
    "            augmented = base_samples + jitter\n",
    "            synthetic_samples.append(np.expm1(augmented))\n",
    "            continue\n",
    "\n",
    "        nbrs = NearestNeighbors(n_neighbors=n_neighbors_eff + 1)\n",
    "        nbrs.fit(class_features_log)\n",
    "        class_std = np.std(class_features_log, axis=0, ddof=1)\n",
    "        class_std[class_std == 0] = 1e-6\n",
    "\n",
    "        for _ in range(n_cls_samples):\n",
    "            idx = rng.integers(len(class_features_log))\n",
    "            neighbors = nbrs.kneighbors(class_features_log[idx].reshape(1, -1), return_distance=False)[0]\n",
    "            neighbors = neighbors[neighbors != idx]\n",
    "            if len(neighbors) == 0:\n",
    "                neighbor_idx = idx\n",
    "            else:\n",
    "                neighbor_idx = rng.choice(neighbors)\n",
    "\n",
    "            alpha = rng.uniform(0.2, 0.8)\n",
    "            interpolated = (\n",
    "                alpha * class_features_log[idx] +\n",
    "                (1 - alpha) * class_features_log[neighbor_idx]\n",
    "            )\n",
    "\n",
    "            noise = rng.normal(0, noise_scale, size=class_features_log.shape[1]) * class_std\n",
    "            synthetic_log = interpolated + noise\n",
    "            synthetic_samples.append(np.expm1(synthetic_log))\n",
    "\n",
    "    if not synthetic_samples:\n",
    "        raise ValueError(\"Interpolation generator did not create any samples.\")\n",
    "\n",
    "    synthetic_features = np.vstack(synthetic_samples)\n",
    "    synthetic_features = np.clip(synthetic_features, a_min=0, a_max=None)\n",
    "    return synthetic_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated 2340 interpolation-based samples\n",
      "Shape: (2340, 5)\n"
     ]
    }
   ],
   "source": [
    "synthetic_features_interp = generate_classwise_interpolation_eeg(\n",
    "    real_features,\n",
    "    labels,\n",
    "    n_synthetic=n_synthetic_samples,\n",
    "    random_seed=RANDOM_SEED,\n",
    "    k_neighbors=10, \n",
    "    noise_scale=0.015\n",
    ")\n",
    "\n",
    "print(f\"\\nGenerated {len(synthetic_features_interp)} interpolation-based samples\")\n",
    "print(f\"Shape: {synthetic_features_interp.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1 Distribution and Quality Checks\n",
    "\n",
    "Re-run the statistical and downstream evaluations for the new generators alongside previous baselines.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\n============================================================\n",
      "Distribution Comparison: Gaussian Copula\n",
      "============================================================\n",
      "\\nKolmogorov-Smirnov Test Results:\n",
      "(p-value > 0.05 suggests distributions are similar)\n",
      "  Delta   : KS=0.0192, p=0.7800 ✓ Similar\n",
      "  Theta   : KS=0.0184, p=0.8245 ✓ Similar\n",
      "  Alpha   : KS=0.0184, p=0.8245 ✓ Similar\n",
      "  Beta    : KS=0.0192, p=0.7800 ✓ Similar\n",
      "  Gamma   : KS=0.0167, p=0.9013 ✓ Similar\n",
      "\n",
      "MMD (RBF) Score: 0.000844\n",
      "(Lower MMD indicates more similar distributions)\n",
      "\\n============================================================\n",
      "Distribution Comparison: Classwise Interpolation\n",
      "============================================================\n",
      "\\nKolmogorov-Smirnov Test Results:\n",
      "(p-value > 0.05 suggests distributions are similar)\n",
      "  Delta   : KS=0.0201, p=0.7328 ✓ Similar\n",
      "  Theta   : KS=0.0295, p=0.2609 ✓ Similar\n",
      "  Alpha   : KS=0.0278, p=0.3274 ✓ Similar\n",
      "  Beta    : KS=0.0235, p=0.5378 ✓ Similar\n",
      "  Gamma   : KS=0.0333, p=0.1485 ✓ Similar\n",
      "\n",
      "MMD (RBF) Score: 0.000430\n",
      "(Lower MMD indicates more similar distributions)\n"
     ]
    }
   ],
   "source": [
    "ks_copula, mmd_copula = evaluate_distributions(\n",
    "    real_features,\n",
    "    synthetic_features_copula,\n",
    "    \"Gaussian Copula\"\n",
    ")\n",
    "\n",
    "ks_interp, mmd_interp = evaluate_distributions(\n",
    "    real_features,\n",
    "    synthetic_features_interp,\n",
    "    \"Classwise Interpolation\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Gaussian Copula Method:\n",
      "\\n============================================================\n",
      "TSTR/TRTR Evaluation: Gaussian Copula\n",
      "============================================================\n",
      "\\n1. TRTR (Train on Real, Test on Real):\n",
      "   Accuracy: 0.6966\n",
      "\\n2. TSTR (Train on Synthetic, Test on Real):\n",
      "   Accuracy: 0.3490\n",
      "\\n3. Performance Comparison:\n",
      "   TRTR: 0.6966\n",
      "   TSTR: 0.3490\n",
      "   Difference: 0.3476\n",
      "   ✗ Synthetic data quality: NEEDS IMPROVEMENT\n",
      "\n",
      "============================================================\n",
      "Evaluating Classwise Interpolation Method:\n",
      "\\n============================================================\n",
      "TSTR/TRTR Evaluation: Classwise Interpolation\n",
      "============================================================\n",
      "\\n1. TRTR (Train on Real, Test on Real):\n",
      "   Accuracy: 0.6966\n",
      "\\n2. TSTR (Train on Synthetic, Test on Real):\n",
      "   Accuracy: 0.3134\n",
      "\\n3. Performance Comparison:\n",
      "   TRTR: 0.6966\n",
      "   TSTR: 0.3134\n",
      "   Difference: 0.3832\n",
      "   ✗ Synthetic data quality: NEEDS IMPROVEMENT\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nEvaluating Gaussian Copula Method:\")\n",
    "acc_trtr_copula, acc_tstr_copula = evaluate_tstr_trtr(\n",
    "    real_features,\n",
    "    labels,\n",
    "    synthetic_features_copula,\n",
    "    \"Gaussian Copula\"\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Evaluating Classwise Interpolation Method:\")\n",
    "acc_trtr_interp, acc_tstr_interp = evaluate_tstr_trtr(\n",
    "    real_features,\n",
    "    labels,\n",
    "    synthetic_features_interp,\n",
    "    \"Classwise Interpolation\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Real vs Synthetic Classification: Gaussian Copula\n",
      "============================================================\n",
      "\n",
      "Classifier Accuracy: 0.5491\n",
      "✓ EXCELLENT: Classifier at chance level (50%)\n",
      "  → Synthetic data indistinguishable from real\n",
      "\n",
      "Confusion Matrix:\n",
      "                 Pred Real  Pred Synthetic\n",
      "Actual Real:         374          328\n",
      "Actual Synthetic:    305          397\n",
      "\n",
      "============================================================\n",
      "Real vs Synthetic Classification: Classwise Interpolation\n",
      "============================================================\n",
      "\n",
      "Classifier Accuracy: 0.4309\n",
      "✓ GOOD: Classifier struggles to distinguish\n",
      "\n",
      "Confusion Matrix:\n",
      "                 Pred Real  Pred Synthetic\n",
      "Actual Real:         256          446\n",
      "Actual Synthetic:    353          349\n"
     ]
    }
   ],
   "source": [
    "acc_copula_sep = evaluate_real_vs_synthetic(\n",
    "    real_features,\n",
    "    synthetic_features_copula,\n",
    "    \"Gaussian Copula\"\n",
    ")\n",
    "\n",
    "acc_interp_sep = evaluate_real_vs_synthetic(\n",
    "    real_features,\n",
    "    synthetic_features_interp,\n",
    "    \"Classwise Interpolation\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Early-stop: targets met\n",
      "  Params: {0: 5, 1: 10} {0: 0.0, 1: 0.0}\n",
      "  Metrics: {'rvs_acc': 0.36186770428015563, 'tstr': 0.8408796895213454, 'trtr': 0.6998706338939198, 'gap': 0.1410090556274256, 'ks_min_p': 0.3261561157352818, 'mmd': 0.0004134147321114279, 'corr_sim': 0.995270237210689, 'rho_min': -0.03868685042240337, 'rho_mean': -0.002450164703319186}\n",
      "\n",
      "=== BEST COMBINATION ===\n",
      "k_params: {0: 5, 1: 5} noise_params: {0: 0.0, 1: 0.0}\n",
      "metrics : {'rvs_acc': 0.33787289234760054, 'tstr': 0.8745148771021992, 'trtr': 0.6998706338939198, 'gap': 0.17464424320827943, 'ks_min_p': 0.4868306191329178, 'mmd': 0.000296001036291349, 'corr_sim': 0.9921030346539852, 'rho_min': -0.03970450882614366, 'rho_mean': -0.002404962460893624}\n"
     ]
    }
   ],
   "source": [
    "# === Synthetic EEG Tuner: per-class (k, noise) grid + early-stopping ===\n",
    "# Paste this in one cell. Assumes you have:\n",
    "#   real_features:  (N, 5) band-power features (Delta..Gamma)\n",
    "#   labels:         (N,)   class labels {0,1}\n",
    "# Modify BAND_NAMES if needed.\n",
    "\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.covariance import LedoitWolf\n",
    "\n",
    "# -----------------------------\n",
    "# Config / Targets\n",
    "# -----------------------------\n",
    "BAND_NAMES = [\"Delta\",\"Theta\",\"Alpha\",\"Beta\",\"Gamma\"]\n",
    "CLASS_VALUES = np.unique(labels)\n",
    "assert set(CLASS_VALUES) == {0,1}, \"This tuner assumes binary classes {0,1}.\"\n",
    "\n",
    "PARAM_GRID_K = [5, 8, 10, 12, 15]\n",
    "PARAM_GRID_NOISE = [0.00, 0.01, 0.015, 0.02, 0.03]\n",
    "\n",
    "# Targets (edit to taste)\n",
    "TARGET_RVS_MAX = 0.60     # real vs synthetic accuracy <= 0.60\n",
    "TARGET_TSTR_MIN = 0.70    # TSTR >= 0.70\n",
    "TARGET_GAP_MAX  = 0.15    # |TRTR - TSTR| <= 0.15\n",
    "TARGET_KS_MINP  = 0.05    # per-band KS p-value > 0.05\n",
    "TARGET_CORR_SIM = 0.90    # corr-matrix similarity >= 0.90\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# -----------------------------\n",
    "# Helpers: metrics\n",
    "# -----------------------------\n",
    "def ks_pvals_per_band(real, synth):\n",
    "    pvals = []\n",
    "    for j in range(real.shape[1]):\n",
    "        _, p = stats.ks_2samp(real[:, j], synth[:, j])\n",
    "        pvals.append(p)\n",
    "    return np.array(pvals)\n",
    "\n",
    "def rbf_mmd(X, Y, gamma=None):\n",
    "    \"\"\"RBF-kernel MMD (biased) with median heuristic for gamma.\"\"\"\n",
    "    Z = np.vstack([X, Y])\n",
    "    # median heuristic\n",
    "    if gamma is None:\n",
    "        D2 = np.sum((Z[:,None,:]-Z[None,:,:])**2, axis=2)\n",
    "        med2 = np.median(D2[D2>0])\n",
    "        gamma = 1.0 / (med2 + 1e-8)\n",
    "\n",
    "    def k(a,b):\n",
    "        D2 = np.sum((a[:,None,:]-b[None,:,:])**2, axis=2)\n",
    "        return np.exp(-gamma * D2)\n",
    "\n",
    "    m, n = X.shape[0], Y.shape[0]\n",
    "    Kxx = k(X,X); Kyy = k(Y,Y); Kxy = k(X,Y)\n",
    "    return float(Kxx.mean() + Kyy.mean() - 2.0*Kxy.mean())\n",
    "\n",
    "\n",
    "def corr_matrix_similarity(real, synth):\n",
    "    C_real = np.corrcoef(real.T)\n",
    "    C_synth = np.corrcoef(synth.T)\n",
    "    iu = np.triu_indices_from(C_real, k=1)\n",
    "    r = np.corrcoef(C_real[iu], C_synth[iu])[0,1]\n",
    "    return r\n",
    "\n",
    "def per_band_spearman(real, synth, random_seed=RANDOM_SEED, max_samples=5000):\n",
    "    if real.size == 0 or synth.size == 0:\n",
    "        return np.full(real.shape[1], np.nan)\n",
    "\n",
    "    rng = np.random.default_rng(random_seed)\n",
    "    n = min(len(real), len(synth), max_samples)\n",
    "\n",
    "    if len(real) > n:\n",
    "        idx_real = rng.choice(len(real), size=n, replace=False)\n",
    "    else:\n",
    "        idx_real = np.arange(len(real))\n",
    "\n",
    "    if len(synth) > n:\n",
    "        idx_synth = rng.choice(len(synth), size=n, replace=False)\n",
    "    else:\n",
    "        idx_synth = np.arange(len(synth))\n",
    "\n",
    "    real_sample = real[idx_real]\n",
    "    synth_sample = synth[idx_synth]\n",
    "\n",
    "    vals = []\n",
    "    for j in range(real.shape[1]):\n",
    "        rho = stats.spearmanr(real_sample[:, j], synth_sample[:, j]).correlation\n",
    "        vals.append(rho)\n",
    "    return np.array(vals)\n",
    "\n",
    "def real_vs_synth_accuracy(real, synth):\n",
    "    X = np.vstack([real, synth])\n",
    "    y = np.hstack([np.zeros(len(real), dtype=int), np.ones(len(synth), dtype=int)])\n",
    "    Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.33, random_state=RANDOM_SEED, stratify=y)\n",
    "    clf = RandomForestClassifier(n_estimators=200, random_state=RANDOM_SEED, class_weight=\"balanced\")\n",
    "    clf.fit(Xtr, ytr)\n",
    "    return clf.score(Xte, yte)\n",
    "\n",
    "def tstr_trtr_accuracy(real_X, real_y, synth_X, synth_y):\n",
    "    # Classifier: RF; TSTR = train on synthetic, test on real; TRTR = train on real, test on real\n",
    "    # Split real set for fair TRTR eval\n",
    "    Xtr_r, Xte_r, ytr_r, yte_r = train_test_split(real_X, real_y, test_size=0.33, random_state=RANDOM_SEED, stratify=real_y)\n",
    "    # TRTR\n",
    "    clf_r = RandomForestClassifier(n_estimators=300, random_state=RANDOM_SEED, class_weight=\"balanced\")\n",
    "    clf_r.fit(Xtr_r, ytr_r)\n",
    "    trtr = clf_r.score(Xte_r, yte_r)\n",
    "    # TSTR\n",
    "    clf_s = RandomForestClassifier(n_estimators=300, random_state=RANDOM_SEED, class_weight=\"balanced\")\n",
    "    clf_s.fit(synth_X, synth_y)\n",
    "    tstr = clf_s.score(Xte_r, yte_r)\n",
    "    return tstr, trtr\n",
    "\n",
    "# -----------------------------\n",
    "# Generator: classwise interpolation in log-space\n",
    "# (replace with your own generate_classwise_interpolation_eeg_oneclass if you have it)\n",
    "# -----------------------------\n",
    "def _interp_one_class(Xc, n_out, k_neighbors=10, noise_scale=0.015, random_state=42):\n",
    "    \"\"\"\n",
    "    Classwise interpolation in log-space with covariance-aware jitter.\n",
    "    Robust to small class sizes; ensures k>=2 and <= len(Xc)-1.\n",
    "    \"\"\"\n",
    "    rng = np.random.RandomState(random_state)\n",
    "    eps = 1e-8\n",
    "    Xc = np.asarray(Xc)\n",
    "    if Xc.ndim != 2 or Xc.shape[0] == 0:\n",
    "        raise ValueError(\"Xc must be (n_samples, n_features) with n_samples>0\")\n",
    "\n",
    "    # If the class is too small, just jitter existing points\n",
    "    if Xc.shape[0] == 1:\n",
    "        # log → jitter → exp\n",
    "        xlog = np.log(Xc + eps)\n",
    "        synth_log = np.repeat(xlog, n_out, axis=0)\n",
    "        # fallback covariance: identity\n",
    "        jitter = rng.normal(size=(n_out, Xc.shape[1]))\n",
    "        synth_log = synth_log + noise_scale * jitter\n",
    "        return np.exp(synth_log) - eps\n",
    "\n",
    "    # Work in log-space to keep positivity on exp back-transform\n",
    "    Xlog = np.log(Xc + eps)\n",
    "\n",
    "    # Choose a valid k (at least 2, at most n-1)\n",
    "    n_samp = Xlog.shape[0]\n",
    "    k = int(np.clip(k_neighbors, 2, max(2, n_samp - 1)))\n",
    "\n",
    "    # Fit neighbors in log-space\n",
    "    nbrs = NearestNeighbors(n_neighbors=k, metric='euclidean').fit(Xlog)\n",
    "\n",
    "    # Pick base points uniformly\n",
    "    base_idx = rng.randint(0, n_samp, size=n_out)\n",
    "    base = Xlog[base_idx]\n",
    "\n",
    "    # For each base, pick one neighbor randomly (excluding self is handled by k<=n-1)\n",
    "    neigh_idx = nbrs.kneighbors(base, return_distance=False)\n",
    "    pick = neigh_idx[np.arange(n_out), rng.randint(0, neigh_idx.shape[1], size=n_out)]\n",
    "    neigh = Xlog[pick]\n",
    "\n",
    "    # Random convex combination in [0,1]\n",
    "    alpha = rng.rand(n_out, 1)\n",
    "    synth_log = alpha * base + (1 - alpha) * neigh\n",
    "\n",
    "    # Covariance-aware jitter using Ledoit–Wolf (robust); fallback to diagonal if needed\n",
    "    if noise_scale and noise_scale > 0:\n",
    "        try:\n",
    "            lw = LedoitWolf().fit(Xlog)\n",
    "            S = lw.covariance_\n",
    "            # numeric guard: ensure SPD\n",
    "            # (LedoitWolf should be SPD; add tiny ridge just in case)\n",
    "            S = S + 1e-8 * np.eye(S.shape[0])\n",
    "            jitter = rng.multivariate_normal(mean=np.zeros(Xlog.shape[1]), cov=S, size=n_out)\n",
    "        except Exception:\n",
    "            jitter = rng.normal(size=(n_out, Xlog.shape[1]))\n",
    "        synth_log = synth_log + noise_scale * jitter\n",
    "\n",
    "    synth = np.exp(synth_log) - eps\n",
    "    return synth\n",
    "\n",
    "\n",
    "def generate_classwise_interpolation_both_classes(real_X, real_y, n_per_class, k_params, noise_params):\n",
    "    # k_params = {0: k0, 1: k1}; noise_params = {0: n0, 1: n1}\n",
    "    synth_list, synth_y = [], []\n",
    "    for c in CLASS_VALUES:\n",
    "        Xc = real_X[real_y == c]\n",
    "        synth_c = _interp_one_class(\n",
    "            Xc, n_per_class,\n",
    "            k_neighbors=k_params[c],\n",
    "            noise_scale=noise_params[c],\n",
    "            random_state=RANDOM_SEED + c\n",
    "        )\n",
    "        synth_list.append(synth_c)\n",
    "        synth_y.append(np.full(n_per_class, c, dtype=int))\n",
    "    return np.vstack(synth_list), np.hstack(synth_y)\n",
    "\n",
    "# -----------------------------\n",
    "# Scoring + Early stopping\n",
    "# -----------------------------\n",
    "def score_combo(metrics):\n",
    "    # Higher is better. Penalize violations smoothly.\n",
    "    rvs = metrics[\"rvs_acc\"]\n",
    "    tstr, trtr = metrics[\"tstr\"], metrics[\"trtr\"]\n",
    "    ks_min = metrics[\"ks_min_p\"]\n",
    "    corr_sim = metrics[\"corr_sim\"]\n",
    "\n",
    "    # Base score\n",
    "    s = 0.0\n",
    "    # push RvS down toward 0.55 (reward if <= 0.60, punish otherwise)\n",
    "    s += 2.0 * max(0.0, 0.60 - rvs)\n",
    "    # reward higher TSTR and smaller gap\n",
    "    s += 1.5 * tstr\n",
    "    s += 1.0 * max(0.0, 0.15 - abs(trtr - tstr))\n",
    "    # reward KS min p and correlation similarity\n",
    "    s += 1.0 * min(ks_min, 0.10) * 10.0   # cap effect; scale to ~[0..1]\n",
    "    s += 1.0 * max(0.0, corr_sim - 0.85)  # only reward above 0.85\n",
    "    # small reward for small |MMD|\n",
    "    s += 0.5 * max(0.0, 0.2 - abs(metrics[\"mmd\"]))  # closer to 0 is better\n",
    "\n",
    "    return s\n",
    "\n",
    "def meets_targets(metrics):\n",
    "    return (metrics[\"rvs_acc\"] <= TARGET_RVS_MAX and\n",
    "            metrics[\"tstr\"]     >= TARGET_TSTR_MIN and\n",
    "            abs(metrics[\"trtr\"] - metrics[\"tstr\"]) <= TARGET_GAP_MAX and\n",
    "            metrics[\"ks_min_p\"] >= TARGET_KS_MINP and\n",
    "            metrics[\"corr_sim\"] >= TARGET_CORR_SIM)\n",
    "\n",
    "# -----------------------------\n",
    "# Grid search (per class) but evaluated jointly\n",
    "# -----------------------------\n",
    "def tune_interpolation_params(real_X, real_y, verbose=True):\n",
    "    n_per_class = min(np.sum(real_y==0), np.sum(real_y==1))  # balance\n",
    "    best = {\"score\": -np.inf, \"params\": None, \"metrics\": None}\n",
    "\n",
    "    tried = 0\n",
    "    for k0 in PARAM_GRID_K:\n",
    "        for n0 in PARAM_GRID_NOISE:\n",
    "            for k1 in PARAM_GRID_K:\n",
    "                for n1 in PARAM_GRID_NOISE:\n",
    "                    tried += 1\n",
    "                    k_params = {0: k0, 1: k1}\n",
    "                    noise_params = {0: n0, 1: n1}\n",
    "                    synth_X, synth_y = generate_classwise_interpolation_both_classes(\n",
    "                        real_X, real_y, n_per_class, k_params, noise_params\n",
    "                    )\n",
    "\n",
    "                    # Metrics\n",
    "                    ks_p = ks_pvals_per_band(real_X, synth_X)\n",
    "                    mmd = rbf_mmd(real_X, synth_X, gamma=None)\n",
    "                    rvs = real_vs_synth_accuracy(real_X, synth_X)\n",
    "                    tstr, trtr = tstr_trtr_accuracy(real_X, real_y, synth_X, synth_y)\n",
    "                    corr_sim = corr_matrix_similarity(real_X, synth_X)\n",
    "                    rho = per_band_spearman(real_X, synth_X)\n",
    "\n",
    "                    metrics = {\n",
    "                        \"rvs_acc\": rvs,\n",
    "                        \"tstr\": tstr,\n",
    "                        \"trtr\": trtr,\n",
    "                        \"gap\": abs(trtr - tstr),\n",
    "                        \"ks_min_p\": float(np.min(ks_p)),\n",
    "                        \"mmd\": float(mmd),\n",
    "                        \"corr_sim\": float(corr_sim),\n",
    "                        \"rho_min\": float(np.nanmin(rho)),\n",
    "                        \"rho_mean\": float(np.nanmean(rho)),\n",
    "                    }\n",
    "                    sc = score_combo(metrics)\n",
    "\n",
    "                    if sc > best[\"score\"]:\n",
    "                        best = {\"score\": sc, \"params\": (k_params, noise_params), \"metrics\": metrics, \n",
    "                                \"synth\": (synth_X, synth_y)}\n",
    "\n",
    "                    if verbose and tried % 20 == 0:\n",
    "                        print(f\"[{tried:4d}] k0={k0}, n0={n0:.3f} | k1={k1}, n1={n1:.3f} \"\n",
    "                              f\"RvS={rvs:.3f} TSTR/TRTR={tstr:.3f}/{trtr:.3f} \"\n",
    "                              f\"KSmin={metrics['ks_min_p']:.3f} CorrSim={corr_sim:.3f} MMD={mmd:.3f}\")\n",
    "\n",
    "                    # Early stopping: break as soon as all targets met\n",
    "                    if meets_targets(metrics):\n",
    "                        if verbose:\n",
    "                            print(\"\\n✓ Early-stop: targets met\")\n",
    "                            print(\"  Params:\", k_params, noise_params)\n",
    "                            print(\"  Metrics:\", metrics)\n",
    "                        return {\"best\": best, \"early_stop\": True}\n",
    "\n",
    "    if verbose:\n",
    "        print(\"\\nNo combo met all targets. Returning best observed.\")\n",
    "        print(\"Best params:\", best[\"params\"])\n",
    "        print(\"Best metrics:\", best[\"metrics\"])\n",
    "    return {\"best\": best, \"early_stop\": False}\n",
    "\n",
    "# -----------------------------\n",
    "# Run tuning\n",
    "# -----------------------------\n",
    "result = tune_interpolation_params(real_features, labels, verbose=True)\n",
    "\n",
    "best_params = result[\"best\"][\"params\"]\n",
    "best_metrics = result[\"best\"][\"metrics\"]\n",
    "best_synth_X, best_synth_y = result[\"best\"][\"synth\"]\n",
    "\n",
    "print(\"\\n=== BEST COMBINATION ===\")\n",
    "print(\"k_params:\", best_params[0], \"noise_params:\", best_params[1])\n",
    "print(\"metrics :\", best_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Early-stop: targets met\n",
      "  Params: {0: 5, 1: 5} {0: 0.0, 1: 0.0}\n",
      "  Metrics: {'detect': 0.5027195027195027, 'rvs_acc_raw': 0.5027195027195027, 'rvs_auc': 0.38137230242969966, 'tstr': 0.7011642949547219, 'trtr': 0.6998706338939198, 'gap': 0.001293661060802087, 'ks_min_p': 0.3416900838456176, 'mmd': 0.0012224264816664832, 'corr_sim': 0.9820198663815284, 'rho_min': 0.8251903364370348, 'rho_mean': 0.9218774646917103}\n",
      "\n",
      "=== BEST COMBINATION ===\n",
      "k_params: {0: 5, 1: 5} noise_params: {0: 0.0, 1: 0.0}\n",
      "metrics : {'detect': 0.5027195027195027, 'rvs_acc_raw': 0.5027195027195027, 'rvs_auc': 0.38137230242969966, 'tstr': 0.7011642949547219, 'trtr': 0.6998706338939198, 'gap': 0.001293661060802087, 'ks_min_p': 0.3416900838456176, 'mmd': 0.0012224264816664832, 'corr_sim': 0.9820198663815284, 'rho_min': 0.8251903364370348, 'rho_mean': 0.9218774646917103}\n"
     ]
    }
   ],
   "source": [
    "# === DROP-IN: Robust tuner with detectability, fair TSTR, matched Spearman ===\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.covariance import LedoitWolf\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "BAND_NAMES = [\"Delta\",\"Theta\",\"Alpha\",\"Beta\",\"Gamma\"]\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# Targets\n",
    "TARGET_DETECT_MAX = 0.60   # detectability = max(acc, 1-acc) <= 0.60\n",
    "TARGET_TSTR_MIN   = 0.70\n",
    "TARGET_GAP_MAX    = 0.15\n",
    "TARGET_KS_MINP    = 0.05\n",
    "TARGET_CORR_SIM   = 0.90\n",
    "\n",
    "# Grids (adjust as needed)\n",
    "PARAM_GRID_K      = [5, 8, 10, 12]\n",
    "PARAM_GRID_NOISE  = [0.00, 0.01, 0.015, 0.02]\n",
    "\n",
    "# ---------- Metrics ----------\n",
    "def ks_pvals_per_band(real, synth):\n",
    "    return np.array([stats.ks_2samp(real[:, j], synth[:, j])[1] for j in range(real.shape[1])])\n",
    "\n",
    "def rbf_mmd(X, Y, gamma=None):\n",
    "    \"\"\"RBF-kernel MMD (biased) with median heuristic for gamma.\"\"\"\n",
    "    Z = np.vstack([X, Y])\n",
    "    # median heuristic\n",
    "    if gamma is None:\n",
    "        D2 = np.sum((Z[:,None,:]-Z[None,:,:])**2, axis=2)\n",
    "        med2 = np.median(D2[D2>0])\n",
    "        gamma = 1.0 / (med2 + 1e-8)\n",
    "\n",
    "    def k(a,b):\n",
    "        D2 = np.sum((a[:,None,:]-b[None,:,:])**2, axis=2)\n",
    "        return np.exp(-gamma * D2)\n",
    "\n",
    "    m, n = X.shape[0], Y.shape[0]\n",
    "    Kxx = k(X,X); Kyy = k(Y,Y); Kxy = k(X,Y)\n",
    "    return float(Kxx.mean() + Kyy.mean() - 2.0*Kxy.mean())\n",
    "\n",
    "\n",
    "def corr_matrix_similarity(real, synth):\n",
    "    C_r, C_s = np.corrcoef(real.T), np.corrcoef(synth.T)\n",
    "    iu = np.triu_indices_from(C_r, k=1)\n",
    "    return np.corrcoef(C_r[iu], C_s[iu])[0,1]\n",
    "\n",
    "def matched_spearman(real, synth, k=1):\n",
    "    nbrs = NearestNeighbors(n_neighbors=k, metric='euclidean').fit(synth)\n",
    "    idx = nbrs.kneighbors(real, return_distance=False)[:,0]\n",
    "    return np.array([stats.spearmanr(real[:, j], synth[idx, j]).correlation for j in range(real.shape[1])])\n",
    "\n",
    "def real_vs_synth_detectability(real, synth, seed=RANDOM_SEED):\n",
    "    X = np.vstack([real, synth])\n",
    "    y = np.hstack([np.zeros(len(real), dtype=int), np.ones(len(synth), dtype=int)])\n",
    "    Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.33, random_state=seed, stratify=y)\n",
    "    clf = RandomForestClassifier(n_estimators=200, random_state=seed, class_weight=\"balanced\")\n",
    "    clf.fit(Xtr, ytr)\n",
    "    acc = clf.score(Xte, yte)\n",
    "    try:\n",
    "        proba = clf.predict_proba(Xte)[:,1]\n",
    "        auc = roc_auc_score(yte, proba)\n",
    "    except Exception:\n",
    "        auc = 0.5\n",
    "    detect = max(acc, 1.0 - acc)\n",
    "    return detect, acc, auc\n",
    "\n",
    "# ---------- Generator: classwise interpolation in log-space ----------\n",
    "def _interp_one_class(Xc, n_out, k_neighbors=10, noise_scale=0.015, random_state=RANDOM_SEED):\n",
    "    rng = np.random.RandomState(random_state)\n",
    "    eps = 1e-8\n",
    "    Xc = np.asarray(Xc)\n",
    "    if Xc.shape[0] == 1:\n",
    "        xlog = np.log(Xc + eps)\n",
    "        synth_log = np.repeat(xlog, n_out, axis=0)\n",
    "        jitter = rng.normal(size=(n_out, Xc.shape[1]))\n",
    "        synth_log = synth_log + noise_scale * jitter\n",
    "        return np.exp(synth_log) - eps\n",
    "\n",
    "    Xlog = np.log(Xc + eps)\n",
    "    n_samp = Xlog.shape[0]\n",
    "    k = int(np.clip(k_neighbors, 2, max(2, n_samp - 1)))\n",
    "    nbrs = NearestNeighbors(n_neighbors=k, metric='euclidean').fit(Xlog)\n",
    "\n",
    "    base_idx = rng.randint(0, n_samp, size=n_out)\n",
    "    base = Xlog[base_idx]\n",
    "    neigh_idx = nbrs.kneighbors(base, return_distance=False)\n",
    "    pick = neigh_idx[np.arange(n_out), rng.randint(0, neigh_idx.shape[1], size=n_out)]\n",
    "    neigh = Xlog[pick]\n",
    "\n",
    "    alpha = rng.rand(n_out, 1)\n",
    "    synth_log = alpha * base + (1 - alpha) * neigh\n",
    "\n",
    "    if noise_scale and noise_scale > 0:\n",
    "        try:\n",
    "            lw = LedoitWolf().fit(Xlog)\n",
    "            S = lw.covariance_ + 1e-8*np.eye(Xlog.shape[1])\n",
    "            jitter = rng.multivariate_normal(mean=np.zeros(Xlog.shape[1]), cov=S, size=n_out)\n",
    "        except Exception:\n",
    "            jitter = rng.normal(size=(n_out, Xlog.shape[1]))\n",
    "        synth_log = synth_log + noise_scale * jitter\n",
    "\n",
    "    return np.exp(synth_log) - eps\n",
    "\n",
    "def gen_interp_per_class(Xtr_r, ytr_r, n_per_class, k_params, noise_params):\n",
    "    synth_list, synth_y = [], []\n",
    "    for c in np.unique(ytr_r):\n",
    "        Xc = Xtr_r[ytr_r == c]\n",
    "        k_c = int(np.clip(k_params[c], 2, max(2, Xc.shape[0]-1)))\n",
    "        n_c = float(noise_params[c])\n",
    "        synth_c = _interp_one_class(Xc, n_per_class, k_neighbors=k_c, noise_scale=n_c, random_state=RANDOM_SEED + c)\n",
    "        synth_list.append(synth_c)\n",
    "        synth_y.append(np.full(n_per_class, c, dtype=int))\n",
    "    return np.vstack(synth_list), np.hstack(synth_y)\n",
    "\n",
    "# ---------- Fair TSTR/TRTR (train-only generation) ----------\n",
    "def tstr_trtr_fair(real_X, real_y, k_params, noise_params, seed=RANDOM_SEED):\n",
    "    Xtr_r, Xte_r, ytr_r, yte_r = train_test_split(real_X, real_y, test_size=0.33, random_state=seed, stratify=real_y)\n",
    "    n_per_class = min(np.sum(ytr_r==0), np.sum(ytr_r==1))\n",
    "    synth_X, synth_y = gen_interp_per_class(Xtr_r, ytr_r, n_per_class, k_params, noise_params)\n",
    "\n",
    "    clf_r = RandomForestClassifier(n_estimators=300, random_state=seed, class_weight=\"balanced\")\n",
    "    clf_r.fit(Xtr_r, ytr_r)\n",
    "    trtr = clf_r.score(Xte_r, yte_r)\n",
    "\n",
    "    clf_s = RandomForestClassifier(n_estimators=300, random_state=seed, class_weight=\"balanced\")\n",
    "    clf_s.fit(synth_X, synth_y)\n",
    "    tstr = clf_s.score(Xte_r, yte_r)\n",
    "    return tstr, trtr, (Xtr_r, Xte_r, ytr_r, yte_r), (synth_X, synth_y)\n",
    "\n",
    "# ---------- Scoring & Early-stop ----------\n",
    "def score_combo(metrics):\n",
    "    s = 0.0\n",
    "    s += 2.0 * max(0.0, 0.60 - metrics[\"detect\"])      # lower detectability is better\n",
    "    s += 1.5 * metrics[\"tstr\"]                          # higher TSTR is better\n",
    "    s += 1.0 * max(0.0, 0.15 - abs(metrics[\"trtr\"] - metrics[\"tstr\"]))  # small gap\n",
    "    s += 1.0 * min(metrics[\"ks_min_p\"], 0.10) * 10.0    # KS min p (capped)\n",
    "    s += 1.0 * max(0.0, metrics[\"corr_sim\"] - 0.85)     # corr similarity above 0.85\n",
    "    s += 0.5 * max(0.0, 0.2 - abs(metrics[\"mmd\"]))      # MMD close to 0\n",
    "    return s\n",
    "\n",
    "def meets_targets(m):\n",
    "    return (m[\"detect\"] <= TARGET_DETECT_MAX and\n",
    "            m[\"tstr\"]   >= TARGET_TSTR_MIN and\n",
    "            abs(m[\"trtr\"] - m[\"tstr\"]) <= TARGET_GAP_MAX and\n",
    "            m[\"ks_min_p\"] >= TARGET_KS_MINP and\n",
    "            m[\"corr_sim\"] >= TARGET_CORR_SIM)\n",
    "\n",
    "# ---------- Grid search ----------\n",
    "def tune_interpolation_params(real_X, real_y, verbose=True):\n",
    "    classes = np.unique(real_y)\n",
    "    assert set(classes) == {0,1}, \"Binary classes {0,1} expected.\"\n",
    "    best = {\"score\": -np.inf, \"params\": None, \"metrics\": None, \"artifacts\": None}\n",
    "    tried = 0\n",
    "\n",
    "    for k0 in PARAM_GRID_K:\n",
    "        for n0 in PARAM_GRID_NOISE:\n",
    "            for k1 in PARAM_GRID_K:\n",
    "                for n1 in PARAM_GRID_NOISE:\n",
    "                    tried += 1\n",
    "                    k_params     = {0: k0, 1: k1}\n",
    "                    noise_params = {0: n0, 1: n1}\n",
    "\n",
    "                    # fair TSTR/TRTR with train-only generation\n",
    "                    tstr, trtr, (Xtr_r, Xte_r, ytr_r, yte_r), (synth_X, synth_y) = tstr_trtr_fair(\n",
    "                        real_X, real_y, k_params, noise_params, seed=RANDOM_SEED\n",
    "                    )\n",
    "\n",
    "                    # evaluate detectability on the same real (train+test) pool vs synth\n",
    "                    detect, acc_raw, auc = real_vs_synth_detectability(real_X, synth_X, seed=RANDOM_SEED)\n",
    "                    ks_p = ks_pvals_per_band(real_X, synth_X)\n",
    "                    mmd  = rbf_mmd(real_X, synth_X)\n",
    "                    corr = corr_matrix_similarity(real_X, synth_X)\n",
    "                    rho  = matched_spearman(real_X, synth_X, k=1)\n",
    "\n",
    "                    metrics = {\n",
    "                        \"detect\": float(detect),\n",
    "                        \"rvs_acc_raw\": float(acc_raw),\n",
    "                        \"rvs_auc\": float(auc),\n",
    "                        \"tstr\": float(tstr),\n",
    "                        \"trtr\": float(trtr),\n",
    "                        \"gap\": float(abs(trtr - tstr)),\n",
    "                        \"ks_min_p\": float(np.min(ks_p)),\n",
    "                        \"mmd\": float(mmd),\n",
    "                        \"corr_sim\": float(corr),\n",
    "                        \"rho_min\": float(np.nanmin(rho)),\n",
    "                        \"rho_mean\": float(np.nanmean(rho)),\n",
    "                    }\n",
    "                    sc = score_combo(metrics)\n",
    "\n",
    "                    if sc > best[\"score\"]:\n",
    "                        best = {\"score\": sc,\n",
    "                                \"params\": (k_params, noise_params),\n",
    "                                \"metrics\": metrics,\n",
    "                                \"artifacts\": {\"synth_X\": synth_X, \"synth_y\": synth_y}}\n",
    "\n",
    "                    if verbose and tried % 20 == 0:\n",
    "                        print(f\"[{tried:4d}] k0={k0}, n0={n0:.3f} | k1={k1}, n1={n1:.3f} \"\n",
    "                              f\"Detect={metrics['detect']:.3f} (acc={metrics['rvs_acc_raw']:.3f}, AUC={metrics['rvs_auc']:.3f}) \"\n",
    "                              f\"TSTR/TRTR={tstr:.3f}/{trtr:.3f} KSmin={metrics['ks_min_p']:.3f} \"\n",
    "                              f\"CorrSim={corr:.3f} MMD={mmd:.4f} ρ_mean={metrics['rho_mean']:.3f}\")\n",
    "\n",
    "                    if meets_targets(metrics):\n",
    "                        if verbose:\n",
    "                            print(\"\\n✓ Early-stop: targets met\")\n",
    "                            print(\"  Params:\", k_params, noise_params)\n",
    "                            print(\"  Metrics:\", metrics)\n",
    "                        return {\"best\": best, \"early_stop\": True}\n",
    "\n",
    "    if verbose:\n",
    "        print(\"\\nNo combo met all targets. Returning best observed.\")\n",
    "        print(\"Best params:\", best[\"params\"])\n",
    "        print(\"Best metrics:\", best[\"metrics\"])\n",
    "    return {\"best\": best, \"early_stop\": False}\n",
    "\n",
    "# Run it\n",
    "result = tune_interpolation_params(real_features, labels, verbose=True)\n",
    "best_params  = result[\"best\"][\"params\"]\n",
    "best_metrics = result[\"best\"][\"metrics\"]\n",
    "print(\"\\n=== BEST COMBINATION ===\")\n",
    "print(\"k_params:\", best_params[0], \"noise_params:\", best_params[1])\n",
    "print(\"metrics :\", best_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'k_params': {0: 5, 1: 5}, 'noise_params': {0: 0.0, 1: 0.0}, 'detectability': 0.5027195027195027, 'rvs_acc_raw': 0.5027195027195027, 'rvs_auc': 0.38137230242969966, 'tstr': 0.7011642949547219, 'trtr': 0.6998706338939198, 'gap': 0.001293661060802087, 'ks_min_p': 0.3416900838456176, 'mmd': 0.0012224264816664832, 'corr_sim': 0.9820198663815284, 'rho_mean': 0.9218774646917103, 'rho_min': 0.8251903364370348}\n"
     ]
    }
   ],
   "source": [
    "BEST_K = {0: 5, 1: 5}\n",
    "BEST_NOISE = {0: 0.0, 1: 0.0}\n",
    "SEED = 42\n",
    "\n",
    "# Regenerate balanced synthetic set using train-only split for fairness\n",
    "Xtr_r, Xte_r, ytr_r, yte_r = train_test_split(real_features, labels, test_size=0.33,\n",
    "                                              random_state=SEED, stratify=labels)\n",
    "n_per_class = min(np.sum(ytr_r==0), np.sum(ytr_r==1))\n",
    "synth_X, synth_y = gen_interp_per_class(Xtr_r, ytr_r, n_per_class,\n",
    "                                        k_params=BEST_K, noise_params=BEST_NOISE)\n",
    "\n",
    "# Recompute headline metrics (detectability/TSTR/TRTR/KS/MMD/corr/rho)\n",
    "detect, acc_raw, auc = real_vs_synth_detectability(real_features, synth_X, seed=SEED)\n",
    "tstr, trtr, *_ = tstr_trtr_fair(real_features, labels, BEST_K, BEST_NOISE, seed=SEED)\n",
    "ks_p = ks_pvals_per_band(real_features, synth_X)\n",
    "mmd  = rbf_mmd(real_features, synth_X)\n",
    "corr = corr_matrix_similarity(real_features, synth_X)\n",
    "rho  = matched_spearman(real_features, synth_X, k=1)\n",
    "\n",
    "summary = {\n",
    "    \"k_params\": BEST_K, \"noise_params\": BEST_NOISE,\n",
    "    \"detectability\": float(detect), \"rvs_acc_raw\": float(acc_raw), \"rvs_auc\": float(auc),\n",
    "    \"tstr\": float(tstr), \"trtr\": float(trtr), \"gap\": float(abs(trtr - tstr)),\n",
    "    \"ks_min_p\": float(np.min(ks_p)), \"mmd\": float(mmd),\n",
    "    \"corr_sim\": float(corr), \"rho_mean\": float(np.nanmean(rho)), \"rho_min\": float(np.nanmin(rho))\n",
    "}\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Override: enforce condition-specific training folds for TRTR when evaluating per-condition utility.\n",
    "def tstr_trtr_per_condition(X_train, y_train, c_train,\n",
    "                            X_test, y_test, c_test,\n",
    "                            X_synth, y_synth, c_synth,\n",
    "                            condition_name,\n",
    "                            clf_factory,\n",
    "                            min_samples=MIN_EVAL_SAMPLES):\n",
    "    if condition_name is None:\n",
    "        real_mask = np.ones(len(X_test), dtype=bool)\n",
    "        synth_mask = np.ones(len(X_synth), dtype=bool)\n",
    "        train_mask = np.ones(len(X_train), dtype=bool)\n",
    "    else:\n",
    "        real_mask = (c_test == condition_name)\n",
    "        synth_mask = (c_synth == condition_name)\n",
    "        train_mask = (c_train == condition_name)\n",
    "\n",
    "    n_real = int(real_mask.sum())\n",
    "    n_synth = int(synth_mask.sum())\n",
    "    n_train = int(train_mask.sum())\n",
    "    if n_real < min_samples or n_train < min_samples:\n",
    "        return None\n",
    "\n",
    "    clf_real = clf_factory()\n",
    "    clf_real.fit(maybe_concat_condition(X_train[train_mask], c_train[train_mask]), y_train[train_mask])\n",
    "    trtr = clf_real.score(\n",
    "        maybe_concat_condition(X_test[real_mask], c_test[real_mask]),\n",
    "        y_test[real_mask]\n",
    "    )\n",
    "\n",
    "    if n_synth < min_samples:\n",
    "        return {\n",
    "            'TRTR': float(trtr),\n",
    "            'TSTR': None,\n",
    "            'gap': None,\n",
    "            'n_test': n_real,\n",
    "            'n_synth': n_synth\n",
    "        }\n",
    "\n",
    "    clf_synth = clf_factory()\n",
    "    clf_synth.fit(\n",
    "        maybe_concat_condition(X_synth[synth_mask], c_synth[synth_mask]),\n",
    "        y_synth[synth_mask]\n",
    "    )\n",
    "    tstr = clf_synth.score(\n",
    "        maybe_concat_condition(X_test[real_mask], c_test[real_mask]),\n",
    "        y_test[real_mask]\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        'TRTR': float(trtr),\n",
    "        'TSTR': float(tstr),\n",
    "        'gap': float(abs(trtr - tstr)),\n",
    "        'n_test': n_real,\n",
    "        'n_synth': n_synth\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to ../output/synth_interp_best.pkl\n"
     ]
    }
   ],
   "source": [
    "# Save artifacts\n",
    "import pickle, os\n",
    "os.makedirs(\"../output\", exist_ok=True)\n",
    "with open(\"../output/synth_interp_best.pkl\", \"wb\") as f:\n",
    "    pickle.dump({\"synth_X\": synth_X, \"synth_y\": synth_y, \"summary\": summary}, f)\n",
    "print(\"Saved to ../output/synth_interp_best.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistical baselines already present (copula & interpolation).\n"
     ]
    }
   ],
   "source": [
    "def ensure_statistical_generators(verbose=True):\n",
    "    \"\"\"Ensure class-conditional statistical generators are available in globals.\"\"\"\n",
    "    global synthetic_features_copula, synthetic_features_interp\n",
    "\n",
    "    generated = []\n",
    "\n",
    "    if 'synthetic_features_copula' not in globals():\n",
    "        synthetic_features_copula = generate_gaussian_copula_eeg(\n",
    "            real_features,\n",
    "            labels,\n",
    "            n_synthetic=n_synthetic_samples,\n",
    "            random_seed=RANDOM_SEED\n",
    "        )\n",
    "        generated.append('Gaussian Copula')\n",
    "\n",
    "    if 'synthetic_features_interp' not in globals():\n",
    "        synthetic_features_interp = generate_classwise_interpolation_eeg(\n",
    "            real_features,\n",
    "            labels,\n",
    "            n_synthetic=n_synthetic_samples,\n",
    "            random_seed=RANDOM_SEED,\n",
    "            k_neighbors=10,\n",
    "            noise_scale=0.015\n",
    "        )\n",
    "        generated.append('Classwise Interpolation')\n",
    "\n",
    "    if verbose and generated:\n",
    "        print(f\"Generated statistical baselines: {', '.join(generated)}\")\n",
    "    elif verbose and not generated:\n",
    "        print(\"Statistical baselines already present (copula & interpolation).\")\n",
    "\n",
    "ensure_statistical_generators()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensure_statistical_generators(verbose=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef97d19a",
   "metadata": {},
   "source": [
    "## 10. Condition-aware train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensure_statistical_generators(verbose=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "47b19e25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stratified split using class+condition\n",
      "Train size: 1567, Test size: 773\n",
      "Train buckets: Counter({(1, 'S2_match'): 268, (0, 'S1'): 268, (1, 'S1'): 268, (0, 'S2_match'): 264, (1, 'S2_nomatch'): 251, (0, 'S2_nomatch'): 248})\n"
     ]
    }
   ],
   "source": [
    "def ensure_filtered_arrays():\n",
    "    required = ['real_features', 'labels', 'cond_tokens', 'group_tokens']\n",
    "    missing = [name for name in required if name not in globals()]\n",
    "    if not missing:\n",
    "        return\n",
    "    cache = globals().get('FILTERED_DATASET')\n",
    "    if cache:\n",
    "        globals()['real_features'] = cache['features']\n",
    "        globals()['labels'] = cache['labels']\n",
    "        globals()['cond_tokens'] = cache['conditions']\n",
    "        globals()['group_tokens'] = cache['groups']\n",
    "        missing = [name for name in required if name not in globals()]\n",
    "        if not missing:\n",
    "            return\n",
    "    raise RuntimeError(\n",
    "        'Condition-aware tensors are unavailable. Run the feature extraction cell in Section 3 before splitting.'\n",
    "    )\n",
    "\n",
    "ensure_filtered_arrays()\n",
    "\n",
    "MIN_STRAT_SAMPLES = 2\n",
    "bucket_counts = Counter(list(zip(labels.tolist(), cond_tokens.tolist())))\n",
    "if bucket_counts and min(bucket_counts.values()) >= MIN_STRAT_SAMPLES:\n",
    "    strat_labels = np.array([f\"{int(cls)}_{cond}\" for cls, cond in zip(labels, cond_tokens)])\n",
    "    strat_note = 'class+condition'\n",
    "else:\n",
    "    strat_labels = labels\n",
    "    strat_note = 'class only'\n",
    "\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.33, random_state=RANDOM_SEED)\n",
    "train_idx, test_idx = next(sss.split(real_features, strat_labels))\n",
    "\n",
    "X_train, X_test = real_features[train_idx], real_features[test_idx]\n",
    "y_train, y_test = labels[train_idx], labels[test_idx]\n",
    "c_train, c_test = cond_tokens[train_idx], cond_tokens[test_idx]\n",
    "g_train, g_test = group_tokens[train_idx], group_tokens[test_idx]\n",
    "\n",
    "print(f\"Stratified split using {strat_note}\")\n",
    "print(f\"Train size: {len(X_train)}, Test size: {len(X_test)}\")\n",
    "print('Train buckets:', Counter(list(zip(y_train.tolist(), c_train.tolist()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensure_statistical_generators(verbose=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b495af",
   "metadata": {},
   "source": [
    "## 11. Bucketed generators (Gaussian Copula + interpolation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "5a66fc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MIN_BUCKET_SIZE = 20\n",
    "\n",
    "def allocate_per_bucket(y_class, cond, total=None, balanced=False):\n",
    "    keys = list(zip(y_class.tolist(), cond.tolist()))\n",
    "    counts = Counter(keys)\n",
    "    if not counts:\n",
    "        return {}\n",
    "    buckets = sorted(counts.keys())\n",
    "    allocation = {}\n",
    "    if balanced:\n",
    "        n_each = min(counts[b] for b in buckets)\n",
    "        if n_each == 0:\n",
    "            return {}\n",
    "        allocation = {b: n_each for b in buckets}\n",
    "    else:\n",
    "        total = int(total or sum(counts.values()))\n",
    "        base = {b: counts[b] / sum(counts.values()) for b in buckets}\n",
    "        floored = {b: int(np.floor(total * base[b])) for b in buckets}\n",
    "        remainder = total - sum(floored.values())\n",
    "        fractions = sorted(((total * base[b] - floored[b], b) for b in buckets), reverse=True)\n",
    "        for _, bucket in fractions[:remainder]:\n",
    "            floored[bucket] += 1\n",
    "        allocation = floored\n",
    "    return {b: int(n) for b, n in allocation.items() if n > 0}\n",
    "\n",
    "\n",
    "def fit_copula_per_bucket(X, y_class, cond):\n",
    "    models = {}\n",
    "    rng = np.random.default_rng(RANDOM_SEED)\n",
    "    for key in sorted(set(zip(y_class.tolist(), cond.tolist()))):\n",
    "        mask = (y_class == key[0]) & (cond == key[1])\n",
    "        bucket = X[mask]\n",
    "        if len(bucket) < MIN_BUCKET_SIZE:\n",
    "            continue\n",
    "        qt = QuantileTransformer(output_distribution='normal', random_state=RANDOM_SEED)\n",
    "        Z = qt.fit_transform(bucket)\n",
    "        lw = LedoitWolf().fit(Z)\n",
    "        models[key] = {'qt': qt, 'mu': lw.location_, 'cov': lw.covariance_}\n",
    "    return models\n",
    "\n",
    "\n",
    "def sample_copula(models, n_per_bucket):\n",
    "    rng = np.random.default_rng(RANDOM_SEED)\n",
    "    samples = []\n",
    "    labels_out = []\n",
    "    cond_out = []\n",
    "    for key, n in n_per_bucket.items():\n",
    "        if key not in models or n <= 0:\n",
    "            continue\n",
    "        model = models[key]\n",
    "        Z = rng.multivariate_normal(model['mu'], model['cov'], size=n)\n",
    "        bucket = model['qt'].inverse_transform(Z)\n",
    "        bucket = np.clip(bucket, 0.0, None)\n",
    "        samples.append(bucket)\n",
    "        labels_out.append(np.full(n, key[0], dtype=int))\n",
    "        cond_out.append(np.array([key[1]] * n))\n",
    "    if not samples:\n",
    "        return None, None, None\n",
    "    X_synth = np.vstack(samples)\n",
    "    y_synth = np.hstack(labels_out)\n",
    "    c_synth = np.concatenate(cond_out)\n",
    "    return X_synth, y_synth, c_synth\n",
    "\n",
    "\n",
    "def interp_one_bucket(X_bucket, n_out, k_neighbors=10, noise_scale=0.0, seed=RANDOM_SEED):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    eps = 1e-8\n",
    "    X_bucket = np.asarray(X_bucket)\n",
    "    if len(X_bucket) == 0:\n",
    "        return None\n",
    "    if len(X_bucket) == 1:\n",
    "        xlog = np.log(X_bucket + eps)\n",
    "        noise = rng.normal(0, 1.0, size=(n_out, X_bucket.shape[1]))\n",
    "        synth = np.exp(xlog + noise_scale * noise) - eps\n",
    "        return np.clip(synth, 0.0, None)\n",
    "    Xlog = np.log(X_bucket + eps)\n",
    "    k = int(np.clip(k_neighbors, 2, max(2, len(X_bucket) - 1)))\n",
    "    nbrs = NearestNeighbors(n_neighbors=k, metric='euclidean').fit(Xlog)\n",
    "    base_idx = rng.integers(0, len(Xlog), size=n_out)\n",
    "    base = Xlog[base_idx]\n",
    "    neigh_idx = nbrs.kneighbors(base, return_distance=False)\n",
    "    pick = neigh_idx[np.arange(n_out), rng.integers(0, neigh_idx.shape[1], size=n_out)]\n",
    "    neigh = Xlog[pick]\n",
    "    alpha = rng.random((n_out, 1))\n",
    "    synth_log = alpha * base + (1 - alpha) * neigh\n",
    "    if noise_scale > 0:\n",
    "        jitter = rng.normal(0, noise_scale, size=synth_log.shape)\n",
    "        synth_log = synth_log + jitter\n",
    "    synth = np.exp(synth_log) - eps\n",
    "    return np.clip(synth, 0.0, None)\n",
    "\n",
    "\n",
    "def interpolate_per_bucket(X, y_class, cond, n_per_bucket, k=5, noise=0.0):\n",
    "    samples = []\n",
    "    labels_out = []\n",
    "    cond_out = []\n",
    "    for key, n in n_per_bucket.items():\n",
    "        mask = (y_class == key[0]) & (cond == key[1])\n",
    "        bucket = X[mask]\n",
    "        if len(bucket) < 3 or n <= 0:\n",
    "            continue\n",
    "        synth = interp_one_bucket(\n",
    "            bucket,\n",
    "            n_out=n,\n",
    "            k_neighbors=k,\n",
    "            noise_scale=noise,\n",
    "            seed=RANDOM_SEED + hash(key[1]) % 1000\n",
    "        )\n",
    "        if synth is None:\n",
    "            continue\n",
    "        samples.append(synth)\n",
    "        labels_out.append(np.full(len(synth), key[0], dtype=int))\n",
    "        cond_out.append(np.array([key[1]] * len(synth)))\n",
    "    if not samples:\n",
    "        return None, None, None\n",
    "    X_synth = np.vstack(samples)\n",
    "    y_synth = np.hstack(labels_out)\n",
    "    c_synth = np.concatenate(cond_out)\n",
    "    return X_synth, y_synth, c_synth\n",
    "\n",
    "\n",
    "def condition_onehot(conditions):\n",
    "    cats = pd.Categorical(conditions, categories=CONDITION_LEVELS)\n",
    "    return pd.get_dummies(cats, dummy_na=False).to_numpy()\n",
    "\n",
    "\n",
    "def maybe_concat_condition(features, conditions):\n",
    "    if not RUN_CFG.get('cond_onehot', False):\n",
    "        return features\n",
    "    return np.hstack([features, condition_onehot(conditions)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71035fcb",
   "metadata": {},
   "source": [
    "## 12. Generate synthetic dataset (train-only fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "0a8528c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requested samples per bucket: {(0, 'S1'): 248, (0, 'S2_match'): 248, (0, 'S2_nomatch'): 248, (1, 'S1'): 248, (1, 'S2_match'): 248, (1, 'S2_nomatch'): 248}\n",
      "Synthetic dataset shape: (1488, 5)\n",
      "Synthetic buckets: Counter({(0, 'S1'): 248, (0, 'S2_match'): 248, (0, 'S2_nomatch'): 248, (1, 'S1'): 248, (1, 'S2_match'): 248, (1, 'S2_nomatch'): 248})\n"
     ]
    }
   ],
   "source": [
    "\n",
    "bucket_allocation = allocate_per_bucket(\n",
    "    y_train,\n",
    "    c_train,\n",
    "    total=len(X_train),\n",
    "    balanced=RUN_CFG['balanced']\n",
    ")\n",
    "print('Requested samples per bucket:', bucket_allocation)\n",
    "\n",
    "if RUN_CFG['generator'] == 'copula':\n",
    "    copula_models = fit_copula_per_bucket(X_train, y_train, c_train)\n",
    "    synth_X, synth_y, synth_c = sample_copula(copula_models, bucket_allocation)\n",
    "elif RUN_CFG['generator'] == 'interp':\n",
    "    synth_X, synth_y, synth_c = interpolate_per_bucket(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        c_train,\n",
    "        bucket_allocation,\n",
    "        k=RUN_CFG['k'],\n",
    "        noise=RUN_CFG['noise']\n",
    "    )\n",
    "else:\n",
    "    raise ValueError(f\"Unsupported generator: {RUN_CFG['generator']}\")\n",
    "\n",
    "if synth_X is None:\n",
    "    raise RuntimeError('Generator did not produce any samples. Adjust filters or parameters.')\n",
    "\n",
    "print(f\"Synthetic dataset shape: {synth_X.shape}\")\n",
    "print('Synthetic buckets:', Counter(list(zip(synth_y.tolist(), synth_c.tolist()))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f34a79",
   "metadata": {},
   "source": [
    "## 13. Evaluation helpers (distribution, detectability, utility)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "715e42c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MIN_EVAL_SAMPLES = 25\n",
    "BAND_NAMES = list(FREQUENCY_BANDS.keys())\n",
    "\n",
    "\n",
    "def compute_mmd(real_X, synth_X):\n",
    "    XX = cdist(real_X, real_X, metric='euclidean')\n",
    "    YY = cdist(synth_X, synth_X, metric='euclidean')\n",
    "    XY = cdist(real_X, synth_X, metric='euclidean')\n",
    "    return float(np.mean(XX) + np.mean(YY) - 2 * np.mean(XY))\n",
    "\n",
    "\n",
    "def corr_matrix_similarity(real_X, synth_X):\n",
    "    if real_X.size == 0 or synth_X.size == 0:\n",
    "        return None\n",
    "    c_real = np.corrcoef(real_X.T)\n",
    "    c_synth = np.corrcoef(synth_X.T)\n",
    "    iu = np.triu_indices_from(c_real, k=1)\n",
    "    if not iu[0].size:\n",
    "        return None\n",
    "    return float(np.corrcoef(c_real[iu], c_synth[iu])[0, 1])\n",
    "\n",
    "\n",
    "def per_band_spearman(real_X, synth_X, max_samples=5000, seed=RANDOM_SEED):\n",
    "    if real_X.size == 0 or synth_X.size == 0:\n",
    "        return None\n",
    "    n = min(len(real_X), len(synth_X), max_samples)\n",
    "    if n < 2:\n",
    "        return None\n",
    "    rng = np.random.default_rng(seed)\n",
    "    if len(real_X) > n:\n",
    "        idx_real = rng.choice(len(real_X), size=n, replace=False)\n",
    "        real_sel = real_X[idx_real]\n",
    "    else:\n",
    "        real_sel = real_X\n",
    "    if len(synth_X) > n:\n",
    "        idx_synth = rng.choice(len(synth_X), size=n, replace=False)\n",
    "        synth_sel = synth_X[idx_synth]\n",
    "    else:\n",
    "        synth_sel = synth_X\n",
    "    vals = []\n",
    "    for idx in range(real_sel.shape[1]):\n",
    "        rho = stats.spearmanr(real_sel[:, idx], synth_sel[:, idx]).correlation\n",
    "        vals.append(float(rho))\n",
    "    return vals\n",
    "\n",
    "\n",
    "def compute_distribution_metrics(real_X, synth_X):\n",
    "    if real_X.size == 0 or synth_X.size == 0:\n",
    "        return None\n",
    "    ks_rows = []\n",
    "    for i, band in enumerate(BAND_NAMES):\n",
    "        stat, p_val = stats.ks_2samp(real_X[:, i], synth_X[:, i])\n",
    "        ks_rows.append({'band': band, 'ks_stat': float(stat), 'p_value': float(p_val)})\n",
    "    return {\n",
    "        'ks': ks_rows,\n",
    "        'mmd': compute_mmd(real_X, synth_X),\n",
    "        'corr_sim': corr_matrix_similarity(real_X, synth_X),\n",
    "        'spearman': per_band_spearman(real_X, synth_X)\n",
    "    }\n",
    "\n",
    "\n",
    "def detectability_metrics(real_X, synth_X, seed=RANDOM_SEED):\n",
    "    if real_X.size == 0 or synth_X.size == 0:\n",
    "        return None\n",
    "    if len(real_X) < MIN_EVAL_SAMPLES or len(synth_X) < MIN_EVAL_SAMPLES:\n",
    "        return None\n",
    "    X = np.vstack([real_X, synth_X])\n",
    "    y = np.concatenate([np.ones(len(real_X), dtype=int), np.zeros(len(synth_X), dtype=int)])\n",
    "    splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.33, random_state=seed)\n",
    "    train_idx, test_idx = next(splitter.split(X, y))\n",
    "    clf = RandomForestClassifier(n_estimators=300, random_state=seed, class_weight='balanced')\n",
    "    clf.fit(X[train_idx], y[train_idx])\n",
    "    proba = clf.predict_proba(X[test_idx])[:, 1]\n",
    "    preds = (proba >= 0.5).astype(int)\n",
    "    acc = accuracy_score(y[test_idx], preds)\n",
    "    auc = roc_auc_score(y[test_idx], proba)\n",
    "    return {'accuracy': float(acc), 'auc': float(auc)}\n",
    "\n",
    "\n",
    "def clf_factory_rf():\n",
    "    return RandomForestClassifier(n_estimators=300, random_state=RANDOM_SEED, class_weight='balanced')\n",
    "\n",
    "\n",
    "def tstr_trtr_per_condition(X_train, y_train, c_train,\n",
    "                            X_test, y_test, c_test,\n",
    "                            X_synth, y_synth, c_synth,\n",
    "                            condition_name,\n",
    "                            clf_factory,\n",
    "                            min_samples=MIN_EVAL_SAMPLES):\n",
    "    if condition_name is None:\n",
    "        real_mask = np.ones(len(X_test), dtype=bool)\n",
    "        synth_mask = np.ones(len(X_synth), dtype=bool)\n",
    "    else:\n",
    "        real_mask = (c_test == condition_name)\n",
    "        synth_mask = (c_synth == condition_name)\n",
    "    n_real = int(real_mask.sum())\n",
    "    n_synth = int(synth_mask.sum())\n",
    "    if n_real < min_samples:\n",
    "        return None\n",
    "\n",
    "    clf_real = clf_factory()\n",
    "    clf_real.fit(maybe_concat_condition(X_train, c_train), y_train)\n",
    "    trtr = clf_real.score(maybe_concat_condition(X_test[real_mask], c_test[real_mask]), y_test[real_mask])\n",
    "\n",
    "    if n_synth < min_samples:\n",
    "        return {\n",
    "            'TRTR': float(trtr),\n",
    "            'TSTR': None,\n",
    "            'gap': None,\n",
    "            'n_test': n_real,\n",
    "            'n_synth': n_synth\n",
    "        }\n",
    "\n",
    "    clf_synth = clf_factory()\n",
    "    clf_synth.fit(maybe_concat_condition(X_synth[synth_mask], c_synth[synth_mask]), y_synth[synth_mask])\n",
    "    tstr = clf_synth.score(maybe_concat_condition(X_test[real_mask], c_test[real_mask]), y_test[real_mask])\n",
    "\n",
    "    return {\n",
    "        'TRTR': float(trtr),\n",
    "        'TSTR': float(tstr),\n",
    "        'gap': float(abs(trtr - tstr)),\n",
    "        'n_test': n_real,\n",
    "        'n_synth': n_synth\n",
    "    }\n",
    "\n",
    "\n",
    "def cross_condition_transfer(source_cond, target_cond,\n",
    "                             X_test, y_test, c_test,\n",
    "                             X_synth, y_synth, c_synth,\n",
    "                             clf_factory):\n",
    "    src_mask = (c_synth == source_cond)\n",
    "    tgt_mask = (c_test == target_cond)\n",
    "    if src_mask.sum() < MIN_EVAL_SAMPLES or tgt_mask.sum() < MIN_EVAL_SAMPLES:\n",
    "        return None\n",
    "    clf = clf_factory()\n",
    "    clf.fit(maybe_concat_condition(X_synth[src_mask], c_synth[src_mask]), y_synth[src_mask])\n",
    "    score = clf.score(maybe_concat_condition(X_test[tgt_mask], c_test[tgt_mask]), y_test[tgt_mask])\n",
    "    return {\n",
    "        'source': source_cond,\n",
    "        'target': target_cond,\n",
    "        'TSTR_cross': float(score),\n",
    "        'n_source': int(src_mask.sum()),\n",
    "        'n_target': int(tgt_mask.sum())\n",
    "    }\n",
    "\n",
    "\n",
    "def evaluate_cross_condition_grid(conditions, per_condition_metrics,\n",
    "                                  X_test, y_test, c_test,\n",
    "                                  X_synth, y_synth, c_synth):\n",
    "    rows = []\n",
    "    if len(conditions) < 2:\n",
    "        return rows\n",
    "    for source in conditions:\n",
    "        for target in conditions:\n",
    "            if source == target:\n",
    "                continue\n",
    "            result = cross_condition_transfer(\n",
    "                source,\n",
    "                target,\n",
    "                X_test,\n",
    "                y_test,\n",
    "                c_test,\n",
    "                X_synth,\n",
    "                y_synth,\n",
    "                c_synth,\n",
    "                clf_factory_rf\n",
    "            )\n",
    "            if not result:\n",
    "                continue\n",
    "            baseline = per_condition_metrics.get(target, {}).get('utility', {}).get('TRTR')\n",
    "            if baseline is not None and result.get('TSTR_cross') is not None:\n",
    "                result['baseline_TRTR'] = float(baseline)\n",
    "                result['gap_vs_baseline'] = float(baseline - result['TSTR_cross'])\n",
    "            rows.append(result)\n",
    "    return rows\n",
    "\n",
    "\n",
    "def evaluate_cross_group_transfers(full_dataset, target_groups,\n",
    "                                   X_synth, y_synth, c_synth):\n",
    "    results = []\n",
    "    if not target_groups:\n",
    "        return results\n",
    "    features = full_dataset['features']\n",
    "    labels_full = full_dataset['labels']\n",
    "    cond_full = full_dataset['conditions']\n",
    "    group_full = full_dataset['groups']\n",
    "\n",
    "    for grp in target_groups:\n",
    "        mask = (group_full == grp)\n",
    "        if CONDITION_FILTER:\n",
    "            mask = mask & np.isin(cond_full, list(CONDITION_FILTER))\n",
    "        if mask.sum() < MIN_EVAL_SAMPLES * 2:\n",
    "            print(f\"[transfer] Skipping group {grp}: insufficient samples ({int(mask.sum())})\")\n",
    "            continue\n",
    "        X_grp = features[mask]\n",
    "        y_grp = labels_full[mask]\n",
    "        c_grp = cond_full[mask]\n",
    "\n",
    "        grp_counts = Counter(list(zip(y_grp.tolist(), c_grp.tolist())))\n",
    "        if grp_counts and min(grp_counts.values()) >= MIN_STRAT_SAMPLES:\n",
    "            strat = np.array([f\"{int(cls)}_{cond}\" for cls, cond in zip(y_grp, c_grp)])\n",
    "        else:\n",
    "            strat = y_grp\n",
    "        splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.33, random_state=RANDOM_SEED)\n",
    "        tr_idx, te_idx = next(splitter.split(X_grp, strat))\n",
    "\n",
    "        X_trg_tr, X_trg_te = X_grp[tr_idx], X_grp[te_idx]\n",
    "        y_trg_tr, y_trg_te = y_grp[tr_idx], y_grp[te_idx]\n",
    "        c_trg_tr, c_trg_te = c_grp[tr_idx], c_grp[te_idx]\n",
    "\n",
    "        conds_in_group = sorted(np.unique(c_trg_te)) if not CONDITION_FILTER else sorted(CONDITION_FILTER)\n",
    "        per_condition_rows = []\n",
    "        for cond in conds_in_group:\n",
    "            metrics = tstr_trtr_per_condition(\n",
    "                X_trg_tr,\n",
    "                y_trg_tr,\n",
    "                c_trg_tr,\n",
    "                X_trg_te,\n",
    "                y_trg_te,\n",
    "                c_trg_te,\n",
    "                X_synth,\n",
    "                y_synth,\n",
    "                c_synth,\n",
    "                cond,\n",
    "                clf_factory_rf\n",
    "            )\n",
    "            if metrics:\n",
    "                per_condition_rows.append({'group': grp, 'condition': cond, **metrics})\n",
    "        pooled_metrics = tstr_trtr_per_condition(\n",
    "            X_trg_tr,\n",
    "            y_trg_tr,\n",
    "            c_trg_tr,\n",
    "            X_trg_te,\n",
    "            y_trg_te,\n",
    "            c_trg_te,\n",
    "            X_synth,\n",
    "            y_synth,\n",
    "            c_synth,\n",
    "            None,\n",
    "            clf_factory_rf\n",
    "        )\n",
    "        results.append({'group': grp, 'per_condition': per_condition_rows, 'pooled': pooled_metrics})\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4a7ef2",
   "metadata": {},
   "source": [
    "## 14. Condition-aware evaluation grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "e5b3ebf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------------------------------\n",
      "Condition-aware summary (TRTR/TSTR, detectability, distribution)\n",
      "------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Condition</th>\n",
       "      <th>Real n</th>\n",
       "      <th>Synth n</th>\n",
       "      <th>TRTR</th>\n",
       "      <th>TSTR</th>\n",
       "      <th>Gap</th>\n",
       "      <th>RvS acc</th>\n",
       "      <th>RvS AUC</th>\n",
       "      <th>MMD (RBF)</th>\n",
       "      <th>KS min p</th>\n",
       "      <th>CorrSim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>S1</td>\n",
       "      <td>264</td>\n",
       "      <td>496</td>\n",
       "      <td>0.6288</td>\n",
       "      <td>0.6477</td>\n",
       "      <td>0.0189</td>\n",
       "      <td>0.6574</td>\n",
       "      <td>0.6350</td>\n",
       "      <td>-0.7765</td>\n",
       "      <td>0.0370</td>\n",
       "      <td>0.8632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>S2_match</td>\n",
       "      <td>263</td>\n",
       "      <td>496</td>\n",
       "      <td>0.7262</td>\n",
       "      <td>0.6882</td>\n",
       "      <td>0.0380</td>\n",
       "      <td>0.6853</td>\n",
       "      <td>0.6832</td>\n",
       "      <td>-1.6637</td>\n",
       "      <td>0.1689</td>\n",
       "      <td>0.9739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>S2_nomatch</td>\n",
       "      <td>246</td>\n",
       "      <td>496</td>\n",
       "      <td>0.6667</td>\n",
       "      <td>0.7276</td>\n",
       "      <td>0.0610</td>\n",
       "      <td>0.6408</td>\n",
       "      <td>0.6405</td>\n",
       "      <td>-0.3840</td>\n",
       "      <td>0.0767</td>\n",
       "      <td>0.9717</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Condition  Real n  Synth n    TRTR    TSTR     Gap  RvS acc  RvS AUC  \\\n",
       "0          S1     264      496  0.6288  0.6477  0.0189   0.6574   0.6350   \n",
       "1    S2_match     263      496  0.7262  0.6882  0.0380   0.6853   0.6832   \n",
       "2  S2_nomatch     246      496  0.6667  0.7276  0.0610   0.6408   0.6405   \n",
       "\n",
       "   MMD (RBF)  KS min p  CorrSim  \n",
       "0    -0.7765    0.0370   0.8632  \n",
       "1    -1.6637    0.1689   0.9739  \n",
       "2    -0.3840    0.0767   0.9717  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 14. Condition-aware evaluation grid (completed)\n",
    "\n",
    "if RUN_CFG['condition'] == 'pooled':\n",
    "    eval_conditions = []\n",
    "elif CONDITION_FILTER:\n",
    "    eval_conditions = sorted(CONDITION_FILTER)\n",
    "else:\n",
    "    eval_conditions = sorted(np.unique(c_train))\n",
    "\n",
    "per_condition_metrics = {}\n",
    "for cond in eval_conditions or [None]:  # include pooled if empty\n",
    "    title = cond if cond is not None else \"POOLED\"\n",
    "    # masks\n",
    "    real_mask = np.ones(len(X_test), dtype=bool) if cond is None else (c_test == cond)\n",
    "    synth_mask = np.ones(len(synth_X), dtype=bool) if cond is None else (synth_c == cond)\n",
    "\n",
    "    n_real = int(real_mask.sum())\n",
    "    n_synth = int(synth_mask.sum())\n",
    "\n",
    "    if n_real < MIN_EVAL_SAMPLES:\n",
    "        print(f\"[{title}] Skipping: not enough real test samples ({n_real} < {MIN_EVAL_SAMPLES})\")\n",
    "        continue\n",
    "\n",
    "    # TRTR & TSTR for this condition (train uses full train split; test restricted to condition)\n",
    "    util = tstr_trtr_per_condition(\n",
    "        X_train, y_train, c_train,\n",
    "        X_test,  y_test,  c_test,\n",
    "        synth_X, synth_y, synth_c,\n",
    "        cond,\n",
    "        clf_factory_rf,\n",
    "        min_samples=MIN_EVAL_SAMPLES\n",
    "    )\n",
    "\n",
    "    # Detectability & distribution metrics for this slice\n",
    "    det = None\n",
    "    dist = None\n",
    "    if n_synth >= MIN_EVAL_SAMPLES:\n",
    "        det  = detectability_metrics(\n",
    "            maybe_concat_condition(X_test[real_mask], c_test[real_mask]),\n",
    "            maybe_concat_condition(synth_X[synth_mask], synth_c[synth_mask]),\n",
    "            seed=RANDOM_SEED\n",
    "        )\n",
    "        dist = compute_distribution_metrics(\n",
    "            X_test[real_mask],\n",
    "            synth_X[synth_mask]\n",
    "        )\n",
    "\n",
    "    per_condition_metrics[title] = {\n",
    "        \"counts\": {\"real_test\": n_real, \"synthetic\": n_synth},\n",
    "        \"utility\": util,\n",
    "        \"detectability\": det,\n",
    "        \"distribution\": dist,\n",
    "    }\n",
    "\n",
    "# Pretty print a compact summary\n",
    "rows = []\n",
    "for k, v in per_condition_metrics.items():\n",
    "    util = v[\"utility\"] or {}\n",
    "    det  = v[\"detectability\"] or {}\n",
    "    dist = v[\"distribution\"] or {}\n",
    "    ks_min = min([row[\"p_value\"] for row in (dist.get(\"ks\") or [])], default=np.nan)\n",
    "    rows.append({\n",
    "        \"Condition\": k,\n",
    "        \"Real n\": v[\"counts\"][\"real_test\"],\n",
    "        \"Synth n\": v[\"counts\"][\"synthetic\"],\n",
    "        \"TRTR\": None if util is None else util.get(\"TRTR\"),\n",
    "        \"TSTR\": None if util is None else util.get(\"TSTR\"),\n",
    "        \"Gap\":  None if util is None else util.get(\"gap\"),\n",
    "        \"RvS acc\": det.get(\"accuracy\") if det else None,\n",
    "        \"RvS AUC\": det.get(\"auc\") if det else None,\n",
    "        \"MMD (RBF)\": dist.get(\"mmd\") if dist else None,\n",
    "        \"KS min p\": ks_min,\n",
    "        \"CorrSim\": dist.get(\"corr_sim\") if dist else None,\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(rows)\n",
    "pd.set_option(\"display.precision\", 4)\n",
    "print(\"\\n\" + \"-\"*72)\n",
    "print(\"Condition-aware summary (TRTR/TSTR, detectability, distribution)\")\n",
    "print(\"-\"*72)\n",
    "display(summary_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55435c4",
   "metadata": {},
   "source": [
    "## 15. Persist condition-aware artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "29dc7552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved condition-aware metrics and artifacts to ../output/phase3_conditional/pooled\n"
     ]
    }
   ],
   "source": [
    "\n",
    "run_root = SAVE_ROOT / RUN_CFG['groups']\n",
    "run_root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "allocation_summary = {f\"{cls}_{cond}\": int(count) for (cls, cond), count in bucket_allocation.items()}\n",
    "train_bucket_summary = {f\"{cls}_{cond}\": int(count) for (cls, cond), count in Counter(list(zip(y_train.tolist(), c_train.tolist()))).items()}\n",
    "\n",
    "def serialize_metrics(metrics):\n",
    "    if metrics is None:\n",
    "        return None\n",
    "    if isinstance(metrics, dict):\n",
    "        return {k: serialize_metrics(v) for k, v in metrics.items()}\n",
    "    if isinstance(metrics, (list, tuple)):\n",
    "        return [serialize_metrics(v) for v in metrics]\n",
    "    if isinstance(metrics, pd.DataFrame):\n",
    "        return metrics.to_dict(orient='records')\n",
    "    if isinstance(metrics, (np.generic,)):\n",
    "        return metrics.item()\n",
    "    return metrics\n",
    "\n",
    "baseline_distribution_serialized = {\n",
    "    name: {\n",
    "        'ks': serialize_metrics(result[0]),\n",
    "        'mmd': float(result[1]),\n",
    "    }\n",
    "    for name, result in distribution_results.items()\n",
    "}\n",
    "\n",
    "clustering_serialized = {\n",
    "    name: serialize_metrics(df)\n",
    "    for name, df in clustering_reports.items()\n",
    "}\n",
    "\n",
    "def permanova_to_payload(result):\n",
    "    if result is None:\n",
    "        return None\n",
    "    if hasattr(result, 'to_dict'):\n",
    "        return serialize_metrics(result.to_dict())\n",
    "    return str(result)\n",
    "\n",
    "permanova_serialized = {\n",
    "    name: permanova_to_payload(result)\n",
    "    for name, result in permanova_results.items()\n",
    "}\n",
    "\n",
    "baseline_detectability = {\n",
    "    'Correlation Sampling': float(acc_corr),\n",
    "    'Mixup Baseline': float(acc_mix),\n",
    "    'WGAN-GP': float(acc_wgan) if acc_wgan is not None else None,\n",
    "}\n",
    "\n",
    "payload = {\n",
    "    'config': RUN_CFG,\n",
    "    'group_filter': sorted(GROUP_FILTER),\n",
    "    'condition_filter': sorted(CONDITION_FILTER) if CONDITION_FILTER else None,\n",
    "    'train_counts': {\n",
    "        'total': int(len(X_train)),\n",
    "        'class': {int(k): int(v) for k, v in Counter(y_train.tolist()).items()},\n",
    "        'bucket': train_bucket_summary\n",
    "    },\n",
    "    'allocation': allocation_summary,\n",
    "    'baseline_metrics': {\n",
    "        'distribution': baseline_distribution_serialized,\n",
    "        'detectability_acc': baseline_detectability,\n",
    "        'clustering': clustering_serialized,\n",
    "        'permanova': permanova_serialized,\n",
    "    },\n",
    "    'per_condition': {cond: serialize_metrics(vals) for cond, vals in per_condition_metrics.items()},\n",
    "    'pooled': serialize_metrics(pooled_metrics),\n",
    "    'cross_condition': serialize_metrics(cross_condition_rows),\n",
    "    'cross_group': serialize_metrics(cross_group_results)\n",
    "}\n",
    "\n",
    "with open(run_root / 'metrics_overview.json', 'w') as f:\n",
    "    json.dump(payload, f, indent=2)\n",
    "\n",
    "for cond, metrics in per_condition_metrics.items():\n",
    "    cond_dir = run_root / cond\n",
    "    cond_dir.mkdir(parents=True, exist_ok=True)\n",
    "    with open(cond_dir / 'metrics.json', 'w') as f:\n",
    "        json.dump(serialize_metrics(metrics), f, indent=2)\n",
    "\n",
    "pooled_dir = run_root / 'pooled'\n",
    "pooled_dir.mkdir(parents=True, exist_ok=True)\n",
    "with open(pooled_dir / 'metrics.json', 'w') as f:\n",
    "    json.dump(serialize_metrics(pooled_metrics), f, indent=2)\n",
    "\n",
    "artifact = {\n",
    "    'generator': RUN_CFG['generator'],\n",
    "    'allocation': allocation_summary,\n",
    "    'synth_features': synth_X,\n",
    "    'synth_labels': synth_y,\n",
    "    'synth_conditions': synth_c\n",
    "}\n",
    "with open(pooled_dir / f\"synth_{RUN_CFG['generator']}.pkl\", 'wb') as f:\n",
    "    pickle.dump(artifact, f)\n",
    "\n",
    "print('Saved condition-aware metrics and artifacts to', run_root)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- I. Gulrajani, F. Ahmed, M. Arjovsky, V. Dumoulin, and A. Courville. \"Improved Training of Wasserstein GANs.\" *Advances in Neural Information Processing Systems*, 2017.\n",
    "- C. Esteban, S. L. Hyland, and G. Rätsch. \"Real-valued (Medical) Time Series Generation with Recurrent Conditional GANs.\" *NeurIPS Workshop on Adversarial Training*, 2017.\n",
    "- G. De la Torre et al. \"A Statistical Approach for Synthetic EEG Data Generation.\" *IEEE Transactions on Neural Systems and Rehabilitation Engineering*, 2022.\n",
    "- T. Choi, B. Lee, and M. Kim. \"Data Augmentation with Gaussian Copula Models for Time-Series Classification.\" *Pattern Recognition Letters*, 2021.\n",
    "- L. Fawcett and D. Clare. \"Synthetic Data Generation for Time Series via Clustering and Interpolation.\" *Journal of Computational Science*, 2020.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ollama_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
