{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 3: Synthetic EEG Data Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0. Load package & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "from collections import defaultdict, Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.covariance import LedoitWolf\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "plt.rcParams[\"figure.figsize\"] = (6, 4)\n",
    "\n",
    "DATA_DIR = Path(\"../output/band_extraction\")\n",
    "OUT_DIR = Path(\"../output/synthetic_generation\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Processed data features check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1. Dataset overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data structure\n",
    "LABEL_COL = \"subject_type\"         \n",
    "CONDITION_COL = \"matching_condition\"   \n",
    "SPLIT_COL = \"dataset_split\"\n",
    "META_COLS = [\"dataset_split\", \"file_name\", \"subject_type\", \"subject_id\", \"channel\", \"trial\", \"matching_condition\", \"Delta\", \"Theta\", \"Alpha\", \"Beta\", \"Gamma\", \"total_power\"]\n",
    "BAND_COLS = [\"Delta\", \"Theta\", \"Alpha\", \"Beta\", \"Gamma\", \"total_power\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full data shape: (60672, 13)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset_split</th>\n",
       "      <th>file_name</th>\n",
       "      <th>subject_type</th>\n",
       "      <th>subject_id</th>\n",
       "      <th>channel</th>\n",
       "      <th>trial</th>\n",
       "      <th>matching_condition</th>\n",
       "      <th>Delta</th>\n",
       "      <th>Theta</th>\n",
       "      <th>Alpha</th>\n",
       "      <th>Beta</th>\n",
       "      <th>Gamma</th>\n",
       "      <th>total_power</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train</td>\n",
       "      <td>Data1.csv</td>\n",
       "      <td>a</td>\n",
       "      <td>co2a0000364</td>\n",
       "      <td>FP1</td>\n",
       "      <td>0</td>\n",
       "      <td>S1 obj</td>\n",
       "      <td>20.048105</td>\n",
       "      <td>5.830134</td>\n",
       "      <td>0.854299</td>\n",
       "      <td>6.705598</td>\n",
       "      <td>6.848762</td>\n",
       "      <td>40.286898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train</td>\n",
       "      <td>Data1.csv</td>\n",
       "      <td>a</td>\n",
       "      <td>co2a0000364</td>\n",
       "      <td>FP2</td>\n",
       "      <td>0</td>\n",
       "      <td>S1 obj</td>\n",
       "      <td>21.769006</td>\n",
       "      <td>6.052321</td>\n",
       "      <td>1.013807</td>\n",
       "      <td>16.487621</td>\n",
       "      <td>15.773774</td>\n",
       "      <td>61.096530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train</td>\n",
       "      <td>Data1.csv</td>\n",
       "      <td>a</td>\n",
       "      <td>co2a0000364</td>\n",
       "      <td>F7</td>\n",
       "      <td>0</td>\n",
       "      <td>S1 obj</td>\n",
       "      <td>7.742259</td>\n",
       "      <td>6.272004</td>\n",
       "      <td>1.893497</td>\n",
       "      <td>39.119253</td>\n",
       "      <td>49.533282</td>\n",
       "      <td>104.560295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train</td>\n",
       "      <td>Data1.csv</td>\n",
       "      <td>a</td>\n",
       "      <td>co2a0000364</td>\n",
       "      <td>F8</td>\n",
       "      <td>0</td>\n",
       "      <td>S1 obj</td>\n",
       "      <td>11.400244</td>\n",
       "      <td>4.816262</td>\n",
       "      <td>2.360998</td>\n",
       "      <td>53.646940</td>\n",
       "      <td>44.502180</td>\n",
       "      <td>116.726624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train</td>\n",
       "      <td>Data1.csv</td>\n",
       "      <td>a</td>\n",
       "      <td>co2a0000364</td>\n",
       "      <td>AF1</td>\n",
       "      <td>0</td>\n",
       "      <td>S1 obj</td>\n",
       "      <td>13.188257</td>\n",
       "      <td>2.347635</td>\n",
       "      <td>0.542750</td>\n",
       "      <td>4.036543</td>\n",
       "      <td>2.914738</td>\n",
       "      <td>23.029923</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  dataset_split  file_name subject_type   subject_id channel  trial  \\\n",
       "0         train  Data1.csv            a  co2a0000364     FP1      0   \n",
       "1         train  Data1.csv            a  co2a0000364     FP2      0   \n",
       "2         train  Data1.csv            a  co2a0000364      F7      0   \n",
       "3         train  Data1.csv            a  co2a0000364      F8      0   \n",
       "4         train  Data1.csv            a  co2a0000364     AF1      0   \n",
       "\n",
       "  matching_condition      Delta     Theta     Alpha       Beta      Gamma  \\\n",
       "0             S1 obj  20.048105  5.830134  0.854299   6.705598   6.848762   \n",
       "1             S1 obj  21.769006  6.052321  1.013807  16.487621  15.773774   \n",
       "2             S1 obj   7.742259  6.272004  1.893497  39.119253  49.533282   \n",
       "3             S1 obj  11.400244  4.816262  2.360998  53.646940  44.502180   \n",
       "4             S1 obj  13.188257  2.347635  0.542750   4.036543   2.914738   \n",
       "\n",
       "   total_power  \n",
       "0    40.286898  \n",
       "1    61.096530  \n",
       "2   104.560295  \n",
       "3   116.726624  \n",
       "4    23.029923  "
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load current feature dataset\n",
    "FEATURE_FP = DATA_DIR / \"band_features_segments.csv\"\n",
    "df_all = pd.read_csv(FEATURE_FP)\n",
    "print(\"Full data shape:\", df_all.shape)\n",
    "df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split counts:\n",
      "dataset_split\n",
      "test     30720\n",
      "train    29952\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Basic sanity checks\n",
    "print(\"Split counts:\")\n",
    "print(df_all[SPLIT_COL].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject_type counts:\n",
      "subject_type\n",
      "a    30400\n",
      "c    30272\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Subject_type counts:\")\n",
    "print(df_all[LABEL_COL].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching_condition counts:\n",
      "matching_condition\n",
      "S1 obj         20480\n",
      "S2 match       20416\n",
      "S2 nomatch,    19776\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Matching_condition counts:\")\n",
    "print(df_all[CONDITION_COL].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (29952, 13)\n",
      "Test shape : (30720, 13)\n"
     ]
    }
   ],
   "source": [
    "# Train / Test based on dataset_split\n",
    "df_train = df_all[df_all[SPLIT_COL] == \"train\"].reset_index(drop=True)\n",
    "df_test = df_all[df_all[SPLIT_COL] == \"test\"].reset_index(drop=True)\n",
    "print(\"Train shape:\", df_train.shape)\n",
    "print(\"Test shape :\", df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train label x condition:\n",
      "matching_condition  S1 obj  S2 match  S2 nomatch,\n",
      "subject_type                                     \n",
      "a                     5120      5120         4800\n",
      "c                     5120      5056         4736\n"
     ]
    }
   ],
   "source": [
    "print(\"Train label x condition:\")\n",
    "print(pd.crosstab(df_train[LABEL_COL], df_train[CONDITION_COL]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test label x condition:\n",
      "matching_condition  S1 obj  S2 match  S2 nomatch,\n",
      "subject_type                                     \n",
      "a                     5120      5120         5120\n",
      "c                     5120      5120         5120\n"
     ]
    }
   ],
   "source": [
    "print(\"Test label x condition:\")\n",
    "print(pd.crosstab(df_test[LABEL_COL], df_test[CONDITION_COL]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2. Basic check for missing values and infinite features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check missing and extra columns\n",
    "missing = [c for c in META_COLS if c not in df_all.columns]\n",
    "extra = [c for c in df_all.columns if c not in META_COLS]\n",
    "\n",
    "if missing:\n",
    "    print(\"MISSING columns:\", missing)\n",
    "if extra:\n",
    "    print(\"EXTRA columns:\", extra)\n",
    "if not missing:\n",
    "    df_all = df_all[META_COLS].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data types:\n",
      "dataset_split          object\n",
      "file_name              object\n",
      "subject_type           object\n",
      "subject_id             object\n",
      "channel                object\n",
      "trial                   int64\n",
      "matching_condition     object\n",
      "Delta                 float64\n",
      "Theta                 float64\n",
      "Alpha                 float64\n",
      "Beta                  float64\n",
      "Gamma                 float64\n",
      "total_power           float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Force to numeric and report any conversion issues\n",
    "for col in BAND_COLS:\n",
    "    df_all[col] = pd.to_numeric(df_all[col], errors=\"coerce\")\n",
    "\n",
    "print(\"Data types:\")\n",
    "print(df_all.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values per column:\n",
      "dataset_split         0\n",
      "file_name             0\n",
      "subject_type          0\n",
      "subject_id            0\n",
      "channel               0\n",
      "trial                 0\n",
      "matching_condition    0\n",
      "Delta                 0\n",
      "Theta                 0\n",
      "Alpha                 0\n",
      "Beta                  0\n",
      "Gamma                 0\n",
      "total_power           0\n",
      "dtype: int64\n",
      "Rows with NaN in any feature: 0\n"
     ]
    }
   ],
   "source": [
    "# Check missing values\n",
    "print(\"Missing values per column:\")\n",
    "print(df_all.isna().sum())\n",
    "mask_nan_features = df_all[BAND_COLS].isna().any(axis=1)\n",
    "n_nan_rows = mask_nan_features.sum()\n",
    "print(f\"Rows with NaN in any feature: {n_nan_rows}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with non-finite feature values: 0\n"
     ]
    }
   ],
   "source": [
    "# Check for non-finite values\n",
    "mask_nonfinite = ~np.isfinite(df_all[BAND_COLS].to_numpy()).all(axis=1)\n",
    "n_nonfinite = mask_nonfinite.sum()\n",
    "print(f\"Rows with non-finite feature values: {n_nonfinite}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.3. Clip outlier check and process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st percentile for features:\n",
      " Delta          0.254321\n",
      "Theta          0.126250\n",
      "Alpha          0.108306\n",
      "Beta           0.191547\n",
      "Gamma          0.057350\n",
      "total_power    1.368929\n",
      "Name: 0.01, dtype: float64\n",
      "99th percentile for features:\n",
      " Delta          125.841570\n",
      "Theta           37.735722\n",
      "Alpha           42.706482\n",
      "Beta            23.624554\n",
      "Gamma           18.142717\n",
      "total_power    187.203228\n",
      "Name: 0.99, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# clip extreme outliers to stabilize covariance\n",
    "q_low = df_all[BAND_COLS].quantile(0.01)\n",
    "q_high = df_all[BAND_COLS].quantile(0.99)\n",
    "\n",
    "print(\"1st percentile for features:\\n\", q_low)\n",
    "print(\"99th percentile for features:\\n\", q_high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After clipping, feature summary (all):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Delta</th>\n",
       "      <th>Theta</th>\n",
       "      <th>Alpha</th>\n",
       "      <th>Beta</th>\n",
       "      <th>Gamma</th>\n",
       "      <th>total_power</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>60672.000000</td>\n",
       "      <td>60672.000000</td>\n",
       "      <td>60672.000000</td>\n",
       "      <td>60672.000000</td>\n",
       "      <td>60672.000000</td>\n",
       "      <td>60672.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>14.194337</td>\n",
       "      <td>5.210760</td>\n",
       "      <td>5.212400</td>\n",
       "      <td>4.203085</td>\n",
       "      <td>1.733450</td>\n",
       "      <td>31.266163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>19.773330</td>\n",
       "      <td>6.283680</td>\n",
       "      <td>7.203401</td>\n",
       "      <td>4.136839</td>\n",
       "      <td>2.747217</td>\n",
       "      <td>31.317794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.254321</td>\n",
       "      <td>0.126250</td>\n",
       "      <td>0.108306</td>\n",
       "      <td>0.191547</td>\n",
       "      <td>0.057350</td>\n",
       "      <td>1.368929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1%</th>\n",
       "      <td>0.254393</td>\n",
       "      <td>0.126368</td>\n",
       "      <td>0.108400</td>\n",
       "      <td>0.191558</td>\n",
       "      <td>0.057352</td>\n",
       "      <td>1.368965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>7.512234</td>\n",
       "      <td>3.138961</td>\n",
       "      <td>2.681088</td>\n",
       "      <td>2.934805</td>\n",
       "      <td>0.805226</td>\n",
       "      <td>22.106393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99%</th>\n",
       "      <td>125.826750</td>\n",
       "      <td>37.729023</td>\n",
       "      <td>42.705766</td>\n",
       "      <td>23.622675</td>\n",
       "      <td>18.129812</td>\n",
       "      <td>187.112226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>125.841570</td>\n",
       "      <td>37.735722</td>\n",
       "      <td>42.706482</td>\n",
       "      <td>23.624554</td>\n",
       "      <td>18.142717</td>\n",
       "      <td>187.203228</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Delta         Theta         Alpha          Beta         Gamma  \\\n",
       "count  60672.000000  60672.000000  60672.000000  60672.000000  60672.000000   \n",
       "mean      14.194337      5.210760      5.212400      4.203085      1.733450   \n",
       "std       19.773330      6.283680      7.203401      4.136839      2.747217   \n",
       "min        0.254321      0.126250      0.108306      0.191547      0.057350   \n",
       "1%         0.254393      0.126368      0.108400      0.191558      0.057352   \n",
       "50%        7.512234      3.138961      2.681088      2.934805      0.805226   \n",
       "99%      125.826750     37.729023     42.705766     23.622675     18.129812   \n",
       "max      125.841570     37.735722     42.706482     23.624554     18.142717   \n",
       "\n",
       "        total_power  \n",
       "count  60672.000000  \n",
       "mean      31.266163  \n",
       "std       31.317794  \n",
       "min        1.368929  \n",
       "1%         1.368965  \n",
       "50%       22.106393  \n",
       "99%      187.112226  \n",
       "max      187.203228  "
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clip outliers FIRST on full dataset\n",
    "df_all[BAND_COLS] = df_all[BAND_COLS].clip(lower=q_low, upper=q_high, axis=1)\n",
    "print(\"After clipping, feature summary (all):\")\n",
    "df_all[BAND_COLS].describe(percentiles=[0.01, 0.5, 0.99])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.4. Subject-wise train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# unique subjects in train: 16\n",
      "# unique subjects in test: 16\n",
      "# overlapping subjects: 16\n",
      "WARNING: Some subject_ids appear in BOTH train and test!\n"
     ]
    }
   ],
   "source": [
    "# make sure subjects don't appear in both splits\n",
    "train_subj = set(df_train[\"subject_id\"])\n",
    "test_subj = set(df_test[\"subject_id\"])\n",
    "overlap = train_subj & test_subj\n",
    "\n",
    "print(f\"# unique subjects in train: {len(train_subj)}\")\n",
    "print(f\"# unique subjects in test: {len(test_subj)}\")\n",
    "print(f\"# overlapping subjects: {len(overlap)}\")\n",
    "\n",
    "if overlap:\n",
    "    print(\"WARNING: Some subject_ids appear in BOTH train and test!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subject-wise train/test split\n",
    "subjects = df_all[\"subject_id\"].unique()\n",
    "rng = np.random.default_rng(42)\n",
    "rng.shuffle(subjects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train subjects: 8\n",
      "Test subjects : 8\n",
      "Overlap       : 0\n"
     ]
    }
   ],
   "source": [
    "# 50/50 split\n",
    "n_train = len(subjects) // 2\n",
    "train_subjects = set(subjects[:n_train])\n",
    "test_subjects = set(subjects[n_train:])\n",
    "\n",
    "df_train = df_all[df_all[\"subject_id\"].isin(train_subjects)].reset_index(drop=True)\n",
    "df_test = df_all[df_all[\"subject_id\"].isin(test_subjects)].reset_index(drop=True)\n",
    "\n",
    "print(\"Train subjects:\", len(train_subjects))\n",
    "print(\"Test subjects :\", len(test_subjects))\n",
    "print(\"Overlap       :\", len(train_subjects & test_subjects))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.5. Check of cleaned data for synthetic generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split counts with new split:\n",
      "dataset_split\n",
      "train    30336\n",
      "test     30336\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Overwrite dataset_split using the NEW split\n",
    "df_all[SPLIT_COL] = np.where(df_all[\"subject_id\"].isin(train_subjects), \"train\", \"test\")\n",
    "df_train = df_all[df_all[\"subject_id\"].isin(train_subjects)].reset_index(drop=True)\n",
    "df_test = df_all[df_all[\"subject_id\"].isin(test_subjects)].reset_index(drop=True)\n",
    "print(\"Split counts with new split:\")\n",
    "print(df_all[SPLIT_COL].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape with new split: (30336, 13)\n",
      "Test shape with new split: (30336, 13)\n"
     ]
    }
   ],
   "source": [
    "print(\"Train shape with new split:\", df_train.shape)\n",
    "print(\"Test shape with new split:\", df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train label x condition with new split:\n",
      "matching_condition  S1 obj  S2 match  S2 nomatch,\n",
      "subject_type                                     \n",
      "a                     5120      5120         4992\n",
      "c                     5120      5056         4928\n",
      "Test label x condition with new split:\n",
      "matching_condition  S1 obj  S2 match  S2 nomatch,\n",
      "subject_type                                     \n",
      "a                     5120      5120         4928\n",
      "c                     5120      5120         4928\n"
     ]
    }
   ],
   "source": [
    "print(\"Train label x condition with new split:\")\n",
    "print(pd.crosstab(df_train[LABEL_COL], df_train[\"matching_condition\"]))\n",
    "\n",
    "print(\"Test label x condition with new split:\")\n",
    "print(pd.crosstab(df_test[LABEL_COL], df_test[\"matching_condition\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.6. Encode labels & standardize features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final matrices ready for synthetic generation:\n",
      "X_train shape: (30336, 6)\n",
      "X_test shape: (30336, 6)\n",
      "y_train distribution: Counter({1: 15232, 0: 15104})\n",
      "y_test distribution: Counter({1: 15168, 0: 15168})\n",
      "Train condition counts: Counter({'S1 obj': 10240, 'S2 match': 10176, 'S2 nomatch,': 9920})\n",
      "Test condition counts: Counter({'S1 obj': 10240, 'S2 match': 10240, 'S2 nomatch,': 9856})\n"
     ]
    }
   ],
   "source": [
    "label_map = {\"c\": 0, \"a\": 1}\n",
    "\n",
    "y_train = df_train[LABEL_COL].map(label_map).values\n",
    "y_test = df_test[LABEL_COL].map(label_map).values\n",
    "\n",
    "X_train_raw = df_train[BAND_COLS].values\n",
    "X_test_raw = df_test[BAND_COLS].values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train_raw)\n",
    "X_test = scaler.transform(X_test_raw)\n",
    "\n",
    "conds_train = df_train[CONDITION_COL].values\n",
    "conds_test = df_test[CONDITION_COL].values\n",
    "\n",
    "print(\"Final matrices ready for synthetic generation:\")\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_train distribution:\", Counter(y_train))\n",
    "print(\"y_test distribution:\", Counter(y_test))\n",
    "print(\"Train condition counts:\", Counter(conds_train))\n",
    "print(\"Test condition counts:\", Counter(conds_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Synthetic Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1. Method 0: Mixup baseline Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mixup_baseline(real_features, n_synthetic=None, random_seed=42):\n",
    "    \"\"\"\n",
    "    Mixup-style baseline for synthetic EEG feature generation.\n",
    "    \n",
    "    - Interpolates between two real samples\n",
    "    - Adds small Gaussian noise proportional to feature std\n",
    "    - Ensures same dimensionality as the input (6 bands)\n",
    "    \"\"\"\n",
    "    np.random.seed(random_seed)\n",
    "\n",
    "    if n_synthetic is None:\n",
    "        n_synthetic = len(real_features)\n",
    "\n",
    "    n_samples, n_features = real_features.shape\n",
    "    synthetic_features = np.zeros((n_synthetic, n_features))\n",
    "\n",
    "    # Noise scale\n",
    "    noise_scale = 0.1 * np.std(real_features, axis=0)\n",
    "\n",
    "    for i in range(n_synthetic):\n",
    "        idx1, idx2 = np.random.choice(n_samples, 2, replace=False)\n",
    "\n",
    "        alpha = np.random.uniform(0.3, 0.7)\n",
    "        interpolated = alpha * real_features[idx1] + (1 - alpha) * real_features[idx2]\n",
    "\n",
    "        # Add Gaussian noise\n",
    "        noise = np.random.normal(0, noise_scale)\n",
    "        synthetic = interpolated + noise\n",
    "\n",
    "        # Ensure non-negative powers\n",
    "        synthetic = np.clip(synthetic, 0, None)\n",
    "\n",
    "        synthetic_features[i] = synthetic\n",
    "\n",
    "    return synthetic_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 30336 mixup synthetic samples...\n",
      "Shape: (30336, 6)\n"
     ]
    }
   ],
   "source": [
    "real_features = df_train[BAND_COLS].to_numpy()\n",
    "n_synthetic_samples = real_features.shape[0]\n",
    "\n",
    "print(f\"Generating {n_synthetic_samples} mixup synthetic samples...\")\n",
    "\n",
    "synthetic_features_mixup = generate_mixup_baseline(\n",
    "    real_features=real_features,\n",
    "    n_synthetic=n_synthetic_samples,\n",
    "    random_seed=RANDOM_SEED\n",
    ")\n",
    "print(\"Shape:\", synthetic_features_mixup.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_syn_mixup = scaler.transform(synthetic_features_mixup)\n",
    "y_syn_mixup = y_train.copy()\n",
    "perm = np.random.permutation(len(y_syn_mixup))\n",
    "X_syn_mixup = X_syn_mixup[perm]\n",
    "y_syn_mixup = y_syn_mixup[perm]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2.2 Method 1: Correlation Sampling Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_correlation_based_eeg(real_features, band_names, n_synthetic=None, random_seed=42):\n",
    "    \"\"\"\n",
    "    Generate synthetic EEG band features using a correlation sampling method.\n",
    "    real_features: np.ndarray of shape (n_samples, n_bands)\n",
    "    band_names: list of band names (for logging, same order as columns)\n",
    "    n_synthetic: number of synthetic samples to generate. If None, use n_synthetic = real_features.shape[0]\n",
    "    \"\"\"\n",
    "    np.random.seed(random_seed)\n",
    "    \n",
    "    if n_synthetic is None:\n",
    "        n_synthetic = real_features.shape[0]\n",
    "    correlation_matrix = np.corrcoef(real_features.T)\n",
    "    mean_features = np.mean(real_features, axis=0)\n",
    "    std_features = np.std(real_features, axis=0)\n",
    "    \n",
    "    print(\"Correlation Matrix of Frequency Bands:\")\n",
    "    for i, band1 in enumerate(band_names):\n",
    "        for j, band2 in enumerate(band_names):\n",
    "            if j >= i:\n",
    "                print(f\"{band1:11s} - {band2:11s}: {correlation_matrix[i, j]:7.3f}\")\n",
    "    \n",
    "    # construct covariance matrix: cov = D * Corr * D\n",
    "    covariance_matrix = np.outer(std_features, std_features) * correlation_matrix\n",
    "    \n",
    "    # Generate synthetic features preserving the correlation structure\n",
    "    synthetic_features = np.random.multivariate_normal(\n",
    "        mean_features,\n",
    "        covariance_matrix,\n",
    "        size=n_synthetic\n",
    "    )\n",
    "    # Ensure non-negative powers (since band powers should be >= 0)\n",
    "    synthetic_features = np.clip(synthetic_features, a_min=0.0, a_max=None)\n",
    "    print(f\"Generated {n_synthetic} synthetic feature vectors\")\n",
    "    print(\"Correlation structure preserved (in expectation)\")\n",
    "    return synthetic_features, correlation_matrix, covariance_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 30336 synthetic samples using correlation sampling\n",
      "Correlation Matrix of Frequency Bands:\n",
      "Delta       - Delta      :   1.000\n",
      "Delta       - Theta      :   0.634\n",
      "Delta       - Alpha      :   0.282\n",
      "Delta       - Beta       :   0.264\n",
      "Delta       - Gamma      :   0.108\n",
      "Delta       - total_power:   0.880\n",
      "Theta       - Theta      :   1.000\n",
      "Theta       - Alpha      :   0.365\n",
      "Theta       - Beta       :   0.291\n",
      "Theta       - Gamma      :   0.110\n",
      "Theta       - total_power:   0.719\n",
      "Alpha       - Alpha      :   1.000\n",
      "Alpha       - Beta       :   0.305\n",
      "Alpha       - Gamma      :   0.056\n",
      "Alpha       - total_power:   0.505\n",
      "Beta        - Beta       :   1.000\n",
      "Beta        - Gamma      :   0.720\n",
      "Beta        - total_power:   0.573\n",
      "Gamma       - Gamma      :   1.000\n",
      "Gamma       - total_power:   0.391\n",
      "total_power - total_power:   1.000\n",
      "Generated 30336 synthetic feature vectors\n",
      "Correlation structure preserved (in expectation)\n",
      "Shape of synthetic features (raw): (30336, 6)\n"
     ]
    }
   ],
   "source": [
    "# Use the band features from the TRAIN set after clipping + subject-wise split\n",
    "real_features = df_train[BAND_COLS].to_numpy()\n",
    "band_names = BAND_COLS\n",
    "\n",
    "# Generate the same number of synthetic samples as real training samples\n",
    "n_synthetic_samples = real_features.shape[0]\n",
    "print(f\"Generating {n_synthetic_samples} synthetic samples using correlation sampling\")\n",
    "\n",
    "synthetic_features_corr_raw, corr_matrix_corr, cov_corr = generate_correlation_based_eeg(\n",
    "    real_features=real_features,\n",
    "    band_names=band_names,\n",
    "    n_synthetic=n_synthetic_samples,\n",
    "    random_seed=RANDOM_SEED,\n",
    ")\n",
    "print(f\"Shape of synthetic features (raw): {synthetic_features_corr_raw.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_syn_corr shape: (30336, 6)\n",
      "First 5 standardized synthetic samples:\n",
      "[[-0.57904767  0.55278216 -0.74434845 -0.28479666  0.14378794 -0.4558341 ]\n",
      " [-0.67371446 -0.80425419 -0.74434845 -1.01361852 -0.68446539 -0.96143269]\n",
      " [-0.67371446 -0.3452406   2.02880318  0.04906978 -0.1556162  -0.01796368]\n",
      " [ 0.33528254  0.49086969  0.20279222  1.81970979  2.63919986  1.08293578]\n",
      " [ 0.54250611  0.71968462  0.91355495 -0.60091889 -0.50811442  0.51950002]]\n"
     ]
    }
   ],
   "source": [
    "# Standardize synthetic features using the scaler fit on REAL training data\n",
    "X_syn_corr = scaler.transform(synthetic_features_corr_raw)\n",
    "print(\"X_syn_corr shape:\", X_syn_corr.shape)\n",
    "print(\"First 5 standardized synthetic samples:\")\n",
    "print(X_syn_corr[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synthetic label distribution: Counter({1: 15232, 0: 15104})\n"
     ]
    }
   ],
   "source": [
    "# Reuse y_train distribution for synthetic labels\n",
    "# This preserves the class balance, even though the generator itself is label-agnostic\n",
    "y_syn_corr = y_train.copy()\n",
    "\n",
    "# shuffle (X_syn_corr, y_syn_corr) together to avoid any accidental order structure\n",
    "perm = np.random.permutation(len(y_syn_corr))\n",
    "X_syn_corr = X_syn_corr[perm]\n",
    "y_syn_corr = y_syn_corr[perm]\n",
    "\n",
    "print(\"Synthetic label distribution:\", Counter(y_syn_corr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.3. Method 2: WGAN-GP Synthetic Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real features shape for WGAN-GP: (30336, 6)\n",
      "Number of synthetic samples to generate: 30336\n"
     ]
    }
   ],
   "source": [
    "# Use the same band-power features\n",
    "real_features = df_train[BAND_COLS].to_numpy().astype(np.float32)\n",
    "n_synthetic_samples = real_features.shape[0]\n",
    "\n",
    "print(\"Real features shape for WGAN-GP:\", real_features.shape)\n",
    "print(\"Number of synthetic samples to generate:\", n_synthetic_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_wgangp_eeg(real_features, n_synthetic=100, noise_dim=16, hidden_dim=64, n_critic=5, gp_lambda=10.0, lr=1e-4, batch_size=128, epochs=300, random_seed=42):\n",
    "    \"\"\"\n",
    "    Train a compact WGAN-GP on band-power features and return synthetic samples.\n",
    "    real_features: np.ndarray of shape (n_samples, n_features) â€“ here (30336, 6)\n",
    "    \"\"\"\n",
    "    torch.manual_seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # real_features already clipped and cleaned\n",
    "    data = torch.from_numpy(real_features.astype(np.float32))\n",
    "    dataset = TensorDataset(data)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "    feature_dim = real_features.shape[1]  # should be 6 (Delta, Theta, Alpha, Beta, Gamma, total_power)\n",
    "\n",
    "    class Generator(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.net = nn.Sequential(\n",
    "                nn.Linear(noise_dim, hidden_dim),\n",
    "                nn.LayerNorm(hidden_dim),\n",
    "                nn.GELU(),\n",
    "                nn.Linear(hidden_dim, hidden_dim),\n",
    "                nn.GELU(),\n",
    "                nn.Linear(hidden_dim, feature_dim),\n",
    "            )\n",
    "\n",
    "        def forward(self, z):\n",
    "            x = self.net(z)\n",
    "            # softplus keeps outputs positive but smooth\n",
    "            return torch.nn.functional.softplus(x)\n",
    "\n",
    "    class Critic(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.net = nn.Sequential(\n",
    "                nn.Linear(feature_dim, hidden_dim),\n",
    "                nn.LayerNorm(hidden_dim),\n",
    "                nn.GELU(),\n",
    "                nn.Linear(hidden_dim, hidden_dim),\n",
    "                nn.GELU(),\n",
    "                nn.Linear(hidden_dim, 1),\n",
    "            )\n",
    "\n",
    "        def forward(self, x):\n",
    "            return self.net(x)\n",
    "\n",
    "    def gradient_penalty(critic, real, fake):\n",
    "        batch_size = real.size(0)\n",
    "        epsilon = torch.rand(batch_size, 1, device=real.device)\n",
    "        epsilon = epsilon.expand_as(real)\n",
    "        interpolated = epsilon * real + (1 - epsilon) * fake\n",
    "        interpolated.requires_grad_(True)\n",
    "        mixed_scores = critic(interpolated)\n",
    "        grad = torch.autograd.grad(\n",
    "            outputs=mixed_scores,\n",
    "            inputs=interpolated,\n",
    "            grad_outputs=torch.ones_like(mixed_scores),\n",
    "            create_graph=True,\n",
    "            retain_graph=True,\n",
    "        )[0]\n",
    "        grad = grad.view(batch_size, -1)\n",
    "        gp = ((grad.norm(2, dim=1) - 1) ** 2).mean()\n",
    "        return gp\n",
    "\n",
    "    G = Generator().to(device)\n",
    "    D = Critic().to(device)\n",
    "\n",
    "    opt_G = optim.Adam(G.parameters(), lr=lr, betas=(0.5, 0.9))\n",
    "    opt_D = optim.Adam(D.parameters(), lr=lr, betas=(0.5, 0.9))\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for i, (real_batch,) in enumerate(loader):\n",
    "            real_batch = real_batch.to(device)\n",
    "\n",
    "            # Critic updates\n",
    "            for _ in range(n_critic):\n",
    "                z = torch.randn(real_batch.size(0), noise_dim, device=device)\n",
    "                fake_batch = G(z).detach()\n",
    "\n",
    "                opt_D.zero_grad()\n",
    "                critic_real = D(real_batch).mean()\n",
    "                critic_fake = D(fake_batch).mean()\n",
    "                gp = gradient_penalty(D, real_batch, fake_batch)\n",
    "                loss_D = -(critic_real - critic_fake) + gp_lambda * gp\n",
    "                loss_D.backward()\n",
    "                opt_D.step()\n",
    "\n",
    "            # Generator update\n",
    "            z = torch.randn(real_batch.size(0), noise_dim, device=device)\n",
    "            opt_G.zero_grad()\n",
    "            fake_batch = G(z)\n",
    "            loss_G = -D(fake_batch).mean()\n",
    "            loss_G.backward()\n",
    "            opt_G.step()\n",
    "\n",
    "        # simple monitoring every 50 epochs\n",
    "        if (epoch + 1) % 50 == 0:\n",
    "            with torch.no_grad():\n",
    "                z = torch.randn(batch_size, noise_dim, device=device)\n",
    "                preview = G(z).cpu().numpy()\n",
    "            preview_mean = preview.mean(axis=0)\n",
    "            print(\n",
    "                f\"Epoch {epoch + 1:03d}/{epochs} | \"\n",
    "                f\"D: {loss_D.item():.4f} | G: {loss_G.item():.4f} | \"\n",
    "                f\"preview mean={np.round(preview_mean, 3)}\"\n",
    "            )\n",
    "\n",
    "    # Generate n_synthetic samples\n",
    "    G.eval()\n",
    "    with torch.no_grad():\n",
    "        synth_chunks = []\n",
    "        remaining = n_synthetic\n",
    "        while remaining > 0:\n",
    "            current = min(batch_size, remaining)\n",
    "            z = torch.randn(current, noise_dim, device=device)\n",
    "            synth = G(z).cpu().numpy()\n",
    "            synth_chunks.append(synth)\n",
    "            remaining -= current\n",
    "\n",
    "    synthetic = np.vstack(synth_chunks)\n",
    "    synthetic = np.clip(synthetic, a_min=0.0, a_max=None)\n",
    "    return synthetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training WGAN-GP generator on band-power features...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 050/300 | D: -0.9164 | G: 101.8643 | preview mean=[12.439  4.572  4.873  5.001  2.099 29.57 ]\n",
      "Epoch 100/300 | D: -0.9513 | G: 95.1498 | preview mean=[12.229  3.953  5.142  4.606  2.421 29.934]\n",
      "Epoch 150/300 | D: -1.1573 | G: 95.1559 | preview mean=[14.428  4.374  4.736  4.736  1.577 30.608]\n",
      "Epoch 200/300 | D: -4.8910 | G: 86.5985 | preview mean=[11.266  3.649  3.564  5.073  3.035 26.998]\n",
      "Epoch 250/300 | D: 2.6741 | G: 85.2702 | preview mean=[11.784  5.169  4.427  4.363  2.219 29.297]\n",
      "Epoch 300/300 | D: -1.0484 | G: 77.1386 | preview mean=[16.441  4.64   4.221  5.543  2.885 34.137]\n",
      "Generated 30336 WGAN-GP samples\n",
      "Shape: (30336, 6)\n"
     ]
    }
   ],
   "source": [
    "print(\"Training WGAN-GP generator on band-power features...\")\n",
    "if torch is None:\n",
    "    print(\"PyTorch not installed; skipping WGAN-GP synthesis.\")\n",
    "    synthetic_features_wgangp = None\n",
    "else:\n",
    "    synthetic_features_wgangp = generate_wgangp_eeg(\n",
    "        real_features=real_features,\n",
    "        n_synthetic=n_synthetic_samples,\n",
    "        random_seed=RANDOM_SEED,\n",
    "    )\n",
    "    print(f\"Generated {len(synthetic_features_wgangp)} WGAN-GP samples\")\n",
    "    print(f\"Shape: {synthetic_features_wgangp.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_syn_wgangp shape: (30336, 6)\n",
      "y_syn_wgangp distribution: Counter({1: 15232, 0: 15104})\n"
     ]
    }
   ],
   "source": [
    "# Standardize synthetic samples\n",
    "X_syn_wgangp = scaler.transform(synthetic_features_wgangp)\n",
    "y_syn_wgangp = y_train.copy()\n",
    "perm = np.random.permutation(len(y_syn_wgangp))\n",
    "X_syn_wgangp = X_syn_wgangp[perm]\n",
    "y_syn_wgangp = y_syn_wgangp[perm]\n",
    "\n",
    "print(\"X_syn_wgangp shape:\", X_syn_wgangp.shape)\n",
    "print(\"y_syn_wgangp distribution:\", Counter(y_syn_wgangp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.4. Method 3: Gaussian Copula Sampling Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _allocate_samples_by_class(labels, n_total):\n",
    "    \"\"\"\n",
    "    Allocate synthetic samples per class, preserving empirical ratios.\n",
    "    Returns a dict: {class_label: n_synth_for_that_class}\n",
    "    \"\"\"\n",
    "    classes, counts = np.unique(labels, return_counts=True)\n",
    "    ratios = counts / counts.sum()\n",
    "    expected = ratios * n_total\n",
    "\n",
    "    allocated = np.floor(expected).astype(int)\n",
    "    remainder = n_total - allocated.sum()\n",
    "\n",
    "    if remainder > 0:\n",
    "        remainders = expected - allocated\n",
    "        order = np.argsort(remainders)[::-1]\n",
    "        for idx in order[:remainder]:\n",
    "            allocated[idx] += 1\n",
    "\n",
    "    return dict(zip(classes, allocated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_gaussian_copula_eeg(real_features, labels, n_synthetic=100, random_seed=42):\n",
    "    \"\"\"\n",
    "    Gaussian copula sampling:\n",
    "    1. For each class (0/1), fit a quantile transformer to map marginals -> N(0,1)\n",
    "    2. Estimate regularised covariance (Ledoit-Wolf) in that latent space\n",
    "    3. Sample multivariate normal per class and invert the transform\n",
    "    4. Clip to non-negative band powers\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(random_seed)\n",
    "    allocation = _allocate_samples_by_class(labels, n_synthetic)\n",
    "    synthetic_blocks = []\n",
    "\n",
    "    print(\"Generating Gaussian copula samples per class...\")\n",
    "    for cls, n_cls_samples in allocation.items():\n",
    "        class_features = real_features[labels == cls]\n",
    "        if len(class_features) == 0 or n_cls_samples == 0:\n",
    "            continue\n",
    "\n",
    "        # quantile transformer to approximate Gaussian marginals\n",
    "        n_quantiles = min(len(class_features), 1000)\n",
    "        transformer = QuantileTransformer(\n",
    "            n_quantiles=n_quantiles,\n",
    "            output_distribution=\"normal\",\n",
    "            random_state=random_seed,\n",
    "        )\n",
    "\n",
    "        latent = transformer.fit_transform(class_features)\n",
    "\n",
    "        # Ledoit-Wolf for stable covariance\n",
    "        cov_estimator = LedoitWolf().fit(latent)\n",
    "        latent_mean = cov_estimator.location_\n",
    "        latent_cov = cov_estimator.covariance_\n",
    "\n",
    "        # sample in latent Gaussian space\n",
    "        latent_samples = rng.multivariate_normal(\n",
    "            latent_mean,\n",
    "            latent_cov,\n",
    "            size=n_cls_samples,\n",
    "        )\n",
    "\n",
    "        # invert back to band-power space\n",
    "        samples = transformer.inverse_transform(latent_samples)\n",
    "\n",
    "        # enforce non-negativity for power features\n",
    "        samples = np.clip(samples, a_min=0, a_max=None)\n",
    "        synthetic_blocks.append(samples)\n",
    "\n",
    "        print(f\"  Class {cls}: real={len(class_features)}, synthetic={n_cls_samples}\")\n",
    "\n",
    "    if not synthetic_blocks:\n",
    "        raise ValueError(\"No synthetic samples were generated. Check class labels.\")\n",
    "\n",
    "    synthetic_features = np.vstack(synthetic_blocks)\n",
    "    return synthetic_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Gaussian copula samples per class...\n",
      "  Class 0: real=15104, synthetic=15104\n",
      "  Class 1: real=15232, synthetic=15232\n",
      "Generated 30336 Gaussian copula samples\n",
      "Shape: (30336, 6)\n"
     ]
    }
   ],
   "source": [
    "synthetic_features_copula = generate_gaussian_copula_eeg(\n",
    "    real_features=real_features,\n",
    "    labels=y_train,\n",
    "    n_synthetic=n_synthetic_samples,\n",
    "    random_seed=RANDOM_SEED,\n",
    ")\n",
    "\n",
    "print(f\"Generated {len(synthetic_features_copula)} Gaussian copula samples\")\n",
    "print(f\"Shape: {synthetic_features_copula.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_syn_copula shape: (30336, 6)\n",
      "y_syn_copula distribution: Counter({1: 15232, 0: 15104})\n"
     ]
    }
   ],
   "source": [
    "# Standardize with the same scaler used for X_train / X_test\n",
    "X_syn_copula = scaler.transform(synthetic_features_copula)\n",
    "\n",
    "# Preserve class ratio but randomize ordering\n",
    "y_syn_copula = y_train.copy()\n",
    "perm = np.random.permutation(len(y_syn_copula))\n",
    "X_syn_copula = X_syn_copula[perm]\n",
    "y_syn_copula = y_syn_copula[perm]\n",
    "\n",
    "print(\"X_syn_copula shape:\", X_syn_copula.shape)\n",
    "print(\"y_syn_copula distribution:\", Counter(y_syn_copula))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.5. Method 4: Classwise Interpolation Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_classwise_interpolation_eeg(real_features, labels, n_synthetic=100, random_seed=42, k_neighbors=8, noise_scale=0.02,):\n",
    "    \"\"\"\n",
    "    Class-conditional interpolation inspired by SMOTE.\n",
    "    Operates in log-power space to better capture multiplicative structure.\n",
    "    \n",
    "    real_features: np.ndarray, shape (N, n_bands) â€“ here (N, 6)\n",
    "    labels       : np.ndarray, shape (N,) with class labels (0/1)\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(random_seed)\n",
    "    allocation = _allocate_samples_by_class(labels, n_synthetic)\n",
    "    synthetic_samples = []\n",
    "\n",
    "    # work in log-power space\n",
    "    log_features = np.log1p(real_features)\n",
    "\n",
    "    for cls, n_cls_samples in allocation.items():\n",
    "        class_mask = labels == cls\n",
    "        class_features_log = log_features[class_mask]\n",
    "        if len(class_features_log) == 0 or n_cls_samples == 0:\n",
    "            continue\n",
    "\n",
    "        # effective neighbors (avoid k > n-1)\n",
    "        n_neighbors_eff = min(k_neighbors, len(class_features_log) - 1)\n",
    "        if n_neighbors_eff <= 0:\n",
    "            # fallback: jitter existing samples\n",
    "            base_samples = np.repeat(\n",
    "                class_features_log,\n",
    "                repeats=max(1, n_cls_samples // max(1, len(class_features_log))),\n",
    "                axis=0,\n",
    "            )\n",
    "            base_samples = base_samples[:n_cls_samples]\n",
    "            jitter = rng.normal(0, noise_scale, size=base_samples.shape)\n",
    "            augmented = base_samples + jitter\n",
    "            synthetic_samples.append(np.expm1(augmented))\n",
    "            continue\n",
    "\n",
    "        nbrs = NearestNeighbors(n_neighbors=n_neighbors_eff + 1)\n",
    "        nbrs.fit(class_features_log)\n",
    "\n",
    "        class_std = np.std(class_features_log, axis=0, ddof=1)\n",
    "        class_std[class_std == 0] = 1e-6\n",
    "\n",
    "        for _ in range(n_cls_samples):\n",
    "            # pick anchor\n",
    "            idx = rng.integers(len(class_features_log))\n",
    "            # get neighbors (excluding itself)\n",
    "            neighbors = nbrs.kneighbors(\n",
    "                class_features_log[idx].reshape(1, -1),\n",
    "                return_distance=False,\n",
    "            )[0]\n",
    "            neighbors = neighbors[neighbors != idx]\n",
    "\n",
    "            if len(neighbors) == 0:\n",
    "                neighbor_idx = idx\n",
    "            else:\n",
    "                neighbor_idx = rng.choice(neighbors)\n",
    "\n",
    "            # interpolate in log-space\n",
    "            alpha = rng.uniform(0.2, 0.8)\n",
    "            interpolated = (\n",
    "                alpha * class_features_log[idx]\n",
    "                + (1 - alpha) * class_features_log[neighbor_idx]\n",
    "            )\n",
    "\n",
    "            # add small Gaussian noise in log-space\n",
    "            noise = rng.normal(0, noise_scale, size=class_features_log.shape[1]) * class_std\n",
    "            synthetic_log = interpolated + noise\n",
    "\n",
    "            synthetic_samples.append(np.expm1(synthetic_log))\n",
    "\n",
    "    if not synthetic_samples:\n",
    "        raise ValueError(\"Interpolation generator did not create any samples.\")\n",
    "\n",
    "    synthetic_features = np.vstack(synthetic_samples)\n",
    "    synthetic_features = np.clip(synthetic_features, a_min=0, a_max=None)\n",
    "    return synthetic_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 30336 interpolation-based samples\n",
      "Shape: (30336, 6)\n"
     ]
    }
   ],
   "source": [
    "synthetic_features_interp = generate_classwise_interpolation_eeg(\n",
    "    real_features=real_features,\n",
    "    labels=y_train,\n",
    "    n_synthetic=n_synthetic_samples,\n",
    "    random_seed=RANDOM_SEED,\n",
    "    k_neighbors=10,\n",
    "    noise_scale=0.015,\n",
    ")\n",
    "\n",
    "print(f\"Generated {len(synthetic_features_interp)} interpolation-based samples\")\n",
    "print(f\"Shape: {synthetic_features_interp.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_syn_interp shape: (30336, 6)\n",
      "y_syn_interp distribution: Counter({1: 15232, 0: 15104})\n"
     ]
    }
   ],
   "source": [
    "# Standardize using the same scaler\n",
    "X_syn_interp = scaler.transform(synthetic_features_interp)\n",
    "\n",
    "# Reuse label distribution and shuffle\n",
    "y_syn_interp = y_train.copy()\n",
    "perm = np.random.permutation(len(y_syn_interp))\n",
    "X_syn_interp = X_syn_interp[perm]\n",
    "y_syn_interp = y_syn_interp[perm]\n",
    "\n",
    "print(\"X_syn_interp shape:\", X_syn_interp.shape)\n",
    "print(\"y_syn_interp distribution:\", Counter(y_syn_interp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ollama_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
