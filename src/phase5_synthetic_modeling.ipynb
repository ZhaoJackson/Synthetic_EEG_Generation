{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7774a967",
   "metadata": {},
   "source": [
    "# Phase 5: Hyperparameter tuning for Classwise Interpolation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2919ab",
   "metadata": {},
   "source": [
    "## 5.0. Path & Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7d45af64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from itertools import product\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "\n",
    "from scipy.stats import ks_2samp, spearmanr\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "DATA_DIR = Path(\"../output/band_extraction\")\n",
    "SYN_BASE_DIR = Path(\"../output/synthetic_generation\")\n",
    "EVAL_BASE_DIR = Path(\"../output/model_tuning\")\n",
    "EVAL_BASE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "BAND_COLS = [\"Delta\", \"Theta\", \"Alpha\", \"Beta\", \"Gamma\"]\n",
    "CANONICAL_CONDITIONS = [\"S1\", \"S2_match\", \"S2_nomatch\"]\n",
    "LABEL_COL = \"label\"\n",
    "COND_COL  = \"condition\"\n",
    "SOURCE_COL = \"source\"\n",
    "\n",
    "MODEL_INFO = {\"interp\": \"Classwise Interpolation\"}\n",
    "\n",
    "model_key  = \"interp\"\n",
    "model_name = MODEL_INFO[model_key]\n",
    "\n",
    "interp_dir = SYN_BASE_DIR / model_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce9aa38",
   "metadata": {},
   "source": [
    "## 5.1. Base real data and baseline best model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8872c6",
   "metadata": {},
   "source": [
    "### 5.1.1. Best Model Data Generation Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b1f51f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline REAL shape: (30336, 9)\n",
      "Baseline SYN  shape: (30336, 9)\n"
     ]
    }
   ],
   "source": [
    "# Baseline Phase-4 outputs\n",
    "interp_real_fp = interp_dir / f\"{MODEL_KEY}_real.csv\"\n",
    "interp_syn_fp  = interp_dir / f\"{MODEL_KEY}_syn.csv\"\n",
    "\n",
    "interp_real = pd.read_csv(interp_real_fp)\n",
    "interp_syn  = pd.read_csv(interp_syn_fp)\n",
    "\n",
    "print(\"Baseline REAL shape:\", interp_real.shape)\n",
    "print(\"Baseline SYN  shape:\", interp_syn.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a1f5d142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline real data shape: (30336, 9)\n",
      "Baseline synthetic data shape: (30336, 9)\n"
     ]
    }
   ],
   "source": [
    "print(\"Baseline real data shape:\", interp_real.shape)\n",
    "print(\"Baseline synthetic data shape:\", interp_syn.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "08466ee6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Delta</th>\n",
       "      <th>Theta</th>\n",
       "      <th>Alpha</th>\n",
       "      <th>Beta</th>\n",
       "      <th>Gamma</th>\n",
       "      <th>total_power</th>\n",
       "      <th>label</th>\n",
       "      <th>condition</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.292732</td>\n",
       "      <td>0.249902</td>\n",
       "      <td>-0.610662</td>\n",
       "      <td>0.411409</td>\n",
       "      <td>1.345733</td>\n",
       "      <td>0.284397</td>\n",
       "      <td>1</td>\n",
       "      <td>S1 obj</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.375690</td>\n",
       "      <td>0.290176</td>\n",
       "      <td>-0.585701</td>\n",
       "      <td>2.490218</td>\n",
       "      <td>3.991401</td>\n",
       "      <td>0.927912</td>\n",
       "      <td>1</td>\n",
       "      <td>S1 obj</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.300488</td>\n",
       "      <td>0.329996</td>\n",
       "      <td>-0.448042</td>\n",
       "      <td>4.006910</td>\n",
       "      <td>4.693634</td>\n",
       "      <td>2.271983</td>\n",
       "      <td>1</td>\n",
       "      <td>S1 obj</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.124150</td>\n",
       "      <td>0.066128</td>\n",
       "      <td>-0.374884</td>\n",
       "      <td>4.006910</td>\n",
       "      <td>4.693634</td>\n",
       "      <td>2.648213</td>\n",
       "      <td>1</td>\n",
       "      <td>S1 obj</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.037956</td>\n",
       "      <td>-0.381335</td>\n",
       "      <td>-0.659416</td>\n",
       "      <td>-0.155800</td>\n",
       "      <td>0.179559</td>\n",
       "      <td>-0.249257</td>\n",
       "      <td>1</td>\n",
       "      <td>S1 obj</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Delta     Theta     Alpha      Beta     Gamma  total_power  label  \\\n",
       "0  0.292732  0.249902 -0.610662  0.411409  1.345733     0.284397      1   \n",
       "1  0.375690  0.290176 -0.585701  2.490218  3.991401     0.927912      1   \n",
       "2 -0.300488  0.329996 -0.448042  4.006910  4.693634     2.271983      1   \n",
       "3 -0.124150  0.066128 -0.374884  4.006910  4.693634     2.648213      1   \n",
       "4 -0.037956 -0.381335 -0.659416 -0.155800  0.179559    -0.249257      1   \n",
       "\n",
       "  condition source  \n",
       "0    S1 obj   real  \n",
       "1    S1 obj   real  \n",
       "2    S1 obj   real  \n",
       "3    S1 obj   real  \n",
       "4    S1 obj   real  "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interp_real.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "2e0b00c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_features = interp_real[BAND_COLS].to_numpy()\n",
    "real_labels = interp_real[\"label\"].to_numpy()\n",
    "real_conds = interp_real[\"condition\"].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "3fbf4513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real condition counts: condition\n",
      "S1 obj         10240\n",
      "S2 match       10176\n",
      "S2 nomatch,     9920\n",
      "Name: count, dtype: int64\n",
      "Syn  condition counts: condition\n",
      "S1 obj         10240\n",
      "S2 match       10176\n",
      "S2 nomatch,     9920\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Real condition counts:\", interp_real[COND_COL].value_counts())\n",
    "print(\"Syn  condition counts:\", interp_syn[COND_COL].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db792a2",
   "metadata": {},
   "source": [
    "## 5.2. Define Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e713a625",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_condition_slice(df: pd.DataFrame, cond_tag: str):\n",
    "    mask = (df[COND_COL] == cond_tag)\n",
    "    df_cond = df.loc[mask].reset_index(drop=True)\n",
    "\n",
    "    X_cond = df_cond[BAND_COLS].to_numpy()\n",
    "    y_cond = df_cond[LABEL_COL].to_numpy()\n",
    "\n",
    "    return X_cond, y_cond, df_cond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d1798a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InterpGenerator(nn.Module):\n",
    "    def __init__(self, input_dim=6, hidden_dim=32, output_dim=6):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "208d97a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune_interp_model_for_condition(\n",
    "    cond_tag: str,\n",
    "    X_cond_real: np.ndarray,\n",
    "    n_epochs: int = 80,\n",
    "    patience: int = 8,\n",
    "    hyper_grid: dict | None = None,\n",
    "    noise_std: float = 0.05,\n",
    "):\n",
    "    \"\"\"\n",
    "    Fine-tune an INTERP-style generator on REAL data for a single condition.\n",
    "\n",
    "    X_cond_real: (N, 6) normalized band features for this condition.\n",
    "    Early stopping is based on reconstruction loss.\n",
    "    Hyperparameter selection is based on best (lowest) final loss.\n",
    "    \"\"\"\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    if hyper_grid is None:\n",
    "        hyper_grid = {\n",
    "            \"hidden_dim\":   [32, 64],\n",
    "            \"lr\":           [1e-3, 5e-4],\n",
    "            \"weight_decay\": [0.0, 1e-4],\n",
    "            \"batch_size\":   [128, 256],\n",
    "        }\n",
    "\n",
    "    X_tensor = torch.from_numpy(X_cond_real.astype(np.float32))\n",
    "\n",
    "    best_cfg   = None\n",
    "    best_score = np.inf\n",
    "    best_state = None\n",
    "\n",
    "    # Iterate over hyperparameter combos\n",
    "    for hd, lr, wd, bs in product(\n",
    "        hyper_grid[\"hidden_dim\"],\n",
    "        hyper_grid[\"lr\"],\n",
    "        hyper_grid[\"weight_decay\"],\n",
    "        hyper_grid[\"batch_size\"],\n",
    "    ):\n",
    "        print(f\"\\n[{cond_tag}] Trying config: hidden_dim={hd}, lr={lr}, wd={wd}, bs={bs}\")\n",
    "\n",
    "        dataset = TensorDataset(X_tensor)\n",
    "        loader  = DataLoader(dataset, batch_size=bs, shuffle=True)\n",
    "\n",
    "        gen = InterpGenerator(\n",
    "            input_dim=len(BAND_COLS),\n",
    "            hidden_dim=hd,\n",
    "            output_dim=len(BAND_COLS),\n",
    "        ).to(device)\n",
    "\n",
    "        optimizer = optim.Adam(gen.parameters(), lr=lr, weight_decay=wd)\n",
    "        criterion = nn.MSELoss()\n",
    "\n",
    "        best_epoch_loss = np.inf\n",
    "        epochs_no_improve = 0\n",
    "\n",
    "        for epoch in range(1, n_epochs + 1):\n",
    "            gen.train()\n",
    "            total_loss = 0.0\n",
    "\n",
    "            for (x_batch,) in loader:\n",
    "                x_batch = x_batch.to(device)\n",
    "\n",
    "                if noise_std > 0.0:\n",
    "                    noise = torch.randn_like(x_batch) * noise_std\n",
    "                    x_noisy = x_batch + noise\n",
    "                else:\n",
    "                    x_noisy = x_batch\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                x_hat = gen(x_noisy)\n",
    "                loss  = criterion(x_hat, x_batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                total_loss += loss.item() * x_batch.size(0)\n",
    "\n",
    "            avg_loss = total_loss / len(dataset)\n",
    "            print(f\"[{cond_tag}] Epoch {epoch:03d} | recon loss={avg_loss:.6f}\")\n",
    "\n",
    "            # Early stopping on reconstruction loss\n",
    "            if avg_loss + 1e-6 < best_epoch_loss:\n",
    "                best_epoch_loss = avg_loss\n",
    "                epochs_no_improve = 0\n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "                if epochs_no_improve >= patience:\n",
    "                    print(f\"[{cond_tag}] Early stopping at epoch {epoch}\")\n",
    "                    break\n",
    "\n",
    "        # After training for this config, select best on loss\n",
    "        if best_epoch_loss < best_score:\n",
    "            best_score = best_epoch_loss\n",
    "            best_cfg   = {\n",
    "                \"hidden_dim\": hd,\n",
    "                \"lr\": lr,\n",
    "                \"weight_decay\": wd,\n",
    "                \"batch_size\": bs,\n",
    "                \"best_epoch_loss\": best_epoch_loss,\n",
    "            }\n",
    "            best_state = gen.state_dict()\n",
    "\n",
    "    # Rebuild generator with best config and load its weights\n",
    "    print(f\"\\n[{cond_tag}] BEST CONFIG:\", best_cfg)\n",
    "\n",
    "    best_gen = InterpGenerator(\n",
    "        input_dim=len(BAND_COLS),\n",
    "        hidden_dim=best_cfg[\"hidden_dim\"],\n",
    "        output_dim=len(BAND_COLS),\n",
    "    ).to(device)\n",
    "    best_gen.load_state_dict(best_state)\n",
    "\n",
    "    # Save checkpoint for this condition\n",
    "    ft_ckpt = interp_dir / f\"{MODEL_KEY}_{cond_tag}_finetuned.pt\"\n",
    "    torch.save(best_gen.state_dict(), ft_ckpt)\n",
    "    print(f\"[{cond_tag}] Fine-tuned weights saved to {ft_ckpt}\")\n",
    "\n",
    "    return best_gen, best_cfg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b4222e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_synthetic_from_gen(\n",
    "    gen: nn.Module,\n",
    "    X_real_cond: np.ndarray,\n",
    "    n_samples: int,\n",
    "    noise_std: float = 0.10,\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate synthetic samples by taking real vectors (as anchors)\n",
    "    and adding noise before passing through the generator.\n",
    "    This preserves the real distribution structure but lets\n",
    "    the generator learn a denoising + interpolation mapping.\n",
    "    \"\"\"\n",
    "    device = next(gen.parameters()).device\n",
    "\n",
    "    N_real = X_real_cond.shape[0]\n",
    "    # Sample indices with replacement to get n_samples anchors\n",
    "    idx = np.random.randint(0, N_real, size=n_samples)\n",
    "    X_anchor = X_real_cond[idx]\n",
    "\n",
    "    X_anchor_tensor = torch.from_numpy(X_anchor.astype(np.float32)).to(device)\n",
    "\n",
    "    if noise_std > 0.0:\n",
    "        noise = torch.randn_like(X_anchor_tensor) * noise_std\n",
    "        X_noisy = X_anchor_tensor + noise\n",
    "    else:\n",
    "        X_noisy = X_anchor_tensor\n",
    "\n",
    "    gen.eval()\n",
    "    X_syn_tensor = gen(X_noisy)\n",
    "    X_syn = X_syn_tensor.cpu().numpy()\n",
    "\n",
    "    return X_syn, idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "15e142c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune_and_regenerate_all_conditions(\n",
    "    interp_real: pd.DataFrame,\n",
    "    interp_syn: pd.DataFrame,\n",
    "    noise_std_train: float = 0.05,\n",
    "    noise_std_gen: float = 0.10,\n",
    "):\n",
    "    \"\"\"\n",
    "    For each condition (S1, S2_match, S2_nomatch):\n",
    "      1. Take REAL normalized features (Phase-4 baseline).\n",
    "      2. Fine-tune an INTERP generator on this condition's real data.\n",
    "      3. Regenerate the SAME number of synthetic samples as in the baseline,\n",
    "         using the trained generator and real anchors.\n",
    "      4. Return a new syn DataFrame with same columns but source='synthetic_finetuned'.\n",
    "    \"\"\"\n",
    "\n",
    "    all_syn_rows = []\n",
    "\n",
    "    for cond in CANONICAL_CONDITIONS:\n",
    "        print(\"\\n\" + \"#\" * 80)\n",
    "        print(f\"FINE-TUNING & REGENERATING — CONDITION = {cond}\")\n",
    "        print(\"#\" * 80)\n",
    "\n",
    "        # Real subset for this condition\n",
    "        X_real_cond, y_real_cond, df_real_cond = get_condition_slice(interp_real, cond)\n",
    "        n_real_cond = X_real_cond.shape[0]\n",
    "\n",
    "        # Baseline synthetic count for this condition\n",
    "        _, _, df_syn_cond = get_condition_slice(interp_syn, cond)\n",
    "        n_syn_cond = df_syn_cond.shape[0]\n",
    "\n",
    "        print(f\"[{cond}] N_real={n_real_cond}, N_syn_baseline={n_syn_cond}\")\n",
    "\n",
    "        # 1) Fine-tune generator on REAL\n",
    "        gen, cfg = fine_tune_interp_model_for_condition(\n",
    "            cond_tag=cond,\n",
    "            X_cond_real=X_real_cond,\n",
    "            n_epochs=80,\n",
    "            patience=8,\n",
    "            hyper_grid=None,           # use default grid\n",
    "            noise_std=noise_std_train, # noise for denoising-style training\n",
    "        )\n",
    "\n",
    "        # 2) Regenerate synthetic data with same count\n",
    "        X_syn_new, idx_used = generate_synthetic_from_gen(\n",
    "            gen,\n",
    "            X_real_cond,\n",
    "            n_samples=n_syn_cond,\n",
    "            noise_std=noise_std_gen,\n",
    "        )\n",
    "\n",
    "        # Build DataFrame\n",
    "        df_syn_new = pd.DataFrame(X_syn_new, columns=BAND_COLS)\n",
    "\n",
    "        # Copy labels and condition from the real anchors that we sampled\n",
    "        df_syn_new[LABEL_COL] = y_real_cond[idx_used]\n",
    "        df_syn_new[COND_COL]  = cond\n",
    "        df_syn_new[\"source\"]  = \"synthetic_finetuned\"\n",
    "\n",
    "        all_syn_rows.append(df_syn_new)\n",
    "\n",
    "    interp_syn_finetuned = pd.concat(all_syn_rows, axis=0).reset_index(drop=True)\n",
    "\n",
    "    print(\"\\nALL CONDITIONS DONE.\")\n",
    "    print(\"Finetuned synthetic shape:\", interp_syn_finetuned.shape)\n",
    "\n",
    "    # Save to disk for Phase-5 evaluation\n",
    "    out_fp = interp_dir / f\"{MODEL_KEY}_syn_finetuned.csv\"\n",
    "    interp_syn_finetuned.to_csv(out_fp, index=False)\n",
    "    print(f\"Saved finetuned synthetic to {out_fp}\")\n",
    "\n",
    "    return interp_syn_finetuned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "112a6e17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "################################################################################\n",
      "FINE-TUNING & REGENERATING — CONDITION = S1\n",
      "################################################################################\n",
      "[S1] N_real=0, N_syn_baseline=0\n",
      "\n",
      "[S1] Trying config: hidden_dim=32, lr=0.001, wd=0.0, bs=128\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "num_samples should be a positive integer value, but got num_samples=0",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[93]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m interp_syn_finetuned = \u001b[43mfine_tune_and_regenerate_all_conditions\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterp_real\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterp_real\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterp_syn\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterp_syn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnoise_std_train\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.05\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnoise_std_gen\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[92]\u001b[39m\u001b[32m, line 34\u001b[39m, in \u001b[36mfine_tune_and_regenerate_all_conditions\u001b[39m\u001b[34m(interp_real, interp_syn, noise_std_train, noise_std_gen)\u001b[39m\n\u001b[32m     31\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcond\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] N_real=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_real_cond\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, N_syn_baseline=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_syn_cond\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# 1) Fine-tune generator on REAL\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m gen, cfg = \u001b[43mfine_tune_interp_model_for_condition\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcond_tag\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcond\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_cond_real\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX_real_cond\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m80\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhyper_grid\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m           \u001b[49m\u001b[38;5;66;43;03m# use default grid\u001b[39;49;00m\n\u001b[32m     40\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnoise_std\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnoise_std_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# noise for denoising-style training\u001b[39;49;00m\n\u001b[32m     41\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[38;5;66;03m# 2) Regenerate synthetic data with same count\u001b[39;00m\n\u001b[32m     44\u001b[39m X_syn_new, idx_used = generate_synthetic_from_gen(\n\u001b[32m     45\u001b[39m     gen,\n\u001b[32m     46\u001b[39m     X_real_cond,\n\u001b[32m     47\u001b[39m     n_samples=n_syn_cond,\n\u001b[32m     48\u001b[39m     noise_std=noise_std_gen,\n\u001b[32m     49\u001b[39m )\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[90]\u001b[39m\u001b[32m, line 43\u001b[39m, in \u001b[36mfine_tune_interp_model_for_condition\u001b[39m\u001b[34m(cond_tag, X_cond_real, n_epochs, patience, hyper_grid, noise_std)\u001b[39m\n\u001b[32m     40\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcond_tag\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] Trying config: hidden_dim=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhd\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, lr=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlr\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, wd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwd\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, bs=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     42\u001b[39m dataset = TensorDataset(X_tensor)\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m loader  = \u001b[43mDataLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     45\u001b[39m gen = InterpGenerator(\n\u001b[32m     46\u001b[39m     input_dim=\u001b[38;5;28mlen\u001b[39m(BAND_COLS),\n\u001b[32m     47\u001b[39m     hidden_dim=hd,\n\u001b[32m     48\u001b[39m     output_dim=\u001b[38;5;28mlen\u001b[39m(BAND_COLS),\n\u001b[32m     49\u001b[39m ).to(device)\n\u001b[32m     51\u001b[39m optimizer = optim.Adam(gen.parameters(), lr=lr, weight_decay=wd)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ollama_env/lib/python3.11/site-packages/torch/utils/data/dataloader.py:388\u001b[39m, in \u001b[36mDataLoader.__init__\u001b[39m\u001b[34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers, pin_memory_device, in_order)\u001b[39m\n\u001b[32m    386\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# map-style\u001b[39;00m\n\u001b[32m    387\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m shuffle:\n\u001b[32m--> \u001b[39m\u001b[32m388\u001b[39m         sampler = \u001b[43mRandomSampler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m    389\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    390\u001b[39m         sampler = SequentialSampler(dataset)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ollama_env/lib/python3.11/site-packages/torch/utils/data/sampler.py:156\u001b[39m, in \u001b[36mRandomSampler.__init__\u001b[39m\u001b[34m(self, data_source, replacement, num_samples, generator)\u001b[39m\n\u001b[32m    151\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m    152\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mreplacement should be a boolean value, but got replacement=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.replacement\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    153\u001b[39m     )\n\u001b[32m    155\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.num_samples, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.num_samples <= \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    157\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mnum_samples should be a positive integer value, but got num_samples=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.num_samples\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    158\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: num_samples should be a positive integer value, but got num_samples=0"
     ]
    }
   ],
   "source": [
    "interp_syn_finetuned = fine_tune_and_regenerate_all_conditions(\n",
    "    interp_real=interp_real,\n",
    "    interp_syn=interp_syn,\n",
    "    noise_std_train=0.05,\n",
    "    noise_std_gen=0.10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b928649f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b43c48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dcc7553",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139c8487",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e0d3d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "153e1239",
   "metadata": {},
   "source": [
    "### 5.2.1. MMD Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1ba78a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_mmd(X_real: np.ndarray, X_syn: np.ndarray) -> float:\n",
    "    \n",
    "    Xr = X_real\n",
    "    Xs = X_syn\n",
    "    n = Xr.shape[0]\n",
    "    m = Xs.shape[0]\n",
    "\n",
    "    # means\n",
    "    mean_r = Xr.mean(axis=0, keepdims=True)\n",
    "    mean_s = Xs.mean(axis=0, keepdims=True)\n",
    "\n",
    "    # linear kernel MMD^2\n",
    "    mmd2 = np.sum((mean_r - mean_s) ** 2)\n",
    "    return float(mmd2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb522f9",
   "metadata": {},
   "source": [
    "### 5.2.2. Distribution Metrics (KS per band + MMD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4f0c11ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_distribution_metrics(X_real: np.ndarray, X_syn: np.ndarray, band_cols=BAND_COLS, model_name: str = \"\", title_suffix: str = \"\"):\n",
    "    \n",
    "    assert X_real.shape[1] == len(band_cols)\n",
    "    assert X_syn.shape[1]  == len(band_cols)\n",
    "\n",
    "    print(f\"\\nDISTRIBUTION METRICS — {model_name} [{title_suffix}]\")\n",
    "\n",
    "    ks_results = []\n",
    "    for i, band in enumerate(band_cols):\n",
    "        r = X_real[:, i]\n",
    "        s = X_syn[:, i]\n",
    "        ks_stat, p_val = ks_2samp(r, s)\n",
    "\n",
    "        similar_flag = \"✓ Similar\" if p_val >= 0.05 else \"✗ Different\"\n",
    "        print(f\"{band:<7}: KS={ks_stat:.4f}, p={p_val:.4f}   {similar_flag}\")\n",
    "\n",
    "        ks_results.append({\n",
    "            \"band\": band,\n",
    "            \"ks\": float(ks_stat),\n",
    "            \"p\": float(p_val),\n",
    "        })\n",
    "\n",
    "    mmd_val = linear_mmd(X_real, X_syn)\n",
    "    print(f\"MMD: {mmd_val:.6f}\")\n",
    "\n",
    "    return ks_results, mmd_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8757d0",
   "metadata": {},
   "source": [
    "### 5.2.3. Real VS Synthetic Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ce597701",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_real_vs_syn(X_real: np.ndarray, X_syn: np.ndarray, model_name: str = \"\", title_suffix: str = \"\", n_repeats: int = 3):\n",
    "\n",
    "    X = np.vstack([X_real, X_syn])\n",
    "    y = np.concatenate([\n",
    "        np.zeros(X_real.shape[0], dtype=int),\n",
    "        np.ones(X_syn.shape[0], dtype=int),\n",
    "    ])\n",
    "\n",
    "    accs = []\n",
    "    for rep in range(n_repeats):\n",
    "        X_tr, X_te, y_tr, y_te = train_test_split(\n",
    "            X, y, test_size=0.3, random_state=RANDOM_SEED + rep, stratify=y\n",
    "        )\n",
    "        clf = RandomForestClassifier(\n",
    "            n_estimators=200,\n",
    "            max_depth=None,\n",
    "            random_state=RANDOM_SEED + rep,\n",
    "            n_jobs=-1,\n",
    "        )\n",
    "        clf.fit(X_tr, y_tr)\n",
    "        acc = clf.score(X_te, y_te)\n",
    "        accs.append(acc)\n",
    "\n",
    "    acc_mean = float(np.mean(accs))\n",
    "    print(f\"\\nREAL-vs-SYN CLASSIFIER — {model_name} [{title_suffix}]\")\n",
    "    print(f\"Accuracy: {acc_mean:.4f}\")\n",
    "\n",
    "    return acc_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667d4055",
   "metadata": {},
   "source": [
    "### 5.2.4. TSTR/TRTR Random Forest Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "957c21ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_tstr_trtr(X_real: np.ndarray, y_real: np.ndarray, X_syn: np.ndarray, y_syn: np.ndarray, model_name: str = \"\", title_suffix: str = \"\"):\n",
    "\n",
    "    Xr_tr, Xr_te, yr_tr, yr_te = train_test_split(\n",
    "        X_real, y_real, test_size=0.3,\n",
    "        random_state=RANDOM_SEED, stratify=y_real\n",
    "    )\n",
    "\n",
    "    clf = RandomForestClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=None,\n",
    "        random_state=RANDOM_SEED,\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "    clf.fit(Xr_tr, yr_tr)\n",
    "\n",
    "    trtr_acc = clf.score(Xr_te, yr_te)\n",
    "    tstr_acc = clf.score(X_syn, y_syn)\n",
    "\n",
    "    print(f\"\\nTSTR / TRTR — {model_name} [{title_suffix}]\")\n",
    "    print(f\"TRTR: {trtr_acc:.4f}\")\n",
    "    print(f\"TSTR: {tstr_acc:.4f}\")\n",
    "    print(f\"Gap : {tstr_acc - trtr_acc:.4f}\")\n",
    "\n",
    "    return float(trtr_acc), float(tstr_acc), float(tstr_acc - trtr_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785f149b",
   "metadata": {},
   "source": [
    "## 5.3. Classwise Interpolation Model Fine Tune"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc1b3e7",
   "metadata": {},
   "source": [
    "### 5.3.1. Interpolation Model Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "017765ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InterpGenerator(nn.Module):\n",
    "    def __init__(self, input_dim=6, hidden_dim=32, output_dim=6):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35560a09",
   "metadata": {},
   "source": [
    "### 5.3.2. Fine Tune for a Single Condition with Grid Search and Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "86c6b041",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune_interp_model_for_condition(cond_tag: str, X_cond_real: np.ndarray, n_epochs: int = 100, patience: int = 10, hyper_grid=None, noise_std: float = 0.0):\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    input_dim = X_cond_real.shape[1]\n",
    "\n",
    "    if hyper_grid is None:\n",
    "        hyper_grid = {\n",
    "            \"hidden_dim\": [32, 64],\n",
    "            \"lr\": [1e-3, 5e-4],\n",
    "            \"batch_size\": [128, 256],\n",
    "        }\n",
    "\n",
    "    # reproducible shuffle / split indices\n",
    "    n = X_cond_real.shape[0]\n",
    "    indices = np.arange(n)\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    # 80% train, 20% val\n",
    "    split = int(0.8 * n)\n",
    "    train_idx = indices[:split]\n",
    "    val_idx   = indices[split:]\n",
    "\n",
    "    X_train = X_cond_real[train_idx]\n",
    "    X_val   = X_cond_real[val_idx]\n",
    "\n",
    "    X_train_t = torch.from_numpy(X_train.astype(np.float32))\n",
    "    X_val_t   = torch.from_numpy(X_val.astype(np.float32))\n",
    "\n",
    "    best_global = {\n",
    "        \"val_loss\": float(\"inf\"),\n",
    "        \"config\":   None,\n",
    "        \"state\":    None,\n",
    "    }\n",
    "\n",
    "    # Grid search\n",
    "    for h_dim in hyper_grid[\"hidden_dim\"]:\n",
    "        for lr in hyper_grid[\"lr\"]:\n",
    "            for bsz in hyper_grid[\"batch_size\"]:\n",
    "                print(f\"\\n[{cond_tag}] Trying config: hidden_dim={h_dim}, lr={lr}, batch_size={bsz}\")\n",
    "\n",
    "                gen = InterpGenerator(input_dim=input_dim, hidden_dim=h_dim, output_dim=input_dim).to(device)\n",
    "\n",
    "                # load baseline checkpoint if exists\n",
    "                baseline_ckpt = interp_dir / f\"{model_key}_{cond_tag}_baseline.pt\"\n",
    "                if baseline_ckpt.exists():\n",
    "                    print(f\"  Loading baseline weights from {baseline_ckpt}\")\n",
    "                    gen.load_state_dict(torch.load(baseline_ckpt, map_location=device))\n",
    "                else:\n",
    "                    print(\"  No baseline checkpoint found. Starting from scratch (for this config).\")\n",
    "\n",
    "                optimizer = optim.Adam(gen.parameters(), lr=lr)\n",
    "                criterion = nn.MSELoss()\n",
    "\n",
    "                train_ds = TensorDataset(X_train_t)\n",
    "                train_loader = DataLoader(train_ds, batch_size=bsz, shuffle=True)\n",
    "\n",
    "                X_val_t_device = X_val_t.to(device)\n",
    "\n",
    "                best_val_for_cfg = float(\"inf\")\n",
    "                epochs_no_improve = 0\n",
    "\n",
    "                for epoch in range(n_epochs):\n",
    "                    gen.train()\n",
    "                    total_train_loss = 0.0\n",
    "\n",
    "                    for (xb,) in train_loader:\n",
    "                        xb = xb.to(device)\n",
    "\n",
    "                        # denoising-style noise\n",
    "                        if noise_std > 0:\n",
    "                            noise = torch.randn_like(xb) * noise_std\n",
    "                            xb_in = xb + noise\n",
    "                        else:\n",
    "                            xb_in = xb\n",
    "\n",
    "                        optimizer.zero_grad()\n",
    "                        x_hat = gen(xb_in)\n",
    "                        loss = criterion(x_hat, xb)\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                        total_train_loss += loss.item() * xb.size(0)\n",
    "\n",
    "                    train_loss = total_train_loss / len(train_ds)\n",
    "\n",
    "                    # validate\n",
    "                    gen.eval()\n",
    "                    with torch.no_grad():\n",
    "                        if noise_std > 0:\n",
    "                            noise_val = torch.randn_like(X_val_t_device) * noise_std\n",
    "                            val_in = X_val_t_device + noise_val\n",
    "                        else:\n",
    "                            val_in = X_val_t_device\n",
    "\n",
    "                        val_hat = gen(val_in)\n",
    "                        val_loss = criterion(val_hat, X_val_t_device).item()\n",
    "\n",
    "                    print(f\"[{cond_tag}] Epoch {epoch+1}/{n_epochs} \"\n",
    "                          f\"(h={h_dim}, lr={lr}, bsz={bsz}) \"\n",
    "                          f\"- train: {train_loss:.4f}, val: {val_loss:.4f}\")\n",
    "\n",
    "                    # early stopping check\n",
    "                    if val_loss < best_val_for_cfg - 1e-4:\n",
    "                        best_val_for_cfg = val_loss\n",
    "                        epochs_no_improve = 0\n",
    "\n",
    "                        # snapshot weights\n",
    "                        best_state_for_cfg = {\n",
    "                            \"state_dict\": gen.state_dict(),\n",
    "                            \"hidden_dim\": h_dim,\n",
    "                            \"lr\": lr,\n",
    "                            \"batch_size\": bsz,\n",
    "                        }\n",
    "                    else:\n",
    "                        epochs_no_improve += 1\n",
    "                        if epochs_no_improve >= patience:\n",
    "                            print(f\"[{cond_tag}] Early stopping (no improvement for {patience} epochs).\")\n",
    "                            break\n",
    "\n",
    "                # compare with global best\n",
    "                if best_val_for_cfg < best_global[\"val_loss\"]:\n",
    "                    best_global[\"val_loss\"] = best_val_for_cfg\n",
    "                    best_global[\"config\"]   = {\n",
    "                        \"hidden_dim\": h_dim,\n",
    "                        \"lr\": lr,\n",
    "                        \"batch_size\": bsz,\n",
    "                    }\n",
    "                    best_global[\"state\"]    = best_state_for_cfg[\"state_dict\"]\n",
    "\n",
    "    # Build final generator with best config and load best weights\n",
    "    best_cfg = best_global[\"config\"]\n",
    "    print(f\"\\n[{cond_tag}] BEST CONFIG: {best_cfg}, best_val_loss={best_global['val_loss']:.6f}\")\n",
    "\n",
    "    best_gen = InterpGenerator(\n",
    "        input_dim=input_dim,\n",
    "        hidden_dim=best_cfg[\"hidden_dim\"],\n",
    "        output_dim=input_dim\n",
    "    ).to(device)\n",
    "    best_gen.load_state_dict(best_global[\"state\"])\n",
    "\n",
    "    # Save fine-tuned weights\n",
    "    ft_ckpt = interp_dir / f\"{model_key}_{cond_tag}_finetuned.pt\"\n",
    "    torch.save(best_gen.state_dict(), ft_ckpt)\n",
    "    print(f\"[{cond_tag}] Fine-tuned weights saved to {ft_ckpt}\")\n",
    "\n",
    "    return best_gen, best_cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db14b32",
   "metadata": {},
   "source": [
    "## 5.4. Generate New Synthetic Data from Fined Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "775a0149",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_from_gen(gen: nn.Module, X_real_cond: np.ndarray, n_samples: int, noise_std: float = 0.1):\n",
    "\n",
    "    device = next(gen.parameters()).device\n",
    "    gen.eval()\n",
    "\n",
    "    idx = np.random.choice(X_real_cond.shape[0], size=n_samples, replace=True)\n",
    "    X_base = X_real_cond[idx]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        x = torch.from_numpy(X_base.astype(np.float32)).to(device)\n",
    "        z = torch.randn_like(x) * noise_std\n",
    "        x_in = x + z\n",
    "        x_out = gen(x_in).cpu().numpy()\n",
    "\n",
    "    return x_out, idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c1871e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune_and_regenerate_all_conditions(interp_real: pd.DataFrame, interp_syn: pd.DataFrame, noise_std_train: float = 0.05, noise_std_gen: float = 0.10):\n",
    "\n",
    "    all_syn_rows = []\n",
    "\n",
    "    for cond in CANONICAL_CONDITIONS:\n",
    "        print(\"\\n\" + \"#\" * 80)\n",
    "        print(f\"FINE-TUNE FOR CONDITION: {cond}\")\n",
    "        print(\"#\" * 80)\n",
    "\n",
    "        # real subset for this condition\n",
    "        mask_real_cond = (interp_real[COND_COL] == cond)\n",
    "        df_real_cond = interp_real.loc[mask_real_cond].reset_index(drop=True)\n",
    "        X_real_cond = df_real_cond[BAND_COLS].to_numpy()\n",
    "        y_real_cond = df_real_cond[LABEL_COL].to_numpy()\n",
    "\n",
    "        # baseline syn subset (to match sample size & label distribution)\n",
    "        mask_syn_cond = (interp_syn[COND_COL] == cond)\n",
    "        df_syn_cond = interp_syn.loc[mask_syn_cond].reset_index(drop=True)\n",
    "        n_syn_cond  = df_syn_cond.shape[0]\n",
    "\n",
    "        print(f\"Condition {cond}: N_real={X_real_cond.shape[0]}, N_syn={n_syn_cond}\")\n",
    "\n",
    "        # fine-tune\n",
    "        gen, cfg = fine_tune_interp_model_for_condition(\n",
    "            cond_tag=cond,\n",
    "            X_cond_real=X_real_cond,\n",
    "            n_epochs=80,\n",
    "            patience=8,\n",
    "            hyper_grid=None,          # default grid\n",
    "            noise_std=noise_std_train # noise in training (denoising style)\n",
    "        )\n",
    "\n",
    "        # generate new synthetic\n",
    "        X_syn_new, idx_used = generate_synthetic_from_gen(\n",
    "            gen,\n",
    "            X_real_cond,\n",
    "            n_samples=n_syn_cond,\n",
    "            noise_std=noise_std_gen,\n",
    "        )\n",
    "\n",
    "        # For labels, reuse labels from the sampled real rows\n",
    "        y_syn_new = y_real_cond[idx_used]\n",
    "\n",
    "        df_syn_new = pd.DataFrame(X_syn_new, columns=BAND_COLS)\n",
    "        df_syn_new[LABEL_COL] = y_syn_new\n",
    "        df_syn_new[COND_COL]  = cond\n",
    "        df_syn_new[SOURCE_COL] = \"synthetic_finetuned\"\n",
    "\n",
    "        all_syn_rows.append(df_syn_new)\n",
    "\n",
    "    interp_syn_ft = pd.concat(all_syn_rows, axis=0, ignore_index=True)\n",
    "    print(\"\\nNew fine-tuned synthetic df shape:\", interp_syn_ft.shape)\n",
    "\n",
    "    # Save for later if needed\n",
    "    out_fp = interp_dir / f\"{model_key}_syn_finetuned.csv\"\n",
    "    interp_syn_ft.to_csv(out_fp, index=False)\n",
    "    print(f\"Saved fine-tuned synthetic data to {out_fp}\")\n",
    "\n",
    "    return interp_syn_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5113f5be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "################################################################################\n",
      "FINE-TUNE FOR CONDITION: S1\n",
      "################################################################################\n",
      "Condition S1: N_real=0, N_syn=0\n",
      "\n",
      "[S1] Trying config: hidden_dim=32, lr=0.001, batch_size=128\n",
      "  No baseline checkpoint found. Starting from scratch (for this config).\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "num_samples should be a positive integer value, but got num_samples=0",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[72]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m interp_syn_finetuned = \u001b[43mfine_tune_and_regenerate_all_conditions\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterp_real\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreal_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterp_syn\u001b[49m\u001b[43m=\u001b[49m\u001b[43msyn_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnoise_std_train\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.05\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnoise_std_gen\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[70]\u001b[39m\u001b[32m, line 24\u001b[39m, in \u001b[36mfine_tune_and_regenerate_all_conditions\u001b[39m\u001b[34m(interp_real, interp_syn, noise_std_train, noise_std_gen)\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCondition \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcond\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: N_real=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX_real_cond.shape[\u001b[32m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, N_syn=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_syn_cond\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# fine-tune\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m gen, cfg = \u001b[43mfine_tune_interp_model_for_condition\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcond_tag\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcond\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_cond_real\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX_real_cond\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m80\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhyper_grid\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m          \u001b[49m\u001b[38;5;66;43;03m# default grid\u001b[39;49;00m\n\u001b[32m     30\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnoise_std\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnoise_std_train\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# noise in training (denoising style)\u001b[39;49;00m\n\u001b[32m     31\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# generate new synthetic\u001b[39;00m\n\u001b[32m     34\u001b[39m X_syn_new, idx_used = generate_synthetic_from_gen(\n\u001b[32m     35\u001b[39m     gen,\n\u001b[32m     36\u001b[39m     X_real_cond,\n\u001b[32m     37\u001b[39m     n_samples=n_syn_cond,\n\u001b[32m     38\u001b[39m     noise_std=noise_std_gen,\n\u001b[32m     39\u001b[39m )\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[68]\u001b[39m\u001b[32m, line 55\u001b[39m, in \u001b[36mfine_tune_interp_model_for_condition\u001b[39m\u001b[34m(cond_tag, X_cond_real, n_epochs, patience, hyper_grid, noise_std)\u001b[39m\n\u001b[32m     52\u001b[39m criterion = nn.MSELoss()\n\u001b[32m     54\u001b[39m train_ds = TensorDataset(X_train_t)\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m train_loader = \u001b[43mDataLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbsz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     57\u001b[39m X_val_t_device = X_val_t.to(device)\n\u001b[32m     59\u001b[39m best_val_for_cfg = \u001b[38;5;28mfloat\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33minf\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ollama_env/lib/python3.11/site-packages/torch/utils/data/dataloader.py:388\u001b[39m, in \u001b[36mDataLoader.__init__\u001b[39m\u001b[34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers, pin_memory_device, in_order)\u001b[39m\n\u001b[32m    386\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# map-style\u001b[39;00m\n\u001b[32m    387\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m shuffle:\n\u001b[32m--> \u001b[39m\u001b[32m388\u001b[39m         sampler = \u001b[43mRandomSampler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m    389\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    390\u001b[39m         sampler = SequentialSampler(dataset)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ollama_env/lib/python3.11/site-packages/torch/utils/data/sampler.py:156\u001b[39m, in \u001b[36mRandomSampler.__init__\u001b[39m\u001b[34m(self, data_source, replacement, num_samples, generator)\u001b[39m\n\u001b[32m    151\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m    152\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mreplacement should be a boolean value, but got replacement=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.replacement\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    153\u001b[39m     )\n\u001b[32m    155\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.num_samples, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.num_samples <= \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    157\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mnum_samples should be a positive integer value, but got num_samples=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.num_samples\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    158\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: num_samples should be a positive integer value, but got num_samples=0"
     ]
    }
   ],
   "source": [
    "interp_syn_finetuned = fine_tune_and_regenerate_all_conditions(\n",
    "    interp_real=real_df,\n",
    "    interp_syn=syn_df,\n",
    "    noise_std_train=0.05,\n",
    "    noise_std_gen=0.10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318662aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ollama_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
